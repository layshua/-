反正也没什么人看，下一期就等明年再翻译吧！另外也说不准，心情不好的话这应该就是最后一篇了……

![](<images/1685518870265.png>)

_“The display is the computer.”_

_“显示器就是计算机。”_

_——黄仁勋_

_（注：黄仁勋，__美籍华人，NVIDIA 公司创始人兼首席执行官。此句大意是指图形显示对于用户的直观与重要性。__）_

从历史上看，图形加速始于在与三角形重叠的每条像素扫描线上插入颜色，然后显示这些值。其中包括访问图像数据的功能，可以将纹理应用于表面。其中还添加了用于内插（interpolating）和测试 z 深度的硬件，提供了内置的可见性检查。由于它们的频繁使用，因此将此类过程用于专用硬件以提高性能。连续几代硬件中添加了渲染管线的更多部分，以及每个部分的更多功能。专用图形硬件相对于 CPU 的唯一计算优势是速度，但速度至关重要。

在过去的二十年中，图形硬件经历了不可思议的转变。1999 年，第一款包含硬件顶点处理的消费者图形芯片（NVIDIA 的 GeForce 256）问世。NVIDIA 创造了图形处理单元（GPU，Graphics [Processing](https://so.csdn.net/so/search?q=Processing&spm=1001.2101.3001.7020) Unit）一词，以将 GeForce 256 与以前可用的仅光栅化芯片区分开来，并且它一直坚持下去。在接下来的几年中，GPU 从复杂的固定功能管道的可配置实现发展到高度可编程的空白状态，开发人员可以在其中实现自己的算法。各种可编程着色器是控制 GPU 的主要方法。为了提高效率，管线的某些部分仍然是可配置的，而不是可编程的，但是趋势是朝着可编程性和灵活性的方向发展 **[175]**。

GPU 通过专注于一组高度可并行化的任务而获得了卓越的速度。他们拥有专门的定制芯片，可以专用于实现 z 缓冲区、快速访问纹理图像和其他缓冲区、查找例如三角形覆盖的像素。这些部件如何执行其功能将在第 23 章中介绍。更重要的是要早点知道 GPU 如何实现其可编程着色器的并行性。

3.3 节介绍了着色器的功能。目前，您需要知道的是着色器核心是一个小型处理器，可以执行一些相对隔离的任务，例如将顶点从其在世界坐标转换为屏幕坐标，或者计算被三角形覆盖的像素的颜色 。每帧有成千上万个三角形发送到屏幕，每秒可能有数十亿次着色器调用（shader invocations），即运行着色器程序的单独实例。

首先，延迟（latency）是所有处理器都面临的问题。访问数据需要花费一些时间。考虑延迟长短的一种基本方法是，信息所处位置离处理器越远，等待时间就越长。第 23.3 节详细介绍了延迟。存储在存储芯片中的信息将比本地寄存器中的信息花费更长的时间。18.4.1 节将更深入地讨论内存访问。关键是等待数据检索意味着处理器停滞了，这降低了性能。

**引用：**

**[175]** Blythe, David, “The Direct3D 10 System,” ACM Transactions on Graphics, vol. 25, no. 3, pp. 724–734, July 2006. Cited on p. 29, 39, 42, 47, 48, 50, 249

## **3.1 数据并行架构 Data-Parallel Architectures**

不同的处理器体系结构使用各种策略来避免停顿。对 CPU 进行了优化，以处理各种数据结构和大型代码库。CPU 可以具有多个处理器，但是每个 CPU 都以串行方式运行代码，有限的 SIMD 向量处理是次要的例外。为了最大程度地减少延迟的影响，CPU 的许多芯片都由快速本地缓存组成，这些缓存中填充了下一步可能需要的数据。CPU 还通过使用诸如分支预测（branch prediction），指令重新排序（instruction reordering），寄存器重命名（register renaming）和缓存预取（cache prefetching ）之类的巧妙技术来避免停顿。**[715]**

GPU 采用不同的方法。GPU 的大部分芯片区域专用于称为着色器核心（shader cores）的大量处理器，通常数量多达数千个。GPU 是流处理器，其中依次处理相似数据的有序集合。由于这种相似性（例如，一组顶点或像素），GPU 可以大规模并行地处理这些数据。另一个重要的部分是这些调用要尽可能地独立，这样它们就不需要来自相邻调用的信息，并且不共享可写的存储位置。有时我们会打破该规则以允许新的功能，但是此类例外的代价是潜在的延迟，因为一个处理器可能会等待另一个处理器完成其工作。

GPU 针对吞吐量（throughput）进行了优化，吞吐量定义为可以处理数据的最大速率。但是，这种快速处理具有成本。由于专用于高速缓存存储器和控制逻辑的芯片面积较小，因此每个着色器内核的等待时间通常比 CPU 处理器遇到的等待时间长得多 **[462]**。

假设网格已光栅化，并且两千个像素具有要处理的片元（fragments）；像素着色器程序将被调用 2000 次。想象只有一个着色器处理器，这是世界上最弱的 GPU。它开始为 2000 的第一个片元执行着色器程序。着色器处理器对寄存器中的值执行一些算术运算。寄存器是本地的，可以快速访问，因此不会发生停顿。然后，着色器处理器会执行一条指令，例如纹理访问； 例如，对于给定的表面位置，程序需要知道应用于网格的图像的像素颜色。纹理是一个完全独立的资源，而不是像素程序本地内存的一部分，并且纹理访问可能会涉及到一定程度。内存提取可能需要数百到数千个时钟周期，在此期间 GPU 处理器不执行任何操作。此时，着色器处理器将停止运行，等待纹理的颜色值返回。

为了使这个糟糕的 GPU 变得更好，我们为每个片元提供一些用于其本地寄存器的存储空间。现在，允许着色器处理器切换并执行另一个片元，即两千个第二个片元，而不是停止纹理获取。此切换速度非常快，除了注意第一条指令正在执行哪条指令之外，第一段或第二段中的内容均不受影响。现在执行第二个片元。与第一个相同，执行一些算术函数，然后再次遇到纹理获取。着色器核心现在切换到另一个片元，即第三个片元。最终，所有两千个片元都以这种方式处理。此时，着色器处理器将返回片元编号一。此时，纹理颜色已被获取并且可以使用，因此着色器程序可以继续执行。处理器以相同的方式进行处理，直到遇到另一个已知会暂停执行的指令，或者程序完成。与着色器处理器（shader processor）始终专注于一个片元相比，执行单个片元所需的时间更长，但是整个片元的总体执行时间将大大减少。

在这种架构中，通过切换到另一个片元使 GPU 保持忙碌来隐藏延迟。GPU 通过将指令执行逻辑与数据分离开来，使该设计更进一步。称为单指令多数据（SIMD，single instruction, multiple data）的这种安排可以在固定数量的着色器程序上以锁定步骤执行同一命令。SIMD 的优点是，与使用单独的逻辑和调度单元运行每个程序相比，用于处理数据和交换的硅（和功率）要少得多。将我们的 2000 片元示例转换为现代 GPU 术语，每个片元的像素着色器调用都称为线程。这种类型的线程与 CPU 线程不同。它由用于着色器输入值的一点内存以及着色器执行所需的任何寄存器空间组成。使用相同着色器程序的线程被分为几组，被 NVIDIA 称为 warp，被 AMD 称为 wavefronts。一个 warp/wavefront 被 计划用于 SIMD 处理，由 8 至 64 之间的任意数量的 GPU 着色器内核执行。每个线程都映射到 SIMD 通道。

假设我们有两千个线程要执行。NVIDIA GPU 的 warps 包含 32 个线程。这将产生 2000/32 = 62.5 个 warps，这意味着分配了 63 个 warps，其中一个 warps 是一半为空。warp 的执行类似于我们的单个 GPU 处理器示例。着色器程序在所有 32 个处理器上以固定步骤执行。因为对所有线程执行相同的指令，遇到内存提取时，所有线程都会同时遇到它。提取信号表明线程 warp 将停止，所有线程都在等待它们的（不同的）结果。此时不会停顿，而是将 warp 换成 32 个线程的另一个 warp，然后由 32 个内核执行。这种交换的速度与我们的单处理器系统一样快，因为在将 warp 换入或换出时，每个线程内的数据都不会被触及。每个线程都有自己的寄存器，每个 warp 都跟踪其正在执行的指令。交换新线程只是将一组核心指向另一组要执行的线程即可。没有其他开销。warp 执行或换出，直到全部完成。参见图 3.1。

![](<images/1685518870308.png>)

_图 3.1。简化的着色器执行示例。三角形片元（称为线程，threads）被收集成 warps。每个 warp 显示为四个线程，但实际上有 32 个线程。要执行的着色器程序长五个指令。四个 GPU 着色器处理器的集合在第一次 warp 时执行这些指令，直到在 “txr” 命令上检测到停顿条件为止，这需要时间来获取其数据。交换第二个 warp，并对其应用着色器程序的前三个指令，直到再次检测到停顿为止。交换第三个 warp 并使其停止后，通过交换第一个 warp 并继续执行。如果此时尚未返回其 “txr” 命令的数据，则执行将真正停止，直到这些数据可用为止。每个 warp 依次完成。_

_（注：寄存器占据越多 ，warps 越少，tex 操作会导致 warps 切换，如果单个 SM 的  warps 切换完了 ,tex 还没做完的话就会造成等待，当然这个不一定，在低端机上可能会，中高端可能不会，我们并不清楚这个具体耗时哪个更快，但是我们能从 shader code 上去避免这个。实际操作下来不断的 tex 操作，确实会造成性能灾难性降低，所以在中间插入其他计算，相比只有单个 tex 操作， 然后每个 warps 只执行  tex 然后就立即切换下一个 warps ，或许能减少造成的延迟。）_

在我们的简单示例中，纹理获取内存的等待时间可能导致 warp 掉出。实际上，因为交换成本非常低，所以可以将 warp 换成较短的延迟。还有其他几种用于优化执行的技术 **[945]**，但 warp 交换（warp-swapping）是所有 GPU 使用的主要延迟隐藏机制。此过程的效率如何涉及多个因素。例如，如果线程很少，那么几乎不会创建任何 warp，从而使延迟隐藏成为问题。

着色器程序的结构是影响效率的重要特征。一个主要因素是每个线程使用的寄存器数量。在我们的示例中，我们假设一次可以将 2000 个线程全部驻留在 GPU 上。与每个线程相关联的着色器程序所需的寄存器越多，则线程中可以驻留的线程越少，因此 warp 也就越少。warps 不足可能意味着无法通过交换来减轻失速。驻留的 warps 被称为 “飞行中”（in flight），这个数字称为占用率（occupancy）。高占用率意味着有许多可用于处理的 warp，因此空闲处理器的可能性较小。占用率低通常会导致性能不佳。内存提取的频率也影响需要多少延迟隐藏。Lauritzen **[993]** 概述了着色器使用的寄存器数量和共享内存如何影响占用率。Wronski **[1911，1914]** 讨论了理想的占用率如何根据着色器执行的操作类型而变化。

影响整体效率的另一个因素是由 “if” 语句和循环引起的动态分支。假设在着色器程序中遇到 “if” 语句。如果所有线程求值并采用同一分支，则 warp 可以继续进行而不必担心其他分支。但是，如果某些线程甚至一个线程采用了替代路径，那么 warp 必须执行两个分支，从而丢弃每个特定线程不需要的结果 **[530，945]**。这个问题称为线程发散（thread divergence），其中一些线程可能需要执行循环迭代或执行 warp 中其他线程不执行的 “if” 路径，从而使它们在此期间处于空闲状态。

所有 GPU 都实现了这些架构思想，从而导致系统具有严格的限制，但每瓦（watt）的计算能力却很大。了解该系统的运行方式将有助于您作为程序员充分利用其提供的功能。在以下各节中，我们讨论 GPU 如何实现渲染管线，可编程着色器如何运行以及每个 GPU 阶段的演变和功能。

**引用：**

**[715]** Hennessy, John L., and David A. Patterson, Computer Architecture: A Quantitative Approach, Fifth Edition, Morgan Kaufmann, 2011. Cited on p. 12, 30, 783, 789, 867, 1007, 1040

**[462]** Fatahalian, Kayvon, and Randy Bryant, Parallel Computer Architecture and Programming course, Carnegie Mellon University, Spring 2017. Cited on p. 30, 55

**[945]** Kubisch, Christoph, “Life of a Triangle—NVIDIA’s Logical Pipeline,” NVIDIA GameWorks blog, Mar. 16, 2015. Cited on p. 32

**[993]** Lauritzen, Andrew, “Future Directions for Compute-for-Graphics,” SIGGRAPH Open Problems in Real-Time Rendering course, Aug. 2017. Cited on p. 32, 812, 908

**[1911]** Wronski, Bartlomiej, “Assassin’s Creed: Black Flag—Road to Next-Gen Graphics,” Game Developers Conference, Mar. 2014. Cited on p. 32, 218, 478, 571, 572, 801

**[1914]** Wronski, Bartlomiej, “GCN—Two Ways of Latency Hiding and Wave Occupancy,” Bart Wronski blog, Mar. 27, 2014. Cited on p. 32, 801, 1005

**[530]** Giesen, Fabian, “A Trip through the Graphics Pipeline 2011,” The ryg blog, July 9, 2011. Cited on p. 32, 42, 46, 47, 48, 49, 52, 53, 54, 55, 141, 247, 684, 701, 784, 1040

**[945]** Kubisch, Christoph, “Life of a Triangle—NVIDIA’s Logical Pipeline,” NVIDIA GameWorks blog, Mar. 16, 2015. Cited on p. 32

## **3.2 GPU 管线概述 GPU Pipeline Overview**

GPU 实现了第 2 章中描述的概念如几何处理，光栅化和像素处理管线阶段。这些阶段分为几个硬件阶段，这些阶段具有不同程度的可配置性或可编程性。图 3.2 显示了根据各个阶段的可编程性或可配置性对其进行颜色编码的各个阶段。请注意，这些物理阶段的划分与第二章中介绍的功能阶段有所不同。

![](<images/1685518870342.png>)

_图 3.2。渲染管线的 GPU 实现。这些阶段根据用户对其操作的控制程度进行颜色编码。绿色阶段是完全可编程的。虚线表示可选阶段。黄色阶段是可配置的，但不是可编程的，例如，可以为合并阶段设置各种混合模式。蓝色阶段的功能完全固定。_

我们在这里描述了 GPU 的逻辑模型（logical model）www，它是由 API 作为程序员向您公开的逻辑模型。正如第 18 和 23 章所讨论的那样，此逻辑管线（物理模型）的实现取决于硬件供应商。通过将命令添加到相邻的可编程阶段，可以在 GPU 上执行逻辑模型中固定功能的阶段。流水线中的单个程序可以分为由单独的子单元执行的元素，也可以由单独的遍历完全执行。逻辑模型可以帮助您推断出哪些因素会影响性能，但不要误以为这是 GPU 实际实现管线的方式。

顶点着色器（vertex shader）是一个完全可编程的阶段，用于实现几何处理阶段。几何着色器是一个完全可编程的阶段，可在图元的顶点（点，线或三角形）上运行。它可用于执行每个图元的着色操作，销毁图元或创建新的图元。曲面细分和几何着色器都是可选的，并非所有 GPU 都支持它们，尤其是在移动设备上。

裁剪，三角形设置和三角形遍历阶段由固定功能硬件实现。屏幕映射受窗口和视口设置的影响，在内部形成简单的比例并重新定位。像素着色器阶段是完全可编程的。尽管合并阶段不是可编程的，但它是高度可配置的，可以设置为执行多种操作。它实现了 “合并” 功能阶段，负责修改颜色，z 缓冲区，混合，模板和任何其他与输出相关的缓冲区。像素着色器的执行与合并阶段一起构成了第 2 章中介绍的概念性像素处理阶段。

随着时间的流逝，GPU 管道已从硬编码操作演变为增加灵活性和控制能力。可编程着色器阶段的引入是这一发展过程中最重要的一步。下一节将介绍各个可编程阶段的通用功能。

## **3.3 可编程着色器阶段 The Programmable Shader Stage**

现代着色器程序使用统一的着色器设计。这意味着与顶点，像素，几何和曲面细分相关的着色器共享一个公共的编程模型。在内部，它们具有相同的指令集体系结构（ISA，instruction set architecture）。实现此模型的处理器在 DirectX 中称为 “通用着色器核心”（common-shader core），据说具有此类核心的 GPU 具有统一的着色器体系结构。这种架构背后的想法是，着色器处理器可以在各种角色中使用，GPU 可以根据需要分配它们。例如，与每个由两个三角形组成的大正方形相比，一组带有小三角形的网格将需要更多的顶点着色器处理。具有单独的顶点和像素着色器核心池的 GPU 意味着严格确定了使所有核心繁忙的理想工作分配。使用统一的着色器核心，GPU 可以决定如何平衡此负载。

描述整个着色器编程模型已经超出了本书的范围，并且已经有许多文档，书籍和网站。着色器使用类似 C 的着色语言（shading languages）进行编程，例如 DirectX 的高级着色语言（HLSL，High-Level Shading Language）和 OpenGL 着色语言（GLSL，OpenGL Shading Language）。DirectX 的 HLSL 可以编译为虚拟机字节码，也称为中间语言（IL 或 DXIL），以提供硬件独立性。中间表示还可以允许着色器程序被编译和离线存储。驱动程序将此中间语言转换为特定 GPU 的 ISA。控制台编程通常避免中间语言步骤，因为那时只有一个 ISA 用于系统。

基本数据类型是 32 位单精度浮点标量和向量，尽管向量只是着色器代码的一部分，并且如上所述在硬件中不受支持。在现代 GPU 上，本机还支持 32 位整数和 64 位浮点数。浮点向量通常包含位置（xyzw），法线，矩阵行，颜色（rgba）或纹理坐标（uvwq）等数据。整数最常用于表示计数器，索引或位掩码。还支持聚合数据类型（Aggregate data types），例如结构，数组和矩阵。

一次 Draw Call 调用图形 API 来绘制一组图元（primitives），从而使图形管线执行并运行其着色器（shaders）。每个可编程着色器阶段都有两种类型的输入：统一输入（uniform inputs），其值在整个绘制调用期间保持不变（但可以在绘制调用之间进行更改），以及变化的输入（varying inputs），即来自三角形顶点或光栅化的数据。例如，像素着色器可以将光源的颜色提供为统一的值，并且三角形表面的位置每像素变化，因此也变化。纹理是一种特殊的统一输入，它曾经总是应用于表面的彩色图像，但现在可以认为是任何大型数据数组。

基础虚拟机（The underlying virtual machine）为不同类型的输入和输出提供特殊的寄存器。用于统一（uniforms）的可用常数寄存器的数量比用于变化（varying）的输入或输出的可用寄存器的数量大得多。发生这种情况是因为需要为每个顶点或像素分别存储变化的输入和输出，因此对于需要多少个输入存在自然的限制。统一输入存储一次，并在绘制调用中的所有顶点或像素之间重复使用。虚拟机还具有用于暂存空间的通用临时寄存器。可以使用临时寄存器中的整数值对所有类型的寄存器进行数组索引。着色器虚拟机的输入和输出如图 3.3 所示。

![](<images/1685518870413.png>)

_图 3.3。Shader Model 4.0 下的统一虚拟机体系结构和寄存器布局。每个资源旁边都会显示最大可用数量。用斜杠分隔的三个数字表示顶点，几何和像素着色器的限制（从左到右）。_

图形计算中常见的操作可在现代 GPU 上高效执行。着色语言通过 * 和 + 等运算符公开了这些运算中最常见的运算（例如加法和乘法）。其余的通过内在函数（intrinsic functions）公开，例如 atan（），sqrt（），log（）以及为 GPU 优化的许多其他函数。对于更复杂的运算，也存在函数，例如向量归一化（vector normalization）和反射（reflection），叉积（cross product），矩阵转置（matrix transpose）和行列式计算（determinant computations）。

术语 “流控制”（flow control）是指使用分支指令来更改代码执行流。与流控制相关的指令用于实现高级语言构造，例如“if” 和“ case”语句，以及各种类型的循环。着色器支持两种类型的流控制。静态流控制（Static flflow control）分支基于统一输入的值。这意味着代码流在绘图调用中是恒定的。静态流控制的主要好处是允许将相同的着色器用于各种不同的情况（例如，不同数量的灯光）。由于所有调用都采用相同的代码路径，因此没有线程差异。动态流控制（Dynamic flflow control）基于变化的输入的值，这意味着每个片段可以不同地执行代码。这比静态流控制功能强大得多，但会降低性能，尤其是在着色器调用之间代码流发生不规则变化时。