# 2.5 凹凸映射 Bump Map 
[【UnityShader】ParallaxMapping 视差映射（7）](https://zhuanlan.zhihu.com/p/574361162#ref_4)

[[图形学/5 书籍/《Real-Time Rendering 3rd》/第六章 · 纹理贴图及相关技术/README#8.1 凹凸贴图 Bump Mapping]]
![[Pasted image 20221029194237.png]]
![[Pasted image 20221029194339.png]]
## 物体细节的尺度
在毛星云的这篇文章中，提到了**三个尺度**：
![[技术美术/04 PBR/PBR白皮书/content/4 法线分布函数/README#一、基于物理的渲染理念：从宏观表现到微观细节]]

**本科中，老师给出了描述一个物体绘制在屏幕上（表达物体的细节）的三个尺度

-   **宏观尺度**
-   特征可能覆盖很多个像素
-   由顶点、三角形、其他几何图元表示
-   例如：角色的四肢、头部

-   **中观尺度**
-   特征可能覆盖几个像素
-   描述了宏观和微观尺度之间的特征
-   包含的细节比较复杂，无法用单个三角形进行渲染，
-   细节相对较大，可以被观察者看到几个像素以上的变化
-   例如：人脸上的皱纹、肌肉的褶皱、砖头的缝隙

-   **微观尺度**
-   特征可能是一个像素
-   通常在着色模型，写在像素着色器中，并且使用纹理贴图作为参数
-   模拟了物体表面微观几何的相互作用
-   例如：
-   有光泽的物体表面是光滑的、漫反射的物体，在微观下表面是粗糙的
-   角色的皮肤和衣服看起来也是不同的，因为使用了不同的着色模型/不同的参数
## Bump Mapping

模拟**中观尺度**的常用方法之一，可以让观察者感受到比模型尺度更小的细节
法线贴图所带来的凹凸感是怎么来的？是改变了【光照信息】（NdotL）而来。我们视觉上认为，较亮的向光面与较暗的背光面会组成一个“立体”的事物。

**基本思想**：
-   在纹理中将尺度相关的信息编码进去
-   着色过程中，用受到干扰的表面去代替真实的表面
-   这样一来，表面就会有小尺度的细节

**原理**：
-   **对物体表面贴图进行变化然后再进行光照计算的一种技术**
-   主要的原理是通过改变表面光照方程的法线，而不是表面的几何法线，或对每个待渲染的像素在计算照明之前都要加上一个从高度图中找到的扰动，来模拟凹凸不平的视觉特征
-   例如：
-   给法线分量添加噪音（法线映射贴图）
-   在一个保存扰动值的纹理图中进行查找（视差映射、浮雕映射贴图）
-   是一种提升物体真实感的有效方法，且不用提升额外的几何复杂度（不用改模型）

-   列举一个使用法线贴图的效果
![[Pasted image 20221029193834.png]]
-   可以看到，使用了法线贴图的有了明显的立体感和细节
-   对于中间的高亮部分，左边的都是均匀的，而右边即使是很高亮度的部分也能看到阴影
## Normal Mapping

![[Pasted image 20221029194529.png]]

![[TA101#NormalMap]]
## 视差映射 Parallax Mapping
参考：[【UnityShader】ParallaxMapping 视差映射（7） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/574361162)
### 原理
-   视差贴图 Parallax Mapping，又称为 Offset Mapping
-   主要为了赋予模型表面遮挡关系的细节。引入了一张高度图
-   可以和法线贴图一起使用，来产生一些真实的效果
-   高度图一般视为顶点位移来使用，此时需要三角形足够多，模型足够精细，否则看起来会有块状
-   如果在有限的三角形面的情况下，怎么办？这就用到了视差映射技术
-   **视差映射技术**：
 ![[Pasted image 20221030144850.png]]
>左：无视差右：基础视差
- 核心：改变纹理坐标
-   需要一张存储模型信息的高度图，利用模型表面高度信息来对纹理进行偏移（例如：低位置的信息被高位置的信息遮挡掉了，所以会采样更高的信息）
### 视差映射的实现

-   和法线贴图一样，是欺骗眼睛的做法（只改变纹路，不增加三角形）
-   我们的模型在**切线空间**下，所有的点都位于切线和副切线组成的平面内（图中 0.0 点），但实际上物体要有更丰富的细节。

-   例如图中的情况
-   如果不使用视差贴图，要计算当前视角下，片元 A 点（黄色）的信息，就是图中的 Ha
-   实际使用视差贴图时，真实的情况应该是视线和 A 点延长线和物体的交点，也就是 B 点，相应的就是<font color="#ff0000">Hb</font>
![[Pasted image 20221029211242.png]]
-   **视差映射的具体算法**：如何在知道 A 的 uv 值的情况下，算出 B 的 uv 值

-   知道 AB 两者的偏移量即可
-   **偏移量的获得**：用近似的方法去求解

-   首先拿 A 的高度信息进行采样，得到物体表面距离水平面（0.0）的深度值 Ha。
-   用深度值 Ha 和视线的三角关系算出物体上等比的偏移方向，算出近似的 B 点（可以看到图中近似点 B 和实际点 B 还是有挺大差距的，所以模拟度比较低）
![[Pasted image 20221029211255.png]]
![[Pasted image 20221029211302.png]]
```c
// uv高度图采样
float height = tex2D(_HeightMap,i.uv).r;  
//根据高度图信息计算出uv偏移量
float3 tangent_ViewDir = normalize(mul(TBN, world_ViewDir));  
float2 offsetUV = tangent_ViewDir.xy / tangent_ViewDir.z * height * _HeightScale;  
i.uv += offsetUV;
```
得到偏移之后 B 点的 uv，再去对法线贴图进行采样、计算时，就不会采样 A 点了，而是 B 点

其中 `viewDir.xy / viewDir.z` 为什么能代表 uvOffset？见下图：
![[Pasted image 20221029233656.png]]
>不同视线的 uv offset 应该不一样
![[Pasted image 20221029233719.png]]
>1. 以 90°角直视表面时，切线空间中的视图方向等于表面法线（0、0、1），因此不会发生位移。视角越浅，投影越大，位移效果越大。  
>2. xy/z 之所以能得到正确的 offset，是由于定义高度差为 1，使用“相似三角形”原理而来。  
1 / offset = z / (x, y) => offset = xy/z


- **理解：视差贴图是如何产生遮挡效果的**

-   当视线看到的是 A 点这样深度吗比较大的，那么视差贴图计算出的偏移值也是非常大的，这样 A 点最终被渲染出来的机会就比较小（偏移后就被采样到其他点上了）
-   当视线看到 B 点这样深度比较小的点，计算出来的偏移就比较下，甚至原来点的附近，所以被采样的机会就比较大
-   深度大的点很容易被深度小的点覆盖掉，这样就会表现出遮挡的效果


## 陡峭视差映射 Steep Parallax Mapping
![[Pasted image 20221029212354.png]]
-   也是近似解，但比视差映射精确

  **基本思想**：
  采用了**光线步进**（RayMarching）的方法。
-   将物体表面分为若干层，从最顶端开始采样，**每次沿着视角方向偏移一定的值**
-   如果当前采样的层数，**大于**实际采样的层数，就停止采样。
-   例如图中 D 点，采样到 0.75 层，实际是 0.5 层，就停止采样，返回偏移坐标

 **陡视差映射的算法**：（计算偏移点的过程）
 定义步长、循环采样、判断高度信息、得到偏移量。
 
-   首先对 A 点采样，得到深度大约为 0.8 的位置，而其对应视线深度为 0.0，不符合我们的基本思想，继续采样
-   采样 B 点，深度为 1，视线深度为 0.25，不符合，继续采样
-   采样 C 点，深度大约为 0.8，视线深度为 0.5，不符合，继续采样
-   采样 D 点，采样深度为 0.5，视线深度约为 0.75，符合上述的条件，认为是比较合理的一个偏移点，就返回结果（return）。

**陡视差的问题**：
-   在于分层机制，如果
-   分层多，性能开销就会大；
-   分层小，渲染锯齿就比较明显。
-   一种做法：可以根据视角 v 和法线 n 的角度限定采样层数
-   **锯齿问题会在浮雕贴图上做改善**

```c
// 陡峭视察映射 光线步进算法  
float2 SteepParallaxMapping(float2 uv, float3 tangent_viewDir)  
{  
    float numLayers = 20;   //最大步进次数, 步进次数越多，效果越好,开销也越大。
    float layerHeight = 1 / numLayers;  //单层步进高度  
    float2 deltaUV = tangent_viewDir.xy / tangent_viewDir.z * _HeightScale / numLayers; //单次uv偏移量  
  
    float currentLayerHeight = 0;   //步进初始高度（视线深度）  
    float2 currentUV = uv;           //当前UV  
    float2 offsetUV = float2(0, 0);  //初始化偏移量  
    float currentHeightMapValue = tex2D(_HeightMap, currentUV).r;  //采样初始高度信息(采样深度)  
  
    for(int i = 0;i<numLayers;i++)  
    {  
        if(currentLayerHeight > currentHeightMapValue)  
        {  
            return offsetUV;  
        }  
        offsetUV += deltaUV;  
        currentHeightMapValue = tex2D(_HeightMap, currentUV + offsetUV).r;  
        currentLayerHeight += layerHeight;  
    }  
    return offsetUV;  
}
```

![[Pasted image 20221030144926.png]]
>numLayers 不同时的采样结果（数值递增）

从上述图我们也能看出，采样次数较少的时候，结果有很明显的分层感。这也是 RayMarching 系特效的通病。

那么其他的 RayMarching 是怎么解决采样次数过少所导致的问题呢？一个屡试不爽的万精油解法就是——**Jitter（抖动）**。

当我们想使用抖动时，需要确定 2 个东西：抖动的噪声，抖动噪声确定，抖动最大 step 次数。
## 浮雕映射 Relief Mapping
![[Pasted image 20221029212425.png]]
![[Pasted image 20221029212436.png]]

-   浮雕映射一般用**光线步进**和**二分查找**来**决定 uv 偏移量**
-   第一步：光线步进部分，和视差贴图一样
-   后边：二分查找部分：通过光线步进找到合适的步进后，在此步进内使用二分查找来找到精确的偏移值（可能要查找多次，性能可以继续优化，由此提出改进方法 POM，见下一节）

-   **为什么不直接使用二分查找？**
-   会产生比较大的误差
-   如果直接使用二分查找，在深度 0 和 1 的中间的 1 点，进一步为 2 点 -> 3 点 ->Q 点。但我们要的结果是 P 点，可以看到结果很明显是错误的
```c
// 浮雕贴图  
float2 ReliefMapping(float2 uv, float3 tangent_viewDir)  
{  
    float numLayers = 30;   //最大步进次数  
    float layerHeight = 1 / numLayers;  //单层步进高度  
    float2 deltaUV = tangent_viewDir.xy / tangent_viewDir.z * _HeightScale / numLayers; //单次uv偏移量  
  
    float currentLayerHeight = 0;   //步进初始高度（视线深度）  
    float2 currentUV = uv;           //当前UV  
    float2 offsetUV = float2(0, 0);  //初始化偏移量  
    float currentHeightMapValue = tex2D(_HeightMap, currentUV).r;  //采样初始高度信息(采样深度)  
  
    // 光线步进  
    for(int i = 0;i<numLayers;i++)  
    {  
         if(currentLayerHeight > currentHeightMapValue)  
        {  
            break;  
        }  
        offsetUV += deltaUV;  
        currentHeightMapValue = tex2D(_HeightMap, currentUV + offsetUV).r;  
        currentLayerHeight += layerHeight;  
    }  
  
    //二分查找(是否可以继续优化？)  
    float2 P2 = uv + offsetUV;  //步进停止点  
    float2 P1 = P2 - deltaUV;   //停止点的上一个点  
  
    for(int j = 0; j < 20; j++)  
    {  
        float2 P = (P1 + P2) / 2;   //中间点  
        float P_LayerHeight =  currentLayerHeight / 2;  //该点视线深度为P2点的二分之一  
        float P_HeightMapValue = tex2D(_HeightMap, P).r; //采样深度  
        if(P_LayerHeight > P_HeightMapValue)  
        {  
            P2 = P;  
        }  
        else  
        {  
            P1 = P;  
        }  
    }  
    return (P1 + P2) / 2 - uv;;  
}
```
## 视差闭塞映射（POM = Parallax Occlusion Mapping）

-   相对于浮雕贴图，不同之处在于最后一步
-   浮雕贴图是在确认最后步进之后进行二分查找（在迭代次数比较多的情况下，还是挺耗的）
-   **视差闭塞贴图是在最后步进的两端 uv 值进行采样（下图红色箭头），采样之后再对这两个结果进行插值，插值的结果作为 P 点最终的偏移值**
- ![[Pasted image 20221029213350.png]]
-  优点：

-   相对于浮雕映射，性能更好（最后只做插值，而浮雕要做二分查找）
-   相对于陡视差贴图，精确性更好

-   要求：
-   因为最后要做插值，所以要求表面是相对比较平滑/连续的，如果有莫名的凸起结果可能会出错

## 代码集合
```c
Shader "Unlit/normalmap"  
{  
    Properties  
    {  
        _MainTex ("Texture", 2D) = "white" {}  
        _NormalMap("NormalMap", 2D)= "gray" {}  
        _NormalScale("NormalScale",float) = 1  
        _HeightMap("_HeightMap", 2D) = "blue" {}  
        _HeightScale("HeightScale",float) = 1  
        _SpecularExp("SpecularExp",float) = 1  
        _SpecularScale("SpecularScale",float) = 1  
    }  
    SubShader  
    {  
        Tags { "RenderType"="Opaque" }  
        LOD 100  
  
        Pass  
        {  
            CGPROGRAM  
            #pragma vertex vert  
            #pragma fragment frag  
  
            #include "UnityCG.cginc"  
  
            struct appdata  
            {  
                float4 vertex : POSITION;  
                float4 normal : NORMAL;  
                float4 tangent : TANGENT;  
                float2 uv : TEXCOORD0;  
            };  
  
            struct v2f  
            {  
                float4 pos : SV_POSITION;  
                float2 uv : TEXCOORD0;  
                float3 normal : TEXCOORD1;  
                float3 tangent : TEXCOORD2;  
                float3 bitangent : TEXCOORD3;  
                float4 worldPosion : TEXCOORD4;  
            };  
  
            sampler2D _MainTex;  
            sampler2D _NormalMap;  
            sampler2D _HeightMap;  
              
            float _NormalScale;  
            float _HeightScale;  
            float _SpecularExp;  
            float _SpecularScale;  
  
  
            // 陡峭视察映射光线步进函数  
            float2 SteepParallaxMapping(float2 uv, float3 tangent_viewDir)  
            {  
                float numLayers = 30;   //最大步进次数  
                float layerHeight = 1 / numLayers;  //单层步进高度  
                float2 deltaUV = tangent_viewDir.xy / tangent_viewDir.z * _HeightScale / numLayers; //单次uv偏移量  
  
                float currentLayerHeight = 0;   //步进初始高度（视线深度）  
                float2 currentUV = uv;           //当前UV  
                float2 offsetUV = float2(0, 0);  //初始化偏移量  
                float currentHeightMapValue = tex2D(_HeightMap, currentUV).r;  //采样初始高度信息(采样深度)  
  
                for (int i = 0; i<numLayers; i++)  
                {  
                    if(currentLayerHeight > currentHeightMapValue)  
                    {  
                        return offsetUV;  
                    }  
                    offsetUV += deltaUV;  
                    currentHeightMapValue = tex2D(_HeightMap, currentUV + offsetUV).r;  
                    currentLayerHeight += layerHeight;  
                }  
                return offsetUV;  
            }  
  
            // 浮雕贴图  
            float2 ReliefMapping(float2 uv, float3 tangent_viewDir)  
            {  
                float numLayers = 30;   //最大步进次数  
                float layerHeight = 1 / numLayers;  //单层步进高度  
                float2 deltaUV = tangent_viewDir.xy / tangent_viewDir.z * _HeightScale / numLayers; //单次uv偏移量  
  
                float currentLayerHeight = 0;   //步进初始高度（视线深度）  
                float2 currentUV = uv;           //当前UV  
                float2 offsetUV = float2(0, 0);  //初始化偏移量  
                float currentHeightMapValue = tex2D(_HeightMap, currentUV).r;  //采样初始高度信息(采样深度)  
  
                // 光线步进  
                for(int i = 0;i<numLayers;i++)  
                {  
                     if(currentLayerHeight > currentHeightMapValue)  
                    {  
                        break;  
                    }  
                    offsetUV += deltaUV;  
                    currentHeightMapValue = tex2D(_HeightMap, currentUV + offsetUV).r;  
                    currentLayerHeight += layerHeight;  
                }  
  
                //二分查找  
                float2 P2 = uv + offsetUV;  //步进停止点  
                float2 P1 = P2 - deltaUV;   //停止点的上一个点  
  
                for(int j = 0; j < 20; j++)  
                {  
                    float2 P = (P1 + P2) / 2;   //中间点  
                    float P_LayerHeight =  currentLayerHeight / 2;  //该点视线深度为P2点的二分之一  
                    float P_HeightMapValue = tex2D(_HeightMap, P).r; //采样深度  
                    if(P_LayerHeight > P_HeightMapValue)  
                    {  
                        P2 = P;  
                    }  
                    else  
                    {  
                        P1 = P;  
                    }  
                }  
                return (P1 + P2) / 2 - uv;;  
            }  
              
            v2f vert (appdata v)  
            {  
                v2f o;  
                o.pos = UnityObjectToClipPos(v.vertex);  
                o.uv = v.uv;  
                o.normal = UnityObjectToWorldNormal(v.normal);  
                o.tangent = UnityObjectToWorldDir(v.tangent);  
                o.bitangent = cross(o.normal, o.tangent) * v.tangent.w;  
                o.worldPosion = mul (unity_ObjectToWorld,v.vertex);   
                return o;  
            }  
  
            fixed4 frag (v2f i) : SV_Target  
            {  
                float3 world_LightDir = normalize(UnityWorldSpaceLightDir(i.worldPosion));  
                float3 world_ViewDir = normalize(UnityWorldSpaceViewDir(i.worldPosion));  
                float3 world_HalfVector = normalize(world_LightDir + world_ViewDir);  
                  
                //TBN  
                float3x3 TBN = float3x3 (i.tangent,normalize (i.bitangent),i.normal);  
  
                // NormalMap  
                float3 NormalMap = UnpackNormal((tex2D(_NormalMap, i.uv)));  
                fixed4 col = tex2D(_MainTex, i.uv);  
                NormalMap *= _NormalScale;  
                float3 world_NormalMap = normalize(mul(NormalMap,TBN));  
                  
                // Parallax Mapping  
                /*                
                float height = tex2D(_HeightMap,i.uv).r;                
                float3 tangent_ViewDir = normalize(mul(TBN, world_ViewDir));                
                float2 offsetUV = tangent_ViewDir.xy / tangent_ViewDir.z * height * _HeightScale;                
                i.uv += offsetUV;                
                */                      
                          
                // Steep Parallax Mapping  
                /*                
                float3 tangent_ViewDir = normalize(mul(TBN, world_ViewDir));                
                float2 offsetUV = SteepParallaxMapping(i.uv, tangent_ViewDir);                
                i.uv += offsetUV;               
                 */           
                                      
                 //Relief Mapping  
                float3 tangent_ViewDir = normalize(mul(TBN, world_ViewDir));  
                float2 offsetUV = ReliefMapping(i.uv, tangent_ViewDir);  
                i.uv += offsetUV;  
                  
                  
                float3 Basecolor = tex2D(_MainTex, i.uv);  
                float3 Diffuse = dot(world_NormalMap,world_LightDir) * 0.5 + 0.5;  
                float3 Specular = pow(max(0, dot(world_NormalMap,world_HalfVector)), _SpecularExp) * _SpecularScale;  
                float3 FinalColor = (Diffuse + Specular) * Basecolor;  
                return float4(FinalColor,1);  
            }  
            ENDCG  
        }  
    }  
}
```

# 2.8 FlowMap
[# 【技术美术百人计划】图形 2.8 flowmap的实现——流动效果实现](https://www.bilibili.com/video/BV1Zq4y157c9?p=2&vd_source=9d1c0e05a6ea12167d6e82752c7bc22a)
[[TA101#FlowMap]]
## 什么是 FlowMap
### 1. FlowMap
![[Pasted image 20221030195248.png]]
-   是 Valve 在 2010 年 GDC 中介绍的一种在求生之路 2 中用来实现水面流动效果的技术
-   容易实现，且运算量较小（所以到现在还在被使用）
-   使用了一张被称为 flowmap 的贴图（右图），来达到控制场景中水面流向的效果
### 2. FlowMap 的实质
![[Pasted image 20221030195419.png]]
>发现左图和中图颜色不对应？下一小节会讲原因
- 是一张记录了 2D 向量信息的纹理
-   **理解**：
-   假设有一个 2D 平面，平面上每个点都对应一个向量，这个向量指向这个点接下来要运动的方向。
-   我们通过颜色的色值（**RG 两个通道**）来记录这些向量的信息，就得到了一张 flowmap
-   在 shader 中干扰 uv（偏移 uv），对纹理进行采样，这样就得到了一个模拟流动的效果

### 3. 回顾：UV 映射（纹理映射）

-   对一个贴图进行纹理查找，就要用到 uv 坐标
-   **理解**：
-   如图为 Unity 中的 uv 坐标，类似于 xy 轴，用此 uv 坐标查找右边贴图的颜色值，采样会得到和贴图一模一样的结果
![[Pasted image 20221030195659.png]]
-   如图，如果改变查找时的 uv 坐标，让每一列都有相同的 uv 值，那么采样结果就是右图的条纹状的结果
![[Pasted image 20221030195810.png]]
如图，如果用同一个 uv 值采样的话，结果就会是同一的颜色, 下图采样（0，0）即左下角颜色
![[Pasted image 20221030195841.png]]
-   **也就是说，uv 贴图上颜色相同的地方：意味着采样纹理时使用了同一位置**
### 4. Flowmap 的原理

-   通过所带有的向量场信息对 uv 进行了一个偏移，来干扰我们采样时候的这个过程。

-   如图可以看到，经过 flowmap 发生偏移后，让原本正常的采样变成了一个扭曲的效果
-   **注意**：
-   **不同软件的不同 uv 坐标：UE4 与 Unity 相比，反转了绿通道**，所以使用的 flowmap 也会发生变化。（根据不同的引擎需求进行调整）
![[Pasted image 20221030195932.png]]
### 5. 为什么要使用 flowmap
![[Pasted image 20221030200104.png]]
## FlowMap shader
### 1. 基本流程
-   1. 采样 flowmap 获取向量场信息
-   2. 用向量场信息，使采样贴图时的 uv 随时间变化
-   3. 对同一贴图以半个周期的相位差采集两次，并线性插值，使贴图**流动连续**

### 2. 实现思路

**目标：根据 flowmap 上的值，使纹理随时间偏移**

-   **操作 1**：最简单的 uv 随时间偏移的方法： **uv - time**
- 
-   **关于为什么是相减**：
-   先理解一下相加的情况：模型上的某个点（u，v）+（time，0）
-   可以理解为随着 time 增加，采样到的像素越远
-   这个效果在视觉上可以形容为：更远距离的像素偏移向这个点，也就是说和我们直观认识到的运算法则是**相反**的。
-   用 uv 值作为向量时，是遵守运算法则的
-   uv 偏移并没有改变顶点位置，只是采样到了更远的像素

-   **操作 2**：从 flowmap 获取流动方向
- 
-   从 flowmap 获取流动的方向，再乘 time，就可以达到让某个点根据 flowmap 流动的目的
-   问题：flowmap 不能直接使用
-   解决：将 flowmap 上的色值从[0，1]映射到方向向量的[-1，1]

-   也就是一个乘 2 减 1 的操作，之前推导 mvp 矩阵时也用过这个方法

-   **操作 3**：流动无缝循环，把偏移控制在一定的范围内（随着时间进行，变形太过夸张）
![[Pasted image 20221030201326.png]]
-   构造两个相位相差半个周期的波形函数
![[Pasted image 20221030201416.png]]
-   用相位差半个周期的两层采样进行加权混合，用纹理流动另一层采样来覆盖一个周期重新开始时的不自然情况

**代码**
![[Pasted image 20221030202010.png]]
![[Pasted image 20221030202017.png]]
![[Pasted image 20221030202025.png]]

### 3. 用 flowmap 修改法线贴图
![[Pasted image 20221030201822.png]]
## Flowmap 的制作

### 1.  Flowmap Painter

-   **Flowmap Painter**

-   下载地址：[http://teckartist.com/?page_id=107](http://teckartist.com/?page_id=107)
-   直接上手绘制即可，很简单
-   可以使用反转 UV 通道选项，以便不同符合不同的引擎需求
-   **注意**：使用 Flowmap Painter 绘制得到的贴图为**线性空间下的颜色**，不需要伽马校正。（Unity 里不用勾选 sRGB）
UE 引擎烘焙时要勾选 File Red 选项。

### 2. Houdini（待学习）

待学习，课程介绍了详细流程！

### 3. 注意事项

-   flowmap 贴图的设置：
-   要使用无压缩或高质量
-   确认色彩空间

# 3.7 纹理压缩
## 一、什么是纹理压缩

-   **纹理压缩是**：
-   为了解决内存、带宽问题，专为在计算机图形渲染系统中**存储纹理**而使用的**图像压缩技术**。

-   **区分图片格式和纹理压缩格式**

-   **概念上讲**

-   图片格式：
-   是图片文件的存储格式，通常在硬盘、内存中存储，传输文件时使用
-   例如：jpg、png、gif、bmp

-   纹理压缩格式：
-   是**显卡能直接进行采样**的**纹理数据格式**，通常在向显卡中加载纹理时才使用

-   **原理上讲**

-   图片格式：
-   图片压缩格式是**基于整张图片进行压缩**，像素之间解码过程中**存在依赖关系**
-   无法实现单个像素级的解析，发挥不了显卡的并行能力
-   并且，无论什么格式在显卡解码后都是 RGBA 的纹理格式
-   总结：无法减少显存的占用率，且需要 CPU 解压后才能被 GPU 读取，结果就是：增加了 CPU 的时间和带宽

-   纹理压缩格式：
-   基于块压缩，能够更快的读取像素所属字节块进行解压缩，以支持快速访问
-   “随机访问”：如果渲染一个物体时，需要在某个坐标上采样纹理，那么 GPU 只需要读取该像素所属固定大小字节块，对其进行解压即可。

-   举例理解：
-   如果拿到一张贴图，设置纹理压缩格式：
-   CPU 会按照我们设定的格式进行压缩，然后传递给 GPU 读取

-   如果不设置纹理压缩格式，以图片格式进行：
-   CPU 也会进行压缩，但是会压缩为 RGBA32 格式，但其实这个格式是非常大的，并没有起到压缩的作用
## 二、为什么要使用纹理压缩
![[Pasted image 20221208232051.png]]
-   实际上，使用纹理压缩的原因在上部分**图片格式和纹理压缩格式区别**的部分里已经讲了

-   **图片压缩格式下**

-   无法实现像素级解析，**无法发挥 GPU 并行能力**，无法减少显存的占用率
-   需要在到 GPU 之间使用 CPU 解压缩，增加了 CPU 的时间和带宽

-   **纹理压缩格式下**

-   GPU 可以直接读取贴图，不需要经过中间 CPU 解码/解压缩的步骤
-   还支持“随机访问”

-   总结一下就是：

-   我们需要一种**内存占用既小又能被 GPU 直接读取的格式**（这种格式就是纹理压缩）

- **总结**
-   纹理压缩相对正常图片格式，能够直接被 GPU 采样，发挥 GPU 强大的并行能力，且优化了带宽问题
## 三、常见的纹理压缩格式
![[Pasted image 20221208232125.png]]
>黄字是我们常用的压缩格式 
### 非纹理压缩格式

-   RGBA8888（RGBA32）
-   R、G、B、A 四个通道各占 8位
-   所以一个像素消耗：4 * 8 = 32 位（bit）= 4 字节（byte）

-   RGBA4444（RGBA16）
-   四个通道各占 4 位内存
-   一个像素消耗：4 * 4 = 16 位 = 2 字节

-   RGB888（RBG24）
-   同理，一个像素消耗**:** 3 字节

-   RGB565 (RGB16)
-   一个像素消耗**：**2 字节

-   **总结**
-   带透明通道的，单通道可以是 4 位、8 位
-   不带透明通道的，单通道可以是 8 位，或者在 RGB 分 16 位（565、绿通道多给 1位）
### 1、DXTC 系列

DXTC 系列的纹理压缩格式来源于 S3 公司提出的 S3TC 算法

-   **基本思想**：
-   把 4×4 的像素块压缩成一个 64 或 128 位的数据块

-   **优点**：
-   创建了一个固定大小且独立的编码片段，没有共享查找表或其他依赖关系，简化了解码过程

#### DXT1（BC1）

-   也叫作 BC1
-   **内容**：
-   将 4×4 的像素块压缩成了一个 64 位的数据块，这个 64 位的数据块中包含：

-   其中 32 位是：
-   两个 16 位 RGB（RGB565）颜色。
-   （这两个 RGB 颜色是 4×4 像素块中的两个极端颜色值，然后通过线性插值计算出剩余的两个中间颜色）

-   剩余的 32 位
-   平均分配给了 16 个像素作为颜色值的索引值，每个像素占 2 位

-   **理解**：
![[Pasted image 20221208232506.png]]
![[Pasted image 20221208232510.png]]
-   64 位分别为：
-   其中 32 位是：蓝色的 A、B 两个 16 位 RGB 颜色，它们是极端颜色（格式为 RGB565）

-   极端颜色通过线性插值得到的中间颜色：红色的 C、D

-   另外 32 位是：16 个像素的颜色索引值，每个像素占 2 位（可能是 00 01 10 11，可以分别表示上边的 A、B、C、D 四种颜色）

-   **注意**：

-   **①**存储极端颜色的格式是 RGB565，也就是说绿（G）通道的精度比其他两个通道精度高一些

-   这就是有些人说把信息放绿通道精度更高的原因
-   //补充：多出来的精度给绿通道是因为：人眼对绿色更敏感

-   **②**DXT1 格式适用于不具有透明度信息或者具有一位透明度信息（表示完全透明 or 完全不透明）的贴图

-   对于没有 Alpha 信息的贴图，压缩遵循上文
-   对于有 Alpha 信息的贴图
-   极端颜色插值时，中间颜色只有一个，另一个表示完全透明 or 完全不透明（例如上述例子中，C 为中间颜色，D 表示透明信息）
-   每个像素索引时，极端颜色+中间颜色表示完全不透明，另外一个表示完全透明

-   **DXT1 的压缩率**

-   参照对象：RGB24（DXT1 主要用于没有 Alpha 信息的贴图）
-   DXT1:

-   总数据块为 64 位，16 个像素共用 =>一个像素 4 位

-   所以压缩率为：24 / 4 = 6:1

#### DXT2/3（BC2）

-   也叫作 BC2
-   128 位
-   颜色信息和 DXT1 是一样占用 64 位，多出 64 位用来增加 Alpha 信息

-   Alpha 信息并没有插值，只是单纯的为每一个像素多给 4 位信息，用来记录 Alpha 信息
-   这样一来，每个像素就占 4+4=8 位，（0~3 表示透明信息，4-7 表示颜色信息）

-   **简单来说就是：**
-   相比 DXT1，多了 64 位用来存 Alpha 信息， 16 个像素，每个4位

-   **DXT2 和 DXT3 的区别**：
-   DXT2 是已经完成了颜色与 Alpha 的混合，当透明度发生改变时，直接改变整体颜色值，不再单独进行复合
-   DXT3 的 Alpha 信息相对独立（分开压缩）

-   **DXT2、3 的压缩率**
-   参照对象：RGBA（32 位）
-   总数据块为 128 位，16 个像素共用 =>一个像素 8 位
-   压缩率为：32 / 8 = 4:1

#### DXT4/5（BC3）

-   也被称为 BC3

-   128 位
-   **和 DXT2/3 的区别：**
-   Alpha 信息是通过线性插值得来的：
-   表示颜色信息的 64 位同上
-   多出的 64 位：
-   2 个 8 位的极端值
-   每个像素 3 位的索引值（16*3）

-   **DXT4 和 DXT5 的区别**：

-   同 2 和 3

-   **DXT4、5 的压缩率**

-   同为4:1

#### 扩展知识

-   Unity 将贴图类型选为法线时，会采用 DXTnm 格式

-   它基于 DXT5，会把法线贴图的 R 通道存入 A 通道，然后将 RB 通道清除为1
-   这样就可以把法线 xy 信息分别存入到 RGB 和 A 中进行压缩，**来获得更高的精度**
-   最后再根据 xy 构建出 z 的信息
### 2、ATI 系列
#### ATI1/2（BC4、BC5）

-   ATI1，也被称为 BC4

-   64 位
-   每一个数据块中存储的是单个颜色通道的数据
-   **主要用于存储：高度图、光滑度贴图等单通道信息**
-   ATI1 的压缩方式：

-   和 DXT5 中，对于 Alpha 数据处理一样

-   ATI2, 也被称为 BC5

-   128 位
-   和 ATI1 的区别在于，它存储了两个颜色通道的数据
-   ATI2 的压缩方式：

-   处理方式也是相同的，相当于存储了两个 ATI1 的数据块

-   如果想要节省通道只存储法线 xy 通道时，就可以采用 BC5（ATI2）压缩格式

-   **优点：**
-   因为每个通道都会有自己的索引，会单独压缩，所以**法线贴图的 xy 信息可以比 DXT1 中有更多保真度**

-   **缺点：**
-   需要使用两倍内存，需要更多的带宽才能将纹理传递到着色器中

-   **压缩比：**
-   ATI1：
-   参照对象：单通道 8 位
-   总数据块为 64 位，16 个像素，所以每个像素 4 位
-   压缩比为：8 / 4 =2:1

-   ATI2：
-   参照对象：两个通道 16 位
-   总数据块为 128 位，16 个像素，所以每个像素 4 位
-   压缩比为：16 / 8 = 2:1

-   //注：一些资料上显示 BC4 和 5 的压缩比为 4/1，查了很多还是没找到详细资料。这一块待定，日后搞清楚了回来修改

-   弹幕有同学提出：维基百科中说，4:1的压缩比，是因为单个像素当作 32 位的大小来计算了

#### BC6/7

-   BC6 和 BC 仅在 D3D11 及以上图形硬件中受到支持
-   他们每个块占用 16 字节

-   BC6
-   针对 RGB 半精度浮点数数据进行压缩
-   **是专门针对 HDR（高动态范围）图像设计的压缩算法**
-   压缩比为6:1

-   BC7
-   针对 8 位 RGB 或 RGBA 的图像进行压缩
-   **是专门针对 LDR（低动态范围）图像设计的压缩算法**，该格式用于高质量的 RGBA 压缩，可以显著减少由于压缩法线带来的错误效果
-   压缩比为3:1

-   **一般我们使用 BC7 给端游高质量图像进行压缩**
-   Reference：
-   BC6 和 BC7 的官方原理说明
-   [https://docs.microsoft.com/zh-cn/windows/uwp/graphics-concepts/bc6h-format](https://docs.microsoft.com/zh-cn/windows/uwp/graphics-concepts/bc6h-format) [https://docs.microsoft.com/zh-cn/windows/uwp/graphics-concepts/bc7-format](https://docs.microsoft.com/zh-cn/windows/uwp/graphics-concepts/bc7-format)

### 3、ETC 系列

-   DirectX 选择 DXTC 作为标准压缩格式，而 OpenGL 选择了爱立信研发的 ETC 格式
-   **几乎所有安卓设备都支持 ETC 格式，所以它在移动端应用广泛**

-   基本思想：
-   ETC 的方案同样将 4×4 的像素单元压缩成 64 位数据块，同时，将像素单元水平或竖直朝向分为两个区块，每个像素颜色等于基础颜色加上索引指向的亮度范围
![[Pasted image 20221208234341.png]]
-   总结：每个区块中有 12 位用来存储颜色信息（12*2），16 位存储其 8 个像素的索引（每个像素 2 位，16*2），4 位存储亮度索引（4*2）
#### ETC1
![[Pasted image 20221208234503.png]]
-   **亮度索引值**：（上表，水平方向）
-   每个区块的亮度索引值（3 位，0-7）会从 8 个亮度索引值中获取当前像素单元的亮度表
-   //注：课程里讲的是 4 位，0-15 个亮度索引值，但资料中显示的是 3 位索引值，表中也是，正确性存疑

-   **像素索引值**（上表，竖直方向）：
-   每个像素的像素索引值（2 位，0-3）可以从亮度表的四个值中选取对应的亮度补充值

-   **最终的颜色 = 12 位基础颜色信息 + 亮度补偿值**
-   补充：

-   **原理：**

-   将 4×4 的像素块编码为 2×4 或者 4×2 像素的两个块
-   每个块指定一个基色，每个像素的颜色铜鼓偶一个编码为相对于这个基色偏移的灰度值确定（上面提到的亮度）

-   **位数占比**：

-   亮度索引 3 位*2
-   像素索引 2 位*16
-   基础颜色 12 位（444*2，或者 555+333）*2
-   flip1 位（控制水平或者竖直划分）*2
-   总位数 = 3*2 + 2*16 + 12*2 + 1*2 = 64 位
-   //注：*2 是因为有两个块，*16 是因为有 16 个像素

-   **压缩率**：

-   参照标准：RBG24
-   总共有 64 个数据块，针对 16 个像素，也就是每个像素 4 位
-   压缩比 = 24 / 4 = 6:1

-   **对于 ETC1 不支持 Alpha 通道的解决方案**
-   采用两张纹理混合的方式

-   **ETC1 的适用情况**

-   长宽为 2 的幂次的贴图
-   不适用于带透明通道的贴图
-   适用于基本所有安卓设备

#### ETC2

-   TEC2 是 ETC1 的扩展，支持了 Alpha 通道（内存占用大于 ETC1）
-   硬件要求 OpenGL ES3.0 和 OpenGL4.3以上

### 4、ASTC

-   ASTC 是由 ARM 和 AMD 联合开发的纹理压缩格式
-   **优点：**
-   可以根据不同图片选择不同压缩率的算法
-   图片长宽不需要是 2 的次幂
-   同时支持 HDR 和 LDR

-   **缺点：**
-   兼容性不够完善
-   解码时间较长
-   无法在 iphone6 以下的设备运行

-   **基本思想：**
-   同样是基于块的压缩算法，与 BC7 类似
-   数据块大小固定为 128 位
-   块中的像素数量可变，从 4×4 到 12×12 像素都有

-   **每个数据块中存储了两个插值端点**
-   存储的不一定是颜色信息，也可能是 Layer 信息，这样可以用来对 Normal 或 Alpha 进行更好的压缩（根据贴图类型进行针对性压缩）

-   **块中的每个纹素，存储其对应插值点的权重值**
-   权重值数量可以少于纹素数量，可以通过插值得到每个纹素的权重值，再进行颜色计算

-   **128 位数据块中存储的信息：**
-   11 位，权重、高度信息、特殊块标识
-   2 位，Part 数量
-   4 位，16 中插值端点模式（LDR/HDR、RGB/RGBA）
-   111 位，插值端点信息、纹素权重值、配置信息

### 5、PVRTC

-   PVRTC 是由 Imagination 公司专为 PowerVR 显卡设计的压缩格式（iphone、ipad，部分安卓机）
-   不是基于块的算法，而是将图像分为了低频和高频信号

-   低频信号由两张低分辨率图像 AB 组成
-   高频信号则是一张记录了每个像素混合的权重值的全分辨率低精度的调制图像
-   解码时，AB 图像经过双线性插值放大，然后根据调制图像权重进行混合

-   **压缩原理**

-   分为 4-bpp 和 2-bpp（bpp = Bit Per Pixel，即每个像素占的位数）
-   **4-bpp 为例：**

-   把 4×4 的像素单元压成一个 64 位数据块

-   64 位数据块中包含了 A、B 两张图（在原图基础上压缩到 1/4 的低分辨率图像）
-   不同模式下每个像素调制数据可以得到不同的混合值，根据这个混合值用 A 和 B 混合得出最终颜色值

-   位数占比：

-   32 位的调制数据（2*16）
-   1 位的调制标志（也称为模式）
-   15 位的颜色 A（554 或 4433），1 位颜色 A 的不透明标志
-   14 位颜色 B（555 或 4443），1 位颜色 B 的不透明标志
-   共计：32 +1 + 16 + 15 = 64 位

-   **压缩率**
-   以 RGB 为参照标准
-   压缩率 = 24 / (64/16) = 6:1
-   以 RGBA 为参照标准
-   压缩率 = 32 / (64/16) = 8:1

-   **2-bpp**
-   把一个 8×4 的像素单元压成了 64 位数据块

## 四、总结

### 1、画质比较

-   RGBA > ASTC 4×4> ASTC6×6 > TEC2 ≈ ETC1
-   //注：画质较为主观，且不同的贴图针对不同压缩格式也不同，仅供参考

### 2、压缩比

-   DXT1 6:1
-   DXT2/3 4:1
-   DXT4/5 4:1
-   ATI1 4:1
-   ATI2 4:1
-   BC6 6:1
-   BC7 3:1
-   ETC1 6:1
-   PVRTC 6:1
-   ASTC 4:1~35.95:1

### 3、实际应用中的选择

**PC**
-   ① 低质量使用 DXT1 格式不支持 A 通道，使用 DXT5 格式支持 A 通道；
-   ② 高质量使用 BC7 格式，支持 A 通道；

**安卓**
-   ① 低质量使用 ETC1 格式，但不支持 A 通道；
-   ② 低质量使用 ETC2 格式，支持 A 通道，需要在 OpenGL ES 3.0/OpenGL 4.3 以上版本；
-   ③ 高质量使用 ASTC 格式，需要在 Android 5.0/OpenGL ES 3.1 以上版本；

**IOS**
-   ① 高质量使用 ASTC 格式，需要 Iphone6 以上版本；
-   ② 低质量使用 PVRTC2 格式，支持 Iphone6 以下版本；

**补充**
-   实际手机端项目中，我们比较常用 ASTC（安卓和 IOS 通用）
-   英伟达和 Unity 官方对于不同类型贴图给出了不同的压缩方案建议，感兴趣的同学可以看下：

-   Using ASTC Texture Compression for Game Assets | NVIDIA Developer
-   Unity - Manual: Recommended, default, and supported texture compression formats, by platform (unity3d. com)

五、其他补充

## ①常见分辨率及纹理压缩格式下的内存占比分析

-   在 Ben Cloward 大佬的视频里也有提过几嘴纹理压缩的问题，有个表还挺直观，在这里也摘过来：

-   具体可以看看我这篇笔记：[https://www.yuque.com/sugelameiyoudi-jadcc/okgm7e/wypves](https://www.yuque.com/sugelameiyoudi-jadcc/okgm7e/wypves)