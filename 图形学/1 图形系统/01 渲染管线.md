
---
title: 01 渲染管线
aliases: []
tags: []
create_time: 2023-06-19 20:09
uid: 202306192009
banner: "![[Pasted image 20230619222654.png]]"
banner_x: 0.09704
---

> [!NOTE]
> 本文是对 RTR4 和龙书渲染管线部分内容的总结
 
# 1 概述

## 关于Pipleline

当我们谈到管线时，我们指的是一个**由多个阶段组成的过程，每个阶段都完成任务的一部分**。在现实世界中，流水线的概念在许多不同的领域中都有应用，比如工厂的生产线和快餐厅的厨房等。

管线的各个阶段都是平行的，这意味着**各个阶段依次依赖于上一个阶段的输出**。理想情况下，将一个非管道系统划分为 n 个流水线阶段可以提供 n 倍的加速。这种性能提升是使用流水线的主要原因。

例如，一系列人可以快速准备大量的三明治，其中一个人负责准备面包，另一个人添加肉，另一个人添加浇头。每个人将结果传递给排队的下一个人，立即开始制作下一个三明治。如果每个人完成任务需要 20 秒，那么每 20 秒可以制作三个三明治。

管线的速度取决于管线中最慢阶段的速度，最慢的阶段被称为**瓶颈 (bottleneck)**。此时，其他的阶段会等待瓶颈的工作完成，称其他阶段此时处于 **starved** 状态。因此，优化管线中最慢的阶段可以提高整个管线的性能。

## 混乱的翻译

> [!NOTE] Pipeline 的翻译
>Pipleline 也叫流水线，图形渲染领域，更多翻译为**“管线”** , 后文也采用管线的译法。
>
>

**渲染管线（Rendering Pipeline）、图形管线、图形流水线(Graphics Pipeline)、图形渲染管线(Rendering Graphics Pipeline)** 其实是一个东西，我更习惯翻译成渲染管线。

- 对于渲染管线，将其分为**按功能阶段性划分的渲染管线**和 **GPU 硬件渲染管线**（本质相同，都是硬件渲染管线）
- 对于游戏制作中更细分的渲染管线，我将其称为**渲染流程（Rendering Processing）**，如下为天涯明月刀 OL 的渲染流程。**渲染流程是个范围更大，更细节的管线。**
![[Pasted image 20230619220403.png|500]]
## 渲染管线
**渲染管线（graphics rendering pipeline）

在计算机图形学中，我们使用渲染管线来实现将 3D 场景转换成 2D 图像的过程。如果给出一台具有确定位置和朝向的摄像机以及某个场景的几何描述，那么渲染管线则是以摄像机为视角进行观察，并据此生成给定 3D 场景 2D 图像的一整套处理步骤。

具体来说，把渲染管线想象为一个工厂里的流水线，里面有不同的**加工环节（渲染阶段）**，可以根据用户需求对每个环节灵活**改造或拆卸（可编程或可配置）**，以此把**原始材料（CPU 端向 GPU 端提交的纹理等资源以及指令等）** 加工为成品出售给**消费者（在 GPU 端，资源流经流水线里的各个阶段, 经指令的调度对其进行处理，最终计算出像素的颜色，将其呈现在用户屏幕上）**。

事实上，渲染管线是种模型，将 3D 场景变换至 2D 场景的处理流程抽象分离为不同的流水线阶段，供用户使用。其本质即指令从 CPU 端的应用程序层发送至 API 运行时、驱动层及至 GPU 端（包括二者间的通信，连接都靠 PCle 接口，实质上就是围绕这种总线传递数据），资源数据在内存与显存间游走，最后是 GPU 内部各种引擎、缓存、命令队列等根据指令配合运作将数据转化为显示器可视信号。


# 2 按功能性阶段划分

在划分渲染管线阶段之前，首先需要区分**渲染管线的功能性阶段**和 **GPU 硬件管线阶段**区分。
- **功能性阶段是概念性的，是我们为了给一个渲染流程进行基本的功能划分而提出来的。**
- **GPU 硬件管线则是硬件层上真正用于实现上述功能的流水线**。
- 一个图形单元 / Core 可能处理多个功能性阶段，一个功能性阶段可能也会拆分成几个硬件单元。

实时渲染管线（real-time rendering pipeline）一般分为如下四个功能性阶段：**应用程序阶段 (Application)**、**几何处理阶段 (Geometry Processing)**、**光栅化阶段 (Rasterization)** 和**像素处理阶段 (Pixel Processing)**。
- **这些阶段中的每个阶段本身也可以是一条管线**（如本书描述的几何处理阶段），这意味着它会由几个子阶段组成。
- 这些阶段也可以是（部分的）并行化阶段（如本书描述的像素处理阶段）
- 本书中应用程序阶段是单个过程，但是该阶段也可以进行管线化或并行化。
- 需要注意的是，光栅化阶段可以调用到图元（如三角形）内部的像素。

![[Pasted image 20230619202118.png|450]]

细节：
![](<images/1683366278896.png>)
## 1 应用程序阶段 

> [!NOTE] 关联
> 产生 DrawCall


> [!summary] 任务
> **主要任务**：输入装配
> **其他任务**：粗粒度剔除 (将完全不可见的物体剔除)，碰撞检测、处理其他源输入 (键盘、鼠标等)、加速算法...

应用程序阶段是**完全可控制**的，因为它**在 CPU 上执行**。并且可以在之后对它进行修改以提高性能表现，另外，此处的修改也会影响后续阶段的性能表现。

输入装配阶段会从显存中读取几何数据 (顶点和索引)，再将它装配为**几何图元 (geometry primitive)**。简单来说，应用程序阶段通过索引将顶点装配在一起，构成图元传递给几何处理阶段。图元拓扑可以分为：点列表，线条带，线列表，三角形带和三角形列表。这通常在多个处理器核心上执行，被称为**超标量结构 (superscalar construction)**。


另外，应用程序阶段也可以通过计算着色器在 GPU 上运行。

> [!quote] compute shader
> 某些应用程序阶段的工作可以由 GPU 执行，使用一种被称为计算着色器（compute shader）的单独模式。此模式会将 GPU 视为高度并行的通用处理器，而忽略它专门用于渲染图形的特殊功能。

## 2 几何处理阶段

> [!NOTE] 关联
> 该阶段涉及 [[02 视图变换]]

> [!summary] 任务
> **把顶点坐标变换到屏幕空间中，再交给光栅器进行处理**
> 通过对输入的渲染图元进行多步处理后，这一阶段将会输出屏幕空间的二维顶点坐标、每个顶点对应的深度值、着色等相关信息，并传递给光栅化阶段。

**几何处理阶段在 GPU 上运行，它处理应用程序阶段发送的渲染图元，负责大部分的逐三角形和逐顶点操作。**

几何处理阶段可以细分为 4 个子阶段：**顶点着色阶段 (Vertex Shading)**，**投影阶段 (Projection)**，**裁剪阶段 (Clipping)** 和**屏幕映射阶段 (Screen Mapping)**。

![[1683366277741.png]]

### 顶点着色阶段

> [!NOTE] 关联
> 该阶段涉及**顶点着色器、曲面细分着色器、几何着色器 **
> [[3 进阶应用#3.3 曲面细分与几何着色器]]


> [!summary] 任务
> **主要任务：**
>1. 计算顶点位置，通过 MVP 矩阵从模型空间变换到齐次裁剪空间
>2. 传递后续流水线需要的用来插值的数据，如法线和纹理坐标

完成顶点着色阶段的两个主要任务后，在 GPU 上可以按照以下顺序进行几个**可选阶段**：**曲面细分**（tessellation），**几何着色**（geometry shading）和**流输出**（stream output）。
并不是所有 GPU 都支持它们，它们彼此独立，并且一般来说不常用

剩下三个阶段不再赘述，就是在执行视图变换的过程。
## 3 光栅化阶段


> [!summary] 任务
> 根据几何处理阶段传来的经过变换和投影的顶点及其关联的着色数据，找到需要渲染的图元（例如一个三角形）内的所有像素，将生成的片元传递给像素处理阶段。
>


> [!NOTE] 片元和像素
> 片元是在图元经过光栅化阶段后，被分割成一个个像素大小的基本单位。片元其实已经很接近像素了，但是它还不是像素。片元包含了比 RGBA 更多的信息，比如可能有深度值，法线，纹理坐标等等信息。片元需要在通过一些测试（如深度测试）后才会最终成为像素。可能会有多个片元竞争同一个像素，而这些测试会最终筛选出一个合适的片元，丢弃法线和纹理坐标等不需要的信息后，成为像素。

光栅化阶段**在 GPU 上执行**。

光栅化阶段的目标是找到处于图元 (三角形) 内部的所有像素，进而将 2D 坐标顶点转为屏幕上的像素，**每个像素附带深度和其他着色信息，它们一并传入 pixel**。它需要对上一个阶段得到的逐顶点数据 (例如纹理坐标 、顶点颜色等) 进行插值，然后再进行逐像素处理。

光栅化阶段分为两个子阶段：**三角形设置 (Triangle Setup)** 和**三角形遍历 (Triangle Traversal)**。

![[Pasted image 20230619204802.png]]
### 三角形设置阶段

计算三角形网格信息，例如三角形顶点坐标和边界表达式。

**edge 函数**用于确定一个像素中心或其他 sampler 是否在一个三角形内，硬件上会对每个三角形边缘应用一个 edge 函数，它基于直线方程。

![](<images/1683366278256.png>)

GPU 会在三角形设置阶段计算三角形上的常数因子，以便三角形遍历阶段能够有效地进行 (edge 方程的 a, b, c 常量)。并且，还会计算与属性插值相关的常量。 **总之，它就是处理前面阶段传递的数据，为三角形遍历阶段做准备。**

### 三角形遍历阶段

> [!NOTE] 关联
>1. 采样
> 2. 透视校正插值（perspective-correct interpolation）也发生在该阶段

> [!summary] 任务
> 检查像素是否在三角形内（采样），如果在的话就会生成一个**片元 (fragment)**。
> 判断方法有多种，由程序员决定。

找到哪些像素被三角网格覆盖的过程就是三角形遍历，这个阶段也被称为**扫描变换 (Scan Conversion)** 。

在三角形遍历阶段，片元的几何处理阶段传递的值会进行插值。GPU 采用**重心坐标系**来对值进行插值。重心坐标系的性质是，每个值的系数与三角形重心坐标顶点系数相同。因此，GPU 可以通过插值顶点的值来计算任意像素的值，并且这些值在三角形内部会进行插值计算。 重心坐标插值的具体计算这里就不展开了。

![](<images/1683366278379.png>)

## 4 像素处理阶段（逐片元阶段）

> [!summary] 任务
> 处理光栅化阶段发送过来的在图元内部的片元序列。GPU 会对每个片元进行像素操作，如颜色和深度的计算、纹理采样、混合等。最终，这些像素被组合成最终的图像（最终呈现在屏幕上的包含 RGBA 值的图像最小单元就是像素）

像素处理阶段**在 GPU 上执行**，它主要

像素处理阶段可以分为两个子阶段：**像素着色阶段 (Pixel Shading)** 和**合并阶段 (Merging)**。

![[Pasted image 20230619204802.png]]

### 像素着色阶段

> [!NOTE] 关联
> 该阶段使用像素着色器


> [!summary] 任务
> 使用光栅化阶段传递的插值后的数据以及纹理计算像素颜色，将计算的颜色传递给合并阶段


![](<images/1683366278498.png>)

需要注意的是，纹理可以认为是一种独立于” 插值 “数据的一种资源。由于顶点和像素着色器一般数据都存在更小更快的 **L1 缓存 (L1 Cache)** 中，但纹理存储在更大更慢的 **L2 缓存 (L2 Cache)** 中，因此 **Warp**(线程组，可以简单认为 Shader 程序通过 Warp 单元指令执行)中的指令可以分为两种，不产生延迟的 (寄存器的算数运算) 和产生延迟的(纹理访问)。 为了高效的并行，需要克服内存读取延迟。每个片元的本地寄存器会留出一点存储空间。Warp 首先执行不产生延迟的指令，并将结果存储在 LD/ST 单元中。当遇到产生延迟的指令时，快速切换到其他片元执行其他任务。这个过程会递归进行，直到最后一个片元。然后返回到第一个片元，取出产生延迟指令的结果。 如果 Shader 中使用的寄存器越多，Warp 留给片元的空间就越少。当遇到内存延迟时，可能会出现没有可切换的 Warp 而被迫等待的情况，这种情况称为占用率过高。

### 合并阶段

> [!NOTE] 关联
> **该过程的测试是对片元操作的**，合并完成后显示在屏幕上的才是像素：
> 深度测试
> 模板测试
> 混合
> 缓冲区

> [!summary] 任务
> **将像素着色阶段产生的片元颜色与当前存储在缓冲区中的颜色进行组合**
>1.  决定每个片元的可见性。这涉及了很多测试工作，例如深度测试、模板测试等。
[[3 进阶应用#3.1.1 StencilTest 模板测试]] 
[[3 进阶应用#3.1.2 深度测试]]
> 
>2. 如果一个片元通过了所有的测试，就需要把这个片元的颜色值和已经存储在颜色缓冲区中的颜色进行合并，或者说是混合。
[[3 进阶应用#3.2 混合模式及剔除]]

合并阶段，又称 **ROP 阶段**，代表 “光栅操作管线”（raster operations pipeline）或 “渲染输出单元”（render output unit）。

与着色阶段不同，**执行此阶段的 GPU 子单元通常不是完全可编程的**。但是，它是**高度可配置**的，可实现各种效果。

**通过一系列测试决定每个片元的可见性：**

![[Pasted image 20221030230409.jpg]]

流程：
-   **像素所有权测试→裁剪测试→透明度测试→模板测试→深度测试→透明度混合**

-   **PixelOwnershipTest（像素所有权测试）：**
-   简单来说就是控制当前屏幕像素的使用权限
-   举例：比如 unity 引擎中仅渲染 scene 和 game 窗口，即只对 scene 和 game 窗口部分的像素具有使用权限

-   **ScissorTest（裁剪测试）：**
-   在渲染窗口再定义要渲染哪一部分，默认全部渲染，可以自己控制。
-   和裁剪空间一起理解，也就是只渲染能看到的部分
-   举例：只渲染窗口的左下角部分

-   **AlphaTest（透明度测试）**
-   提前设置一个透明度阈值
-   只能实现不透明效果和全透明效果
-  举例：设置透明度 a 为 0.5，如果片元大于这个值就通过测试，如果小于 0.5 就剔除掉


-   **StencilTest（模板测试）**

-   **DepthTest（深度测试）**

-   **Blending（透明度混合）**
-   可以实现半透明效果

-   完成接下来的其他一系列操作后，我们会将合格的片元/像素输出到**帧缓冲区（FrameBuffer）**，最后渲染到屏幕上。


我们的屏幕显示的就是颜色缓冲区中的颜色值。但是，为了避免我们看到那些正在进行光栅化的图元，GPU 会使用**双重缓冲 (Double Buffering)** 的策略。这意味着，场景的渲染发生在后台缓冲区的屏幕外，即在**后置缓冲 (Back Buffer)** 中。一旦场景已经被渲染到了后置缓冲中， GPU 就会交换后置缓冲区和**前置缓冲 (Front Buffer)** 中的内容，而前置缓冲区是之前显示在屏幕上的图像。由此，保证了我们看到的图像总是连续的。

帧缓冲区（framebuffer）通常由系统上的所有缓冲区组成。
# 3 GPU 硬件渲染管线
DX12 的硬件管线：
[[DX12理论#3 渲染管线]] 

# 4 前向/延迟渲染路径

## 渲染路径

什么是渲染路径（Rendering Path）

-   **决定光照的实现方式**。（也就是当前渲染目标使用**光照的流程**）

## 渲染方式

首先看一下两者的直观的不同
![[Pasted image 20221208214124.png]]

### 前向/正向渲染 Forward Rendering

一句话概括：每个光照都计算

#### 1 流程
![[Pasted image 20221208214244.png]]
-   如图所示，**流程**为：

-   待渲染几何体 → 顶点着色器 → 片元着色器 → 渲染目标
-   在渲染每一帧的时，每一个顶点/片元都要执行一次片元着色器代码，这时需要将所有的光照信息传到片元着色器中。

-   虽然大部分情况下的光照都趋向于小型化，而且照亮区域也不大，但即便是离这个像素所对应的世界空间的位置很远的光源，光照计算还是会把所有的光源考虑进去的。
-   **简单来说就是不管光源的影响大不大，计算的时候都会把所有光源计算进去**，这样就会造成一个很大的浪费

#### 2 规则（如何渲染每一帧的）和注意事项

-   **发生在顶点处理阶段，会计算所有顶点的光照**。全平台支持

-   规则 1：最亮的几个光源会被实现为逐像素光照
-   规则 2：然后就是，最多四个光源会被实现逐顶点光照
-   规则 3：剩下的光源会实现为效率较高的**球谐光照（Spherical Hamanic）**，这是一种模拟光照。SH 光照可以被非常快速地渲染，它只消耗很少的 CPU 性能，几乎不消耗 GPU 性能。并且增加 SH 灯光的数量不会影响性能的消耗。

一个灯光是逐像素光照还是其他方式渲染取决于以下几点：
（1）渲染模式（Render Mode）设置为 Not Important 的灯光总是以逐顶点或者 SH 的方式渲染。
（2）渲染模式（Render Mode）设置为 Important 的灯光总是逐像素渲染。
（3）最亮的平行光总是逐像素渲染。
（4）如果逐像素光照的灯光数量少于项目质量设置中 Pixel Light Count（最大像素光照数量），那么其余比较亮的灯光将会被逐像素渲染。
（5）最后剩下的光源，按照规则 2 或 3。

**Render Mode 设置：** 默认为 Auto，Unity 会根据灯光的亮度以及与物体的距离自动判断该灯光是否重要。
![[Pasted image 20230615135723.png|350]]

**Pixel Light Count 设置：**
![[Pasted image 20221208214300.png|450]]

-   所以，如果一个物体受到 n 个光源影响，那么每个片元着色器执行代码时，都必须把 n 个光源传递给着色器中进行计算。
![[Pasted image 20221208214311.png]]
#### 3 两种 Pass 

![[Pasted image 20230615140142.png|350]]
一个物体受到 A-H 共 8 个灯光照射，假设所有灯光有相同颜色和强度，并且它们的渲染模式为自动。

最终这 8 个灯光的渲染模式如图所示，由于 A-D 这 4 个灯光距离物体更近，因此亮度更亮，会逐像素渲染，然后最多 4 个灯光（D-G）逐顶点渲染，最后剩余的灯光（G-H）以 SH 渲染。
![[Pasted image 20230615140211.png|400]]

灯光 D 既是逐像素照明又是逐顶点照明，灯光 G 既是逐顶点照明又是 SH 照明，这是因为当物体或者灯光移动的时候，不同渲染模式的灯光交界处会出现明显的缺陷，为了避免这个问题，**Unity 将不同的灯光组之间进行了重叠**。

**前向渲染有两种 Pass：Base Pass 和 Additional Pass**

![image-20220707190035317](image-20220707190035317.png)

1. Base Pass 中的渲染计算包含一个逐像素的平行光和所有逐顶点或 SH 的灯光，并且也会包含所有来自于 Shader 的光照贴图、环境光和自发光。BasePass 中的平行光默认支持投射阴影，
2. 其他逐像素的灯光会在额外的 Additional Pass 中渲染，每一个灯光会产生一个额外的 Pass。在 Additional Pass 中的光源默认没有阴影效果，可以使用 `multi_compile_fwdadd_fullshadows` 编译指令代替 `multi_compile_fwdadd` 编译指令，为点光源和聚光灯开启阴影效果。
3. Additional Pass 中还要开启混合模式，因为我们希望每个 Additoinal Pass 可以和上次一的光照结果在帧缓存冲进行叠加，从而得到最终的有多个光照的渲染结果。如果没有开启和设置混合模式 Blend One One 或其他，那么 Additional Pass 的渲染结果会覆盖掉之前的渲染结果，看起来好像该物体之受该光源的影响。
4. 对于前向渲染来说，一个 Unity Shader 通常会定义一个 Base Pass (Base Pass 也可以定义多次，例如需要双面渲染等情况）以及一个 Additional Pass。**一个 Base Pass 仅会执行一次** (定义了多个 Base Pass 的情况除外)，**而一个 Additional Pass 会根据影响该物体的其他逐像素光源的数目被多次调用，即每个逐像素光源会执行一次 Additional Pass。**

### 延迟渲染 Deferred Rendering

一句话概括：先不计算光照，延迟到最后再一起计算

#### 1、什么是延迟渲染

-   主要用来解决大量光照渲染的方案
-   延迟渲染的实质是：
-   先不要做迭代三角形做光照计算，而是先找出来你能看到的所有像素，再去迭代光照。
-   直接迭代三角形的话，由于大量三角形是看不到的，会造成极大的浪费。

#### 2、流程

![[Pasted image 20221208215309.png]] -   流程为：待渲染几何体 → 顶点着色器 → 片元着色器（**写入颜色，但不进行光照计算**）→  MRT 多重渲染目标 → 光照计算（全部执行逐像素渲染） → 渲染目标

过程可以拆分为**两个 pass：**
-   第一个 pass：**几何处理通路**。
    -   首先将场景渲染一次，获取到的待渲染对象的各种几何信息存储到名为 G-buffer 的缓冲区中，这些缓冲区用来之后进行更复杂的光照计算。
    -   由于有深度测试，所以最终写入 G-buffer 中的，都是离摄像机最近的片元的集合属性，这就意味着，在 G-buffer 中的片元必定要进行光照计算。
-   第二个 pass：**光照处理通路**。
    -   这个 pass 会遍历所有 G-buffer 中的位置、颜色、法线等参数，执行一次光照计算。

#### 3、一些注意事项

-   **G-buffer 的概念**

-   G-Buffer，全称 Geometric Buffer ，译作**几何缓冲区**，它主要用于**存储每个像素对应的位置（Position），法线（Normal），漫反射颜色（Diffuse Color）以及其他有用材质参数。**

-   根据这些信息，就可以在像空间（二维空间）中对每个像素进行光照处理。

-   如图为一个典型的 G-buffer
![[Pasted image 20221208215538.jpg]]

-   UE4 默认使用的是延迟管线

-   我们在视图模式---缓冲显示---总览，就可以看到所有 G-buffer 的预览
![[Pasted image 20221208215548.png]]

-   **延迟渲染**<font color="#ff0000">不支持透明物体的渲染</font>，因为没有深度信息，所以渲染透明物体时引擎会自动使用前向渲染处理
- 延迟着色不支持正交投影，当摄像机使用正交投影模式的时候，摄像机会自动使用前向渲染。
> [!NOTE] MRT 现在已经支持 MSAA 抗锯齿！
> DX10.1 支持了带 MSAA 的 MRT，很多老文章说延迟渲染不支持抗锯齿是因为十几年前 DX9 时代的 MRT 不支持 MSAA

-   伪代码
- ![[Pasted image 20221208215555.png]]
-   RT（G-buffer）相当于把整个屏幕的信息绘制到一个图中，每个 RT 都可以写到一个 G-buffer 中
-   G-buffer 中的数据都是 2D 的，所以我们的光照计算就相当于一个**2D 的光照后处理**
### 三、不同渲染路径的特性

#### 1、后处理方式不同

-   如何需要深度信息进行后处理的话

-   前向渲染需要单独渲染出一张深度图
-   延迟渲染直接用 G-buffer 中的深度图计算

#### 2、着色计算不同（shader）

延迟渲染因为是最后统一计算光照的，所以只能算一个光照模型（如果需要其他光照模型，只能切换 pass）

## 不同渲染路径的优劣

### 前向渲染的优点、缺点

**优点**

-   1. 支持半透明渲染
-   2. 支持使用多个光照 pass
-   3. 支持自定义光照计算方式

-   （延迟渲染是渲染到 Gbuffer，再一起计算光照，所以不支持每一个物体用单独的光照方式计算）

**缺点**

-   1. 光源数量对计算复杂度影响巨大
-   2. 访问深度等数据需要额外计算（需要再渲染一张深度图）

### 延迟渲染的优点、缺点

**优点**

-   1. 大量光照场景的情况下，优势明显
-   2. 只渲染可见像素，节省计算量
-   3. 对后处理支持良好（例如深度信息：直接拿 G-buffer 中的就行）
-   4. 用更少的 shader（所有的物体光照模型都一样，很多东西不用再定义了）

缺点
-   1. 对 MSAA 支持不友好
-   2. 透明物体渲染存在问题（深度问题，只渲染力物体最近的物体，渲染透明度时会出现问题）
-   3. 占用大量的显存带宽，所以移动端用得较少（原神是延迟渲染）

-   涉及一个 clear 的操作，如果不清理的话，后边可以继续获取到
-   每一帧都需要几张 rt 在显存中传输、清理等，会更耗带宽
-   4. 只能使用同一个光照 pass

## 其他部分

### 1 渲染路径的设置

-   Project Setting 中进行设置
![[Pasted image 20221208221443.png]]
![[Pasted image 20221208221457.png]]
### 2 TBDR（分块延迟渲染）
针对移动端的优化

-   有两个 TBDR，名字一样，内容不同

-   第一个：
- 是 SIGGRAPH2010 提出的，作为传统 Defferred Rendering 的另一种主要改进，**分块延迟渲染（Tile-Based Deferred Rendering，TBDR）旨在合理分摊开销（amortize overhead）**，自 SIGGRAPH 2010 上提出以来逐渐为业界所了解。基于延迟渲染的优化方式，通过分块来降低带宽内存用量（解决带宽和内存问题）
![[Pasted image 20221208221615.png]]
延迟渲染的分块，把整个图像分为很多块，再一块一块的渲染

-   第二个：
-   PowerVR 基于手机 GPU 的 TBR 框架提出的改进，通过 HSR 减少 Overdraw
-   TBDR 这个架构是 PowerVR 提出来的对 TBR 的一次改进，在 TBR 的基础上再加了一个 Deferred。
-   通过做一些可见性测试来减少 Overdraw
-   **涉及手机 GPU 架构，和延迟渲染没什么关系**
[[3 进阶应用#3.7 现代移动端的TBR和TBDR渲染管线]]
### 3 其他渲染路径

**延迟光照（Light Pre-Pass / Deferred Lighting）**

-   减少 G-buffer 占用的过多开销，支持多种光照模型

-   和延迟渲染的区别：
-   用更少的 buffer 信息，着色计算的时候用的是 forward，所以第三步开始都是前向渲染（可以对不同的物体进行不同的光照模型）

**Forward+（即 Tiled Forward Rendering，分块正向渲染）**

-   减少带宽，支持多光源，强制需要一个 preZ
-   通过分块索引的方式，以及深度和法线信息来到需要进行光照计算的片元进行光照计算。
-   需要法线和深度的后处理需要单独渲染一个 rt 出来
-   强制使用了一个 preZ（如果没涉及过这个概念的话，可以理解为进行了一个深度预计算 Pass）

**群组渲染（Clustered Rendering）**

-   带宽相对减少，多光源下效率提升
-   分为 forward 和 deferred 两种
-   详细补充拓展：[https://zhuanlan.zhihu.com/p/54694743](https://zhuanlan.zhihu.com/p/54694743)

### 4 PreZ (Zprepass)

-   实际上就是一个深度计算

**和深度图的区别：**

-   都是深度信息
-   PreZ 是用一个 pass，只算深度
-   深度图是算成了一张 RT（RenderTexture），把**深度信息绘制到了一张 RT 上**。

-   具体用途：
-   大规模草、透明排序会用到 PreZ

-   early-z 和 PreZ 的区别
-   early-z，自动的，对面数有要求（硬件自动）
-   PreZ，当 early-z 失效的时候，或者需要深度图的时候，一种手动代替的方案
[[3 进阶应用#3.5 Early-z和Z-prepass]]

### 5 一些补充

-   Unity 的 urp 是不支持延迟渲染的，老管线支持。
-   UE 默认管线就是延迟渲染管线
-   一般延迟渲染用于主机/大项目


# 2.9 GPU 硬件架构概述（未学习）
[深入GPU硬件架构及运行机制 - 0向往0 - 博客园 (cnblogs.com)](https://www.cnblogs.com/timlly/p/11471507.html#32-gpu%E5%BE%AE%E8%A7%82%E7%89%A9%E7%90%86%E7%BB%93%E6%9E%84)
# 3.8 现代移动端的 TBR 和 TBDR 渲染管线（未学习）
先学习 PC 端的再来看。。。
[IMR, TBR, TBDR 还有GPU架构方面的一些理解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/259760974)
