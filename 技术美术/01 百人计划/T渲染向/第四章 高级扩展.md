# 4.1 Bloom
## 一、什么是Bloom算法
### 1、首先看一下Bloom效果长什么样
![[Pasted image 20221209110858.png]]
![[Pasted image 20221209110901.png]]
![[Pasted image 20221209110903.png]]
### 2、什么是Bloom

-   Bloom，也称辉光，是一种常见的屏幕效果
-   模拟摄像机的一种图像效果，让画面中较亮的区域“扩散”到周围的区域中，造成一种朦胧的效果
-   可以让物体具有真实的明亮效果
-   可以实现光晕效果

### 3、Bloom的实现原理
#### ①Bloom实现原理

-   实现思路：

-   1.提取原图较亮区域（利用阈值）
-   2.模糊该图像
-   3.与原图混合/叠加
- ![[Pasted image 20221209110919.png]]
- //在HDR和LDR的那节课中也提到过bloom，我直接把当时的作的一张流程图摘过来以供参考
 ![[Pasted image 20221209111030.png]]
#### ②前置知识1：HDR和LDR
[[第二章 光照基础#2.7 LDR和HDR]]
-   HDR和LDR分别是是高动态范围和低动态范围的缩写
-   **LDR**
-   jpg、png格式图片
-   RGB范围在[0,1]

-   **HDR**
-   HDR、EXR格式图片
-   可以超过1

-   因为自然界中的亮度差异是很大的（比如蜡烛的光强度约为15，而太阳的强度约为10w），只用LDR的话很多效果完全表现不出来
-   //其他具体细节参考2.7节

#### ③前置知识2：高斯模糊

-   实现图像模糊的一种方式
-   **高斯模糊：**
-   <font color="#ff0000">利用高斯核进行卷积运算，得到模糊的图像</font>
![[Pasted image 20221209111419.png]]
-   高斯核：通过高斯函数定义的**卷积核**
-   核中心：(0,0)
-   核大小：3x3
-   标准方差σ（sei ge ma）：1.5

-   计算步骤：
	- 将（x，y）带入公式中，计算出权重值，**（权重值代表当前处理像素的影响程度，离中心越近权重越大，影响程度越大）**
	- 为了保证卷积后图像不变暗，需要对高斯核进行归一化处理（每个权重除以所有权重的和）
	- ![[Pasted image 20221209111535.png]]

**二维高斯核的特点**
-   计算量大，N×N的高斯核需要N * N * W * H次纹理采样（图像的宽度和高度分别为W、H）。

**二维高斯核的可分离性**
-   二维高斯核可以拆成两个一维高斯核
-   利用可分离性，我们就可以优化算法
	-   我们可以用两个一维高斯核先后对图像进行两次卷积操作
	-   这样一来，结果一样，采样次数变为了2 * N * W * H
-   再进一步
	-   一维高斯核中包括了很多重复的权重，即对称性（下例中0.0545，0.02442）
	-   下例中大小为5的高斯核，<font color="#ff0000">实际上只需要记录三个权重值即可（0.0545、0.2442、0.4026）</font>

![[Pasted image 20221209111755.png]]

#### 卷积

-   **课程内容**
-   是一种图像操作
-   利用“卷积核”对图像的每个像素进行一系列操作

-   **卷积核**：
	-   通常是由四方形网格结构，该区域内每个放个都有一个权重值
	-   当我们对图像中的像素进行卷积时：
	-   会把卷积核的中心放置在该像素上
	-   翻转核之后再依次计算核中每个元素和其覆盖的图像像素值的乘积并求和
	-   得到的结果就是该位置的新像素值

-   一个例子：
![[卷积.gif]]
-   计算步骤：先水平反转卷积核，再将位置一一对应求和

-   **补充：**
-   参考：GAMES101-L6内容
-   **滤波（Filtering）**
	-   滤波就是抹掉特殊频率的东西
	-   不同滤波的效果：
		-   高通滤波 = 边界
		-   低通滤波 = 模糊
![[Pasted image 20221209112711.png]]
-   **滤波（Filtering）=卷积（Convolution）=平均（Averaging）**
-   **卷积操作的定义**
-   ①原始信号的任意一个位置，取其周围的平均
-   ②作用在一个信号上，用一种滤波操作，得到一个结果Result

![[Pasted image 20221209112739.png]]
-   **卷积定理**
-   时域上的卷积就是频域上的乘积（时域上的乘积=频域上的卷积）

-   根据卷积定理，我们实现一个卷积操作可以有两种方法：
-   方法1：图和滤波器直接在时域上做卷积操作
-   方法2：先把图傅里叶变换，变换到频域上，把滤波器变到频域上，两者相乘；乘完之后再逆傅里叶变换到时域上
- ![[Pasted image 20221209113030.png]]
### 4、总结

①我们想用bloom实现什么效果？
-   自然界中亮度差异较大的效果，LDR不能表现的，光晕的效果

②实现bloom效果的步骤？
-   先利用阈值提取图像中较亮的区域
-   对这个区域做模糊
-   将模糊后的高亮部分叠加回原图

## 二、用Unity实现Bloom算法
![[1 4.gif]]
### 1、C#脚本部分
-   **基本思路**：
-   调用OnRenderImage函数，得到渲染纹理
-   将纹理和相关参数传入shader

-   **具体内容**：
-   将基类改为PostEffectsBase（为我们提供了检查shader材质的方法，用来检查shader材质是否可用（其实就是Valid））
-   声明这个脚本使用的shader、材质

-   定义shader中的相关参数
-   高斯模糊迭代次数
-   高斯模糊范围（blurSpread）
-   下采样系数（downSample，downSample控制渲染纹理大小）
-   阈值

-   OnRenderImage函数部分
-   OnRenderImage是官方提供的函数，可以使用这个函数获取当前的屏幕图像（得到渲染纹理）
-   实现内容：
-   检查材质的可用性（valid）
-   将阈值传入材质
-   定义rtW、rtH变量，作为屏幕的实际宽度、高度
-   创建一块大小小于原屏幕分辨率的缓冲区buffer0

-   将滤波模式改为双线性滤波

-   调用“Graphics.Blit”方法pass1提取图像中较亮的区域
-   用for循环对图像进行高斯模糊处理

-   shader中传入高斯模糊范围
-   定义第二个缓冲区buffer1
-   调用“Graphics.Blit”方法进行竖直方向的高斯模糊
-   调用“ReleaseTemporary”方法释放缓冲区buffer0（为了让模糊后的结果存入buffer1，再用buffer1覆盖buffer0，重新分配buffer1，这样一来每次模糊使用的都是上次模糊做完的结果）
-   用同样的思路完成水平方向的高斯模糊
-   迭代后的buffer0的结果就是我们需要的结果

-   将模糊后的buffer0作为纹理传入shader
-   用“Graphics.Blit”方法调用最后一个pass，将模糊后的图像和原图混合叠加，作为最终结果输出

```c
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class DS_Bloom : PostEffectsBase
{
    //定义使用的shader和材质
    public Shader bloomShader;
    private Material bloomMaterial = null;

    public Material material
    {
        get
        {
            bloomMaterial = CheckShaderAndCreateMaterial(bloomShader,bloomMaterial); 
            //调用PostEffectsBase基类中的函数，检查shader并且创建材质
            return bloomMaterial;
        }
    }
    
    //定义shader中的参数
    [Range(0, 4)] public int iterations = 3;//高斯模糊迭代次数
    [Range(0.2f, 3.0f)] public float blurSpread = 0.6f;//高斯模糊范围
    [Range(1,8)] public int downSample = 2;//下采样，缩放系数
    [Range(0.0f, 4.0f)] public float luminanceThreshold = 0.6f;//阈值
    
    //调用OnRenderImage函数来实现Bloom
    private void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if (material != null) 
        {
            material.SetFloat("_LuminanceThreshold", luminanceThreshold);//传入阈值
            
            //src.width和hight代表屏幕图像的宽度和高度
            int rtW = src.width / downSample;//得到渲染纹理的宽度
            int rtH = src.height / downSample;//得到渲染纹理的高度
            
            //创建一块分辨率小于原屏幕的缓冲区：buffer0
            RenderTexture buffer0 = RenderTexture.GetTemporary(rtW,rtH,0);
            buffer0.filterMode = FilterMode.Bilinear;//滤波模式为双线性
            
            //用Blit方法调用shader中的第一个pass，提取图像中较亮的区域
            Graphics.Blit(src, buffer0, material, 0);//结果存在buffer0
            
            //迭代进行高斯模糊
            for (int i = 0; i <iterations; i++)
            {
                material.SetFloat("_BlurSize", 1.0f + i * blurSpread);//传入模糊半径
                
                //定义第二个缓冲区：buffer1
                RenderTexture buffer1 = RenderTexture.GetTemporary(rtW, rtH, 0);
                
                //用Blit方法调用shader中的第二个pass，进行竖直方向的高斯模糊
                Graphics.Blit(buffer0, buffer1, material, 1);
                
                //释放缓冲区buffer0，将buffer1的赋值给buffer，并重新分配buffer1
                RenderTexture.ReleaseTemporary(buffer0);
                buffer0 = buffer1;
                buffer1 = RenderTexture.GetTemporary(rtW, rtH, 0);
                
                //用Blit方法调用shader中的第三个pass，进行水平方向的高斯模糊
                Graphics.Blit(buffer0, buffer1, material, 2);
                
                //原理同上次释放，故技重施
                RenderTexture.ReleaseTemporary(buffer0);
                buffer0 = buffer1;
                
                //迭代完成后，buffer0的结果就是高斯模糊后的结果
            }
            
            // 将完成高斯模糊后的结果buffer0传递给材质中的_Bloom纹理属性
            material.SetTexture("_Bloom", buffer0);
            
            //用Blit方法调用shader中的第四个pass，完成混合
            Graphics.Blit(src, dest, material, 3);//dest是最终输出
            
            //最后记得释放临时缓冲区
            RenderTexture.ReleaseTemporary(buffer0);
        }
        else
        {
            Graphics.Blit(src, dest);
        }
    }
}

```

### 2、shader部分

-   **基本思路**

-   使用4个pass完成bloom效果，对应bloom的实现步骤

-   pass1：提取亮部区域
-   pass2：实现竖直方向的高斯模糊
-   pass3：实现水平方向的高斯模糊
-   pass4：模糊后的高亮区域叠加到原图
-   //只是为了更理解原理，结合脚本用一个pass同样可以实现

-   **具体内容**

-   声明相关参数
-   渲染状态：

-   开启深度测试，关闭剔除，关闭深度写入

-   定义四个pass相关函数名
-   引入相关头文件
-   声明相关变量
-   剩下就是各个pass需要的输入输出结构体和顶点像素着色器
-   **整理一下其中需要注意的几点**

-   **亮度如何获取？**

-   亮度计算公式

-   **较亮区域如何提取？**

-   采样后获取亮度值，再减去阈值，最后用clamp截取

-   **如何对竖直/水平方向进行高斯模糊？**

-   **在顶点着色器中计算uv**

-   在vertex shader中计算的好处：

-   计算量(次数)少（一般情况下顶点数量<像素数量）
-   在顶点着色器中计算纹理坐标可以减少运算提高性能
-   而且由于顶点到片元的插值是线性的，因此不会影响纹理坐标的计算结果

-   用5维数组存5个uv，其中**uv0位当前纹理坐标，其他四个是高斯模糊对邻域采样时用到的纹理坐标**
-   uv0就是纹理的坐标，直接从输入结果传过来即可
-   邻域uv的计算：

-   以uv1为例，假设uv1是（0,1），也就是在基础uv向上移动
-   对于移动的距离，我们要利用到模糊范围
-   同理，uv2向下就是-1，uv3就是向上两个单位，uv4是向下两个单位
-   相同的思路，将竖直的y改为x就能给水平方向用

-   **在像素着色器中进行模糊**

-   用数组存需要的三个权重值
-   定义sum变量用来存模糊后的像素值，并通过纹理采样获取像素值
-   模糊操作：

-   用for循环进行卷积运算

-   **混合时有哪些注意点？**

-   顶点着色器：

-   uv的xy是渲染纹理坐标，zw是模糊后的纹理坐标
-   考虑平台差异处理

-   如果y是负的，就进行反转操作

-   片元着色器：

-   直接返回uv采样的xy、zw之和即可

-   还有一点要注意

-   bloom模拟的是相机的一种图像处理效果，所以C#脚本给的是camera
```c
Shader "Unlit/DS_Bloom"
{
    Properties
    {
        // _MainTex为渲染纹理，变量名固定不能改变
        //模糊结果、阈值、模糊半径的变量名与C#脚本中的对应
        _MainTex ("Texture", 2D) = "white" {}
        _Bloom ("Bloom (RGB)", 2D) = "black" {} //高斯模糊后的结果
		_LuminanceThreshold ("Luminance Threshold", Float) = 0.5 //阈值
		_BlurSize ("Blur Size", Float) = 1.0 //模糊半径
    }
    SubShader
    {
        //用CGINCLUDE和ENDCG
        //Unity会把它们之间的代码插入到每一个pass中，已达到声明一遍，多次使用的目的。
        CGINCLUDE
        #include "UnityCG.cginc"

        //声明属性和C#脚本中用到的变量
        sampler2D _MainTex;
		half4 _MainTex_TexelSize;//纹素大小
		sampler2D _Bloom;
		float _LuminanceThreshold;
		float _BlurSize;

        //########第1个pass使用########
        //输出结构
        struct v2fExtractBright {
			float4 pos : SV_POSITION; 
			half2 uv : TEXCOORD0;
		};
        
        //顶点着色器
        v2fExtractBright vertExtractBright(appdata_img v) {
        	//appdata_img是官方提供的输入结构，只包含图像处理时必须的顶点坐标和uv等变量
			v2fExtractBright o;
			o.pos = UnityObjectToClipPos(v.vertex);
			o.uv = v.texcoord;	 
			return o;
		}
        
        // 明亮度公式
		fixed luminance(fixed4 color) {
        	//计算得到像素的亮度值
			return  0.2125 * color.r + 0.7154 * color.g + 0.0721 * color.b; 
		}
        
        //片元着色器->提取高亮区域
        fixed4 fragExtractBright(v2fExtractBright i) : SV_Target {
        	fixed4 c = tex2D(_MainTex, i.uv);// 贴图采样
			
			fixed val = clamp(luminance(c) - _LuminanceThreshold, 0.0, 1.0);
        	// 调用luminance得到采样后像素的亮度值，再减去阈值
			// 使用clamp函数将结果截取在[0,1]范围内
        	
			return c * val;
        	// 将val与原贴图采样得到的像素值相乘，得到提取后的亮部区域
		}

        
        //########第2、3个pass使用########
        //输出结构
        struct v2fBlur {
			float4 pos : SV_POSITION;
        	half2 uv[5]: TEXCOORD0;
			// 此处定义5维数组用来计算5个纹理坐标
        	// 由于卷积核大小为5x5的二维高斯核可以拆分两个大小为5的一维高斯核
        	// uv[0]存储了当前的采样纹理
        	// uv[1][2][3][4]为高斯模糊中对邻域采样时使用的纹理坐标
		};
        
        //顶点着色器->计算竖直方向进行高斯模糊的uv
        v2fBlur vertBlurVertical(appdata_img v) {
			
			v2fBlur o;
			o.pos = UnityObjectToClipPos(v.vertex);//将顶点从模型空间变换到裁剪空间下
			half2 uv = v.texcoord;
			o.uv[0] = uv;
        	//uv[0]就是（0,0）
        	//对竖直方向进行模糊
			//对应到邻域就是下边的情况
        	//uv[1]，向上挪动1个单位(0, 1)
        	//uv[2]，向下挪动1个单位(0, -1)
        	//uv[3]，向上挪动2个单位(0, 2)
        	//uv[3]，向下挪动2个单位(0, -2)
        	//最后乘上模糊半径作为参数控制
			o.uv[1] = uv + float2(0.0, _MainTex_TexelSize.y * 1.0) * _BlurSize;
			o.uv[2] = uv - float2(0.0, _MainTex_TexelSize.y * 1.0) * _BlurSize;
			o.uv[3] = uv + float2(0.0, _MainTex_TexelSize.y * 2.0) * _BlurSize;
			o.uv[4] = uv - float2(0.0, _MainTex_TexelSize.y * 2.0) * _BlurSize;
					 
			return o;
		}
        
        //顶点着色器->计算水平方向进行高斯模糊的uv
        v2fBlur vertBlurHorizontal(appdata_img v) {
			v2fBlur o;
			o.pos = UnityObjectToClipPos(v.vertex);
			half2 uv = v.texcoord;
        	o.uv[0] = uv;
        	//uv[0]就是（0,0）
        	//对水平方向进行模糊
			//同理，uv[1]到[4]分别对应(1, 0)、(-1, 0)、(2, 0)、(-2, 0)
			o.uv[1] = uv + float2(_MainTex_TexelSize.x * 1.0, 0.0) * _BlurSize;
			o.uv[2] = uv - float2(_MainTex_TexelSize.x * 1.0, 0.0) * _BlurSize;
			o.uv[3] = uv + float2(_MainTex_TexelSize.x * 2.0, 0.0) * _BlurSize;
			o.uv[4] = uv - float2(_MainTex_TexelSize.x * 2.0, 0.0) * _BlurSize;
					 
			return o;
		}
        //片元着色器->进行高斯模糊
        fixed4 fragBlur(v2fBlur i) : SV_Target {
			
			float weight[3] = {0.4026, 0.2442, 0.0545};
        	// 因为二维高斯核具有可分离性，而分离得到的一维高斯核具有对称性
			// 所以只需要在数组存放三个高斯权重即可
			
			fixed3 sum = tex2D(_MainTex, i.uv[0]).rgb * weight[0];
        	// 结果值sum初始化为当前的像素值乘以它对应的权重值

        	// 进行卷积运算，根据对称性完成两次循环
				// 第一次循环计算第二个和第三个格子内的结果
				// 第二次循环计算第四个和第五个格子内的结果
			for (int it = 1; it < 3; it++) {
				sum += tex2D(_MainTex, i.uv[it*2-1]).rgb * weight[it];
				sum += tex2D(_MainTex, i.uv[it*2]).rgb * weight[it];
			}
			
			return fixed4(sum, 1.0);// 返回滤波后的结果
		}

        
        //########第4个pass使用########
        //输出结构
        struct v2fBloom {
			float4 pos : SV_POSITION; 
			half4 uv : TEXCOORD0;
		};

        //顶点着色器
        v2fBloom vertBloom(appdata_img v) {
			v2fBloom o;
			o.pos = UnityObjectToClipPos (v.vertex);
			
			o.uv.xy = v.texcoord; //xy分量为_MainTex的纹理坐标		
			o.uv.zw = v.texcoord; //zw分量为_Bloom的纹理坐标
			
			// 平台差异化处理
        	//判断y是否小于0，如果是就进行翻转处理
			#if UNITY_UV_STARTS_AT_TOP			
			if (_MainTex_TexelSize.y < 0.0)
				o.uv.w = 1.0 - o.uv.w;
			#endif
			return o; 
		}

        //片元着色器->混合亮部和原图
        fixed4 fragBloom(v2fBloom i) : SV_Target {
		    // 把这两张纹理的采样结果相加即可得到最终效果
			return tex2D(_MainTex, i.uv.xy) + tex2D(_Bloom, i.uv.zw);
		}
    	ENDCG
    	
        // 开启深度测试，关闭剔除和深度写入
        ZTest Always 
    	Cull Off 
    	ZWrite Off
        
    	//第一个pass，提取较亮区域
        Pass{
            CGPROGRAM
			#pragma vertex vertExtractBright
			#pragma fragment fragExtractBright
            ENDCG
        }
	    
    	//第二个pass，进行竖直方向高斯模糊
    	Pass{
            CGPROGRAM
			#pragma vertex vertBlurVertical
			#pragma fragment fragBlur
            ENDCG
        }
    	
    	//第三个pass，进行水平方向高斯模糊
    	Pass{
            CGPROGRAM
			#pragma vertex vertBlurHorizontal
			#pragma fragment fragBlur
            ENDCG
        }
    	
    	//第四个pass，混合高亮区域和原图
    	Pass{
            CGPROGRAM
			#pragma vertex vertBloom
			#pragma fragment fragBloom
            ENDCG
        }
    }
	FallBack Off
}
```

## 三、 Bloom算法的应用

### 1、配合自发光贴图
![[Pasted image 20221209114223.png]]
### 2、配合特效
![[Pasted image 20221209114240.png]]
### 3、配合ToneMapping
-   bloom效果和ToneMapping结合可以较好的保留暗部和亮部的细节
![[Pasted image 20221209114250.png]]
### 4、GoodRay效果
-   使用**径向模糊**代替高斯模糊，模拟光线往某个方向扩散的效果
![[Pasted image 20221209114329.png]]
-   GoodRay配合ToneMapping
![[Pasted image 20221209114348.png]]
-   可以看到ToneMapping解决了**过曝**的问题
#### ①径向模糊介绍及原理
-   径向模糊（Radial Blur）可以给画面带来很好的速度感，是各类游戏中后处理的常客，也常用于Sun Shaft等后处理特效中作为光线投射的模拟。
![[Pasted image 20221209114607.png]]
**径向模糊的原理：**
-   首先选取一个**径向轴心**（Radial Center）
-   然后**将每一个采样点的uv基于此径向轴心进行偏移**（offset）
-   并进行一定次数的**迭代采样**
-   最终将采样得到的RGB值累加，并除以迭代次数。
#### ②实现过程
相比Bloom效果的高斯模糊，这里我们使用径向模糊来代替它
![[1 5.gif]]
##### C#脚本
```C#
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class GodRay : PostEffectsBase
{
    // 声明GodRay效果需要的Shader，并创建相应的材质
    public Shader godRayShader;
    private Material godRayMaterial = null;
    public Material material
    {
        get
        {
            // 调用PostEffectsBase基类中检查Shader和创建材质的函数
            godRayMaterial = CheckShaderAndCreateMaterial(godRayShader, godRayMaterial);
            return godRayMaterial;
        }
    }

    // 高亮部分提取阈值
    public Color colorThreshold = Color.gray;
    // 光颜色
    public Color lightColor = Color.white;
    // 光强度
    [Range(0.0f, 20.0f)]
    public float lightFactor = 0.5f;
    // 径向模糊uv采样偏移值
    [Range(0.0f, 10.0f)]
    public float samplerScale = 1;
    // 迭代次数
    [Range(1, 5)]
    public int blurIteration = 2;
    // 分辨率缩放系数
    [Range(1, 5)]
    public int downSample = 1;
    // 光源位置
    public Transform lightTransform;
    // 光源范围
    [Range(0.0f, 5.0f)]
    public float lightRadius = 2.0f;
    // 提取高亮结果Pow系数，用于适当降低颜色过亮的情况
    [Range(1.0f, 4.0f)]
    public float lightPowFactor = 3.0f;

    private Camera targetCamera = null;

    void Awake()
    {
        targetCamera = GetComponent<Camera>();
    }

    void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if (material && targetCamera)
        {
            int rtW = src.width / downSample;
            int rtH = src.height / downSample;
            // 创建一块大小小于原屏幕分辨率的缓冲区buffer0
            RenderTexture buffer0 = RenderTexture.GetTemporary(rtW, rtH, 0, src.format);

            //计算光源位置从世界空间转化到视口空间
            Vector3 viewPortLightPos = lightTransform == null ? new Vector3(.5f, .5f, 0) : targetCamera.WorldToViewportPoint(lightTransform.position);

            // 参数传给材质
            material.SetVector("_ColorThreshold", colorThreshold);
            material.SetVector("_ViewPortLightPos", new Vector4(viewPortLightPos.x, viewPortLightPos.y, viewPortLightPos.z, 0));
            material.SetFloat("_LightRadius", lightRadius);
            material.SetFloat("_PowFactor", lightPowFactor);
            Graphics.Blit(src, buffer0, material, 0);// 根据阈值提取高亮部分,使用pass0进行高亮提取，比Bloom多一步计算光源距离剔除光源范围外的部分
            material.SetVector("_ViewPortLightPos", new Vector4(viewPortLightPos.x, viewPortLightPos.y, viewPortLightPos.z, 0));
            material.SetFloat("_LightRadius", lightRadius);
            
            // 径向模糊的采样uv偏移值
            float samplerOffset = samplerScale / src.width;
            
            // 通过循环迭代径向模糊
            for (int i = 0; i < blurIteration; i++)
            {
                RenderTexture buffer1 = RenderTexture.GetTemporary(rtW, rtH, 0, src.format);
                float offset = samplerOffset * (i * 2 + 1);
                material.SetVector("_offsets", new Vector4(offset, offset, 0, 0));
                Graphics.Blit(buffer0, buffer1, material, 1);

                offset = samplerOffset * (i * 2 + 2);
                material.SetVector("_offsets", new Vector4(offset, offset, 0, 0));
                Graphics.Blit(buffer1, buffer0, material, 1);
                RenderTexture.ReleaseTemporary(buffer1);
            }
            
            //将完成模糊的结果传递给材质中的属性

            material.SetTexture("_BlurTex", buffer0);
            material.SetVector("_LightColor", lightColor);
            material.SetFloat("_LightFactor", lightFactor);
            
            // 将径向模糊结果与原图进行混合
            Graphics.Blit(src, dest, material, 2);
            
            //最后释放临时缓冲
            RenderTexture.ReleaseTemporary(buffer0);
        } else {
            Graphics.Blit(src, dest);
        }
    }

}

```

##### shader
```c
Shader "Unlit/GodRay"
{
    Properties
	{
		//对应原图和提取出来的部分
		_MainTex("Base (RGB)", 2D) = "white" {}
		_BlurTex("Blur", 2D) = "white"{}
	}
 
	CGINCLUDE
	#define RADIAL_SAMPLE_COUNT 6
	#include "UnityCG.cginc"
	
	// 提取亮部图像
	struct v2fExtractBright
	{
		float4 pos : SV_POSITION;
		float2 uv : TEXCOORD0;
	};
 
	// 径向模糊
	struct v2fRadialBlur
	{
		float4 pos : SV_POSITION;
		float2 uv  : TEXCOORD0;
		float2 blurOffset : TEXCOORD1;
	};
 
	// 混合
	struct v2fGodRay
	{
		float4 pos : SV_POSITION;
		float2 uv  : TEXCOORD0;
		float2 uv1 : TEXCOORD1;
	};

	//声明属性和C#脚本中用到的变量
	sampler2D _MainTex;
	float4 _MainTex_TexelSize;
	sampler2D _BlurTex;
	float4 _BlurTex_TexelSize;
	float4 _ViewPortLightPos;
	
	float4 _offsets;
	float4 _ColorThreshold; //高亮部分阈值
	float4 _LightColor; //光颜色
	float _LightFactor; //光强度
	float _PowFactor; //提取高亮结果Pow系数，用于适当降低颜色过亮的情况
	float _LightRadius; //光源范围
 
	// 提取亮部图像VS
	v2fExtractBright vertExtractBright(appdata_img v)
	{
		v2fExtractBright o;
		o.pos = UnityObjectToClipPos(v.vertex);
		o.uv = v.texcoord.xy;
		// 平台差异化处理
		#if UNITY_UV_STARTS_AT_TOP
		if (_MainTex_TexelSize.y < 0)
			o.uv.y = 1 - o.uv.y;
		#endif	
		return o;
	}
 
	// 提取亮部图像PS
	fixed4 fragExtractBright(v2fExtractBright i) : SV_Target
	{
		fixed4 color = tex2D(_MainTex, i.uv);
		float distFromLight = length(_ViewPortLightPos.xy - i.uv);
		float distanceControl = saturate(_LightRadius - distFromLight);

		// 仅当color大于设置的阈值的时候才输出
		float4 thresholdColor = saturate(color - _ColorThreshold) * distanceControl;
		float luminanceColor = Luminance(thresholdColor.rgb);
		luminanceColor = pow(luminanceColor, _PowFactor);
		return fixed4(luminanceColor, luminanceColor, luminanceColor, 1);
	}
 
	// 径向模糊VS
	v2fRadialBlur vertRadialBlur(appdata_img v)
	{
		v2fRadialBlur o;
		o.pos = UnityObjectToClipPos(v.vertex);
		o.uv = v.texcoord.xy;

		// 径向模糊采样偏移值 * 沿光的方向权重
		o.blurOffset = _offsets * (_ViewPortLightPos.xy - o.uv);

		return o;
	}
 
	// 径向模拟PS
	fixed4 fragRadialBlur(v2fRadialBlur i) : SV_Target
	{
		half4 color = half4(0,0,0,0);
		//通过迭代，将采样得到的RGB值累加
		for(int j = 0; j < RADIAL_SAMPLE_COUNT; j++)   
		{	
			color += tex2D(_MainTex, i.uv.xy);
			i.uv.xy += i.blurOffset; 	
		}
		//最后除以迭代次数
		return color / RADIAL_SAMPLE_COUNT;
	}
 
	// 混合VS
	v2fGodRay vertGodRay(appdata_img v)
	{
		v2fGodRay o;
		o.pos = UnityObjectToClipPos(v.vertex);
		o.uv.xy = v.texcoord.xy;
		o.uv1.xy = o.uv.xy;
		#if UNITY_UV_STARTS_AT_TOP
		if (_MainTex_TexelSize.y < 0)
			o.uv.y = 1 - o.uv.y;
		#endif	
		return o;
	}

 	// 混合PS
	fixed4 fragGodRay(v2fGodRay i) : SV_Target
	{
		fixed4 ori = tex2D(_MainTex, i.uv1);
		fixed4 blur = tex2D(_BlurTex, i.uv);
		return ori + _LightFactor * blur * _LightColor;
	}
 
	ENDCG
 
	SubShader
	{
		ZTest Always Cull Off ZWrite Off

		// 提取高亮部分
		Pass
		{
			CGPROGRAM
			#pragma vertex vertExtractBright
			#pragma fragment fragExtractBright
			ENDCG
		}
 
		// 径向模糊
		Pass
		{
			CGPROGRAM
			#pragma vertex vertRadialBlur
			#pragma fragment fragRadialBlur
			ENDCG
		}
 
		// 将亮部图像与原图进行混合得到最终的GodRay效果
		Pass
		{
			CGPROGRAM
			#pragma vertex vertGodRay
			#pragma fragment fragGodRay
			ENDCG
		}
	}
}

```
## 四、扩展/课后作业
### 如何实现bloom的mask功能

①用Alpha通道
-   参考：[https://blog.csdn.net/SnoopyNa2Co3/article/details/88075047](https://blog.csdn.net/SnoopyNa2Co3/article/details/88075047)
②用SRP渲染一张Mask图
③用Command-Buffer
④用模板测试
⑤ 直接用Mask图（简单情况下）

### 其他资料扩展/同学笔记
-   十种图像模糊像算法的总结与实现-------毛星云
-   [https://zhuanlan.zhihu.com/p/125744132](https://zhuanlan.zhihu.com/p/125744132)

-   SRP做Mask---by丷葉丷
-   [https://quaint-author-3ce.notion.site/Bloom-1b2f32f3502c471a867a4ca07d27306d](https://quaint-author-3ce.notion.site/Bloom-1b2f32f3502c471a867a4ca07d27306d)

-   CommandBufffer做Mask-------by清盐
-   [https://www.yuque.com/qingyan-bng85/zagg8x/epl3qs](https://www.yuque.com/qingyan-bng85/zagg8x/epl3qs)
## 五、Reference

-   图片参考：· [https://unsplash.com/·](https://unsplash.com/·)
-   自行拍摄项目参考：· [https://github.com/keijiro/KinoBloom·](https://github.com/keijiro/KinoBloom·) [https://github.com/MarcusXie3D/FastBloomForMobiles](https://github.com/MarcusXie3D/FastBloomForMobiles)
-   资料参考：

-   learnopengl：[https://learnopengl.com/Advanced-Lighting/Bloom·](https://learnopengl.com/Advanced-Lighting/Bloom·)
-   Unity Shader入门精要：12.5 Bloom效果·
-   [https://en.wikipedia.org/wiki/Bloom_(shader_effect)](https://en.wikipedia.org/wiki/Bloom_(shader_effect)·)
-   [https://en.wikipedia.org/wiki/High_dynamic_range](https://en.wikipedia.org/wiki/High_dynamic_range·)
-   [https://zhuanlan.zhihu.com/p/76505536](https://zhuanlan.zhihu.com/p/76505536)

# 4.2 SSAO（不懂，后续补上）

日常生活中我们可以看到在角落里，光线很难到达的地方会很暗（缝隙、角落等等），这里就引申出来了AO  

## AO  

环境光遮蔽，全称Ambient Occlusion，是计算机图形学中的一种着色和渲染技术，模拟光线达到物体的能力的粗略的全局方法，描述光线到达物体表面的能力。  

![](1678971269015.png)  

## SSAO  

屏幕空间环境光遮蔽，全称Screen Space Ambient Occlusion，一种用于计算机图形中实时实现近似环境光遮蔽效果的渲染技术。通过获取像素的深度缓冲、法线缓冲以及像素坐标来计算实现，来近似的表现物体在间接光下产生的阴影。  

![](1678971269063.png)  

### SSAO历史  

AO这项技术最早是在Siggraph 2002年会上由ILM（工业光魔）的技术主管Hayden Landis所展示，当时就被叫做Ambient Occlusion。  

2007年，Crytek公司发布了一款叫做屏幕空间环境光遮蔽（Screen-Space Ambient Occlusion，SSAO）的技术，并用在了他们的看家作孤岛危机上  

### SSAO原理  

![](1678971269099.png)  

第一步：获取深度、法线缓冲（很重要）  

*   深度：用来计算像素在屏幕空间中的坐标值，把每一个像素的位置反推出来。整个屏幕算完之后，再利用深度值，把整一个3D场景反向展现出来，每一个像素和其他像素之间的关系。  
    

*   法线：推出法线半球随机向量。  
    

AO核心：for循环  

*   每一次for循环都会在法线半球中获取一个随机向量，根据这个向量我们会求出它对应的深度值，然后跟我们当前的深度值做比较，如果大于直接舍去，小于的话我们认为有遮蔽，算进加权中，最后我们合成AO，然后再加上一些后期处理优化效果。  
    

### 深度缓冲  

![](1678971269136.png)  

上面的图片中，越黑的地方离相机越近，越白离相机越远。  

深度缓冲的目的：得到场景中物体与相机的远近关系。  

### 法线缓冲  

![](1678971269186.png)  

法线缓冲用于获得法线推出法线半球，我们需要获得法线半球的空间，我们的随机向量需要转换到法线半球空间中才能得到正确的值。  

这两个我们需要在相机中设置，才能得到其Buffer的值。  

### 法线半球  

![](1678971269225.png)  

图中的像素值的排列是有点不太正常的，是因为这是俯视视角，我们正常在屏幕中看到可能就是一个平面，这里就是理解下每个像素是带有深度的。每个像素求完法线后会根据随机向量求出切线，然后再求出空间。  

*   因为从相机开始到像素，模拟出一条向量，是在相机空间中的。  
    

*   我们利用法线和随机向量构建出一个法线半球空间。  
    

*   然后把相机空间中的向量转化到这个半球空间里，再加上随机向量，就可以得出像素对应的位置。  
    

法线半球目的：在随机采样的时候替我们做一个转化操作，让我们正确地求出边上这些随机像素值的坐标。  

不用全球的原因是，避免采样浪费，而且如果用全球可能还会导致平整墙面都显得灰蒙蒙的。  

## SSAO算法实现  

### 获取深度&法线缓冲数据  

```
private void Start(){
	cam = this.GetComponent();
    cam.depthTextureMode = cam.depthTextureMode | DepthTextureMode.DepthNormals;
} 
``` 

*   获取相机的组件  
    

*   深度纹理模式调整成带深度和带法线的。  
    

![](1678971269263.png)  

*   这里的UV是屏幕空间的UV  
    

*   ![](1678971269299.png)  
    

注意：  

*   此案例中使用的Unity版本为 Unity 2019.3.5f1  
    

*   场景中的项目为透视模式  
    

*   相机渲染路径为Forward，如果设置为Deferred延迟渲染，则会由对应的g-buffer生成，在shader中作为全局变量访问。  
    

*   使用OnRenderImage()来处理后期，进而实现SSAO  
    

### 重建相机空间坐标  

![](1678971269337.png)  

![](1678971269377.png)  

至此，我们已经获得了相机空间中的像素坐标。  

### 构建法向量正交基  

![](1678971269442.png)  

切线和副切线的求法是：  

*   法线（已经归一化）和随机向量进行点乘，得到一个投影在法线方向的一个标量a。  
    

*   使用法线（已经归一化）进行向量和标量的乘法，得到一个法线方向上的向量a。  
    

*   使用随机向量randvec减去向量a，得到切线。  
    

*   最后使用Cross叉乘法线和切线得到副切线，完成一组正交基的构建（两两点乘等于0，即两两垂直~）。  
    

### AO采样核心  

![](1678971269481.png)  

其中，_SampleKernelArray[i]是在C#中计算的随机值，然后其实随机值影响的是切向量的方向  

*   ![](1678971269517.png)  
    

*   法向量不变，切向量改变，则意味着正交基（TBN矩阵）就发生了改变。因此mul(_SampleKernelArray[i].xyz, TBN)的值就变得更加随机了（Because 随机数组 * 随机变化影响下的随机正交基矩阵 = 更加随机化的向量）  
    

*   _SampleKernalRadius是影响半球半径的因素  
    

*   ![](1678971269575.png)  
    

*   randomPos = viewPos + randomVec * _SampleKernalRedius  
    

*   ![](1678971269627.png)  
    

*   由于之前在样本相对相机的方向时取得是CameraInvProjection，这回再取反：  
    

*   ![](1678971269668.png)  
    

*   经过反推得到rscreenPos，其中还经过了范围的映射变化，映射到了屏幕的0-1空间中  
    

*   最后会通过rscreenPos来采样_CameraDepthNormalsTextrue，并计算转化至屏幕空间后对应的深度值，然后拿这个和linear01Depth进行比较，如果randomDepth>=linear01Depth，则说明对AO有贡献，进而加权。  
    

*   ![](1678971269718.png)  
    

这里添加一些对细节的理解：  

_ProjectionParams是一个存有不同意义的xyzw分量的一个投影参数向量，其中 x是 1.0（或 –1.0，如果当前使用翻转投影矩阵进行渲染），y是相机的近平面，z是相机的远平面，并且w是 1/FarPlane。  

那么，代码中有一处是这么用的：  

![](1678971269754.png)  

我们知道screenPos计算出来的结果是xy分量变化，zw分量不变。ndcPos又做了一定的映射处理，使得xy分量最终保持在[0,1]范围，z分量跟远近平面和原始的模型空间中的z坐标有关，w分量本来是-z，经过ComputeScreenPos处理后保持-z不变，但经过ndcPos(归一化)处理后变成了常数1。  

![](1678971269789.png)  

另外，从上面的公式可以看出，投影以后的屏幕坐标的x/y分量，都跟3D空间camera space的z坐标有关。举例来说，x分量为 ![](1678971270112.png) 。camera space 3D空间中，对于相同的x，如果z（深度）越大，投影变换以后的x分量越靠近0。"近大远小"的透视效果，就是这么算出来的~  

Unity shader 里面，要获取投影矩阵，有两个变量：unity_CameraProjection (float4x4) 和 UNITY_MATRIX_P (float4x4)。需要注意的是，这两个矩阵的内容实际上不一样。unity_CameraProjection，跟前面分析的矩阵是一样的。而 UNITY_MATRIX_P，经过分析测试，实际上是这样的一个矩阵：  

![](1678971270148.png)  

这个矩阵也是能用的，算出来的结果需要做一些额外的变换才能够跟 unity_CameraProjection 一样变成正常的 clip space 计算。具体为啥在 shader 里要增加这个内置变量，可能是为了一些兼容性吧，我也没完全弄明白。  

如果想要把一个camera space的向量，从 camera space 映射到 clip space / screen space，需要采取的操作是：用投影矩阵(unity_CameraProjection) 去乘那个向量(向量的齐次坐标 w 分量为0)，然后 只保留 x\y 分量。从下面的公式可以看出，这实际上计出来的是 点(x,y,z,1) 和 (0,0,z,1) 投影空间的向量(未做透视除法)。那么把这个计算结果做一个归一化，就可以得到方向向量投影到屏幕上的结果向量。  

![](1678971270182.png)  

首先将屏幕空间坐标经过透视除法转换到NDC空间中，将screenPos的xy分量[0,w]转化为[0,1]，进而再映射到[-1,1]的范围里。对于zw分量的变化就是z分量未知，w分量变为1，然后映射后仍旧是1。  

float4 ndcPos = (o.screenPos / o.screenPos.w) * 2 - 1;  

然后将屏幕像素对应在摄像机远平面（Far plane）的点转换到剪裁空间（Clip space）。因为在NDC空间中远平面上的点的z分量为1，所以可以直接乘以摄像机的Far值来将其转换到剪裁空间（实际就是反向透视除法）。  

float far = _ProjectionParams.z;  

float3 clipVec = float3(ndcPos.x, ndcPos.y, 1.0) * far;  

接着通过逆投影矩阵（Inverse Projection Matrix）将点转换到观察空间（View space）。  

float3 o.viewVec = mul(unity_CameraInvProjection, clipVec.xyzz).xyz;  

已知在观察空间中摄像机的位置一定为（0，0，0），所以从摄像机指向远平面上的点的向量就是其在观察空间中的位置。  

将向量乘以线性深度值，得到在深度缓冲中储存的值的观察空间位置。  

float depth = UNITY_SAMPLE_DEPTH(tex2Dproj(_CameraDepthTexture, i.screenPos));  

float3 viewPos = i.viewVec * Linear01Depth(depth);  

到这里可能会有疑惑——为什么非得要把点放到远平面去呢？  

Unity中的观察线性深度（Eye depth）就是顶点在观察空间（View space）中的z分量，范围在[0，Far]，而01线性深度（01 depth）就是观察线性深度通过除以摄像机远平面重新映射到[0，1]区间所得到的值。  

那么可以回答上面的问题，一个点的深度信息，放到远平面之后经过Linear01Depth函数可以重新回到[0,1]的范围。因此上面在求clipVec的时候会有“将z分量置为1，然后乘以far”的操作。  

## SSAO效果改进  

### 1.随机正交基  

![](1678971270229.png)  

### 2.AO累加平滑优化  

![](1678971270266.png)  

其中x值是我们控制的_RangeStrength，如果randomDepth和linearDepth的差值的绝对值大于我们设定的阈值，那么认为是天空（无穷远），就不需要遮蔽了，就当做和模型同样深度即可（range为0）。  

同样，同一个平面上的像素可能有时离得近了，会出现AO，所以加入：  

![](1678971270303.png)  

AO权重：  

只要1、2、3点深度值比我们要求的像素要小，我们认为对AO都有贡献（非0即1）。  

然后我们根据随机向量的长度值变化来做一个（0-0.2区间的）smoothstep  

越远离当前像素的，权重小一点。  

越靠近当前像素的，权重大一点。  

![](1678971270353.png)  

AO现在包含：  

*   range ——指当前像素深度和法向半球算得深度的差值大于某个阈值时，AO则立即变为0，否则就是正常计算selfCheck * weight（非0即1）。  
    

*   selfCheck——自阴影，当前像素的深度值大于某个阈值时，在法向半球算得深度值上+1，否则+0。这个其实是想让法向半球算得深度比当前像素的深度值要大，让这个点附近的某些像素深度比当前像素大，不要贡献AO（记住，只有小于当前像素深度的其他像素才能对当前像素贡献AO）。  
    

*   weight——权重，我们使用随机向量的长度作为因子来控制权重的大小，距离当前像素越远，对当前像素的影响越小。  
    

### 3.模糊  

![](1678971270403.png)  

## 比对模型烘焙AO  

![](1678971270441.png)  

### 方式一：三维建模软件烘焙AO  

优点：  

1、单一物体可控性强（通过单一物体的材质球上的AO纹理贴图），可以控制单一物体的AO的强弱；  

2、弥补场景烘焙的细节，整体场景的烘焙（包含AO信息），并不能完全包含单一物体细节上的AO，而通过三维建模软件烘焙到纹理的方式，增加物体的AO细节；  

3、不影响其（Unity场景中）静态或者动态；  

缺点：  

1、操作较其他方式繁琐，需要对模型进行UW处理，再进行烘焙到纹理；  

2、不利于整体场景的整合（如3DMax烘焙到纹理，只能选择单一物体，针对整体场景的处理工作量巨大）；  

3、增加AO纹理贴图，不利于资源优化（后期可通过shader把通道利用起来整合资源）；  

4、只有物体本身具有AO信息，获取物体之间的AO信息工作量巨大（不是不可能）。  

### 方式二：游戏引擎烘焙AO（Unity3D Lighting）  

优点：  

1、操作简易，整体场景的烘焙，包含AO的选择；  

2、不受物体本身的UW影响，Unity通过Generate Lightmap UVs生成模型第二个纹理坐标数据；  

3、可生成场景中物体与物体之间的AO信息；  

缺点：  

1、缺少单一物体的细节（可调整参数提高烘焙细节，但换之将增加烘焙纹理数量和尺寸，以及烘焙时间）；  

2、受物体是否静态影响，动态物体无法进行烘焙，获得AO信息；  

3、其实相当于拿CPU的内存去换计算量。  

### 方式三：SSAO  

优点：  

1、不依赖场景的复杂度，其效果质量依赖于最终图片像素大小（屏幕分辨率）；  

2、实时计算，可用于动态场景；  

3、可控性强，灵活性强，操作简单；  

缺点：  

1、性能消耗较之上述2种方式更多，计算非常昂贵；  

2、AO质量上要比较离线式烘焙（上述2种）不佳（理论上）。  

## SSAO性能消耗  

![](1678971270488.png)  

### AO核心采样消耗说明  

本案例SSAO算法中，主要核心为计算AO随机法向半球的采用点，并加以半段计算AO权值。  

1、使用For结构代码进行半球随机向量的采用，If、For等对于GPU计算性能上并不友好（For循环可能会打破并行计算）；  

![](1678971270525.png)  

2、采用数的数量（上图中的SampleKernelCount，针对For循环的次数），过低的采用数得不到好的结果；以64为例，1334x750的分辨率，每个像素计算循环64次，合计1334*750*64次AO核心计算；  

3、循环体重的采样，同样以64为例，每个像素计算需要采样64次来求得屏幕深度值法线值（着色器里面采样是很耗性能的）。  

![](1678971270562.png)  

### 滤波采样消耗说明  

![](1678971270617.png)  

纵向和横向各做一次Blit  

在工程中有一段这样的代码是关于计算64次采样中法向半球的随机向量的计算  

![](1678971270661.png)  

顶点着色器中重点部分：  

![](1678971270708.png)  

## 课程参考链接：  

【环境遮罩之SSAO原理】[https://zhuanlan.zhihu.com/p/46633896](https://zhuanlan.zhihu.com/p/46633896)  

【游戏后期特效第四发--屏幕空间环境光遮蔽（SSAO）】[https://zhuanlan.zhihu.com/p/25038820](https://zhuanlan.zhihu.com/p/25038820)  

[https://www.iquilezles.org/www/articles/ssao/ssao.htm](https://www.iquilezles.org/www/articles/ssao/ssao.htm)  

【learnopengl-cn】[https://learnopengl-cn.github.io/05%20Advanced%20Lighting/09%20SSAO/](https://learnopengl-cn.github.io/05%20Advanced%20Lighting/09%20SSAO/)  

【屏幕空间环境光遮蔽（SSAO）算法的实现】[https://blog.csdn.net/qq_39300235/article/details/102460405](https://blog.csdn.net/qq_39300235/article/details/102460405)  

【Unity Shader-Ambient Occlusion环境光遮蔽】[https://blog.csdn.net/puppet](https://blog.csdn.net/puppet) master/article/details/82929708  

【SSAO与深度重构】[https://wiki.jikexueyuan.com/project/modern-opengl-tutorial/tutorial46.html](https://wiki.jikexueyuan.com/project/modern-opengl-tutorial/tutorial46.html)  

【Ambient Occlusion（AO）使用指南】[https://zhuanlan.zhihu.com/p/150431414](https://zhuanlan.zhihu.com/p/150431414)  

【图形和滤波】[http://www.ruanyifeng.com/blog/2017/12/image-and-wave-filters.html](http://www.ruanyifeng.com/blog/2017/12/image-and-wave-filters.html)  

【双边滤波】[https://blog.csdn.net/puppet_master/article/details/83066572](https://blog.csdn.net/puppet_master/article/details/83066572)  

## 小作业  

### 实现SSAO效果  

![](1678971270769.png)  

SSAO-Sample Kernel 半径调整时，动图效果如下：  

![](1678971270824.png)  

整理一些代码中比较核心的地方：  

### 生成采样核心  

在OnRenderImage开始前我们首先要做的，就是生成采样核心。每个核心中的样本将会被用来偏移观察空间片元位置从而采样周围的几何体，如果没有变化采样核心，我们将需要大量的样本来获得真实的结果。通过引入一个随机的转动到采样核心中，我们可以很大程度上减少这一数量。  

```
private void GenerateAOSampleKernel()
{
        if (SampleKernelCount == sampleKernelList.Count)
            return;
        sampleKernelList.Clear();
        for (int i = 0; i < SampleKernelCount; i++)
        {
            var vec = new Vector4(Random.Range(-1.0f, 1.0f), Random.Range(-1.0f, 1.0f), Random.Range(0, 1.0f), 1.0f);
            vec.Normalize();
            var scale = (float)i / SampleKernelCount;
            //使分布符合二次方程曲线
            scale = Mathf.Lerp(0.1f, 1.0f, scale * scale);
            //后面给向量乘一个scale是为了让生成的随机采样点更靠近当前片元，这样得到的采样点更有意义
            vec *= scale;
            sampleKernelList.Add(vec);
        }
}

``` 

根据[这篇文章](https://zhuanlan.zhihu.com/p/164992374)，我们可知上面的代码中引入了一个变量scale，这是为了让生成的随机采样点更靠近当前片元。  

相关的数学描述如下:  

![](1678971271092.png)  

曲线反应的函数是  

Distance(i)=g(i).x2+g(i).y2+g(i).z2​(1)

  

其中有  

g(i)={0.1+0.9∗(SampleKernelCounti​)2}∗vec(2)

  

我们认为公式(2)中vec的系数为scale，因此有：  

scale=0.1+0.9∗(SampleKernelCounti​)2(3)

  

其中公式(3)也可以被转化成Lerp函数的形式：  

scale=0.1∗(1−SampleKernelCount2i2​)+1.0∗(SampleKernelCounti​)2(4)

  

当公式(4)转换成Lerp函数，其因子是

scale2

，最终又重新赋值给scale：  

 ```
 scale = Mathf.Lerp(0.1f, 1.0f, scale * scale);

``` 

因此可以认为，最终与vec相乘的scale是一个二次方程图像。  

当我们调整Sample Kernel Count的时候，可以发现虽然由于随机采样会让阴影时刻变化，但是在Count数量由小到大变化的整体过程中，阴影（也就是我们的采样点范围）会在宏观上来看是减少的，是越来越靠近当前像素的。  

Count 为最小值： Count为最大值：  

![](1678971271126.png)  

原来的vec的各个分量本身就是在0~1之间的浮点数，乘上一个二次函数上取到的平滑的0~1之间的浮点数，这样就使得采样点更接近片段点了，如上图所示效果确实是这样。  

## 使用其他AO算法进行对比  

我选择的其他AO算法是——HBAO（Horizon-Based Ambient Occlusion）和 GTAO（Ground Truth Ambient Occlusion）  

![](1678971271442.png)  

### HBAO篇  

HBAO是在后处理阶段，逐像素构建半球面。采用蒙特卡洛采样计算像素对应物体坐标点的AO值。下图是nvddia官方给出的HBAO实现流程。  

![](1678971271493.png)  

根据算法介绍我们知道，需要重建对应点坐标和法线，计算AO再进行模糊处理，最终和原始图像合并得到最终效果。以下是HBAO基于u3d前向渲染的实现。  

#### 屏幕空间坐标重建  

 下图中θ为相机的fov, 均是分析对应p屏幕空间的y坐标（其中v0, v1, v2对应uv值中的v映射至-1~1）。  

![](1678971271551.png)  

 对应代码如下：  

 ```
		var tanHalfFovY = Mathf.Tan(mCamera.fieldOfView * 0.5f * Mathf.Deg2Rad);
		var tanHalfFovX = tanHalfFovY * ((float)mCamera.pixelWidth / mCamera.pixelHeight);

    	//计算相机空间:x = (2* u - 1) * tanHalfFovX * depth  (2u - 1将坐标映射到-1,1)
		mMaterial.SetVector(ShaderProperties.UV2View, 
    	new Vector4(2 * tanHalfFovX, 2 * tanHalfFovY, -tanHalfFovX, -tanHalfFovY));


``` ```
inline float3 FetchViewPos(float2 uv)
{
        float depth = LinearEyeDepth(FetchDepth(uv));
        return float3((uv * _UV2View.xy + _UV2View.zw) * depth, depth);
}

``` 

#### 计算HBAO  

上面已经介绍了半球某一特定方向上切片遮挡的计算方法，接下来使用蒙特卡洛积分，采样半球不同方向上的切片的遮挡，模拟HBAO积分结果。  

为了避免明显的交错感，每次采样时进行一次随机，让采样方向产生一点偏移。如下图所示：  

![](1678971271848.png)  

随机算法采用了[the book of shader 11](https://thebookofshaders.com/11/)中提到的value-nosie。  

```
inline float random(float2 uv) {
    return frac(sin(dot(uv.xy, float2(12.9898, 78.233))) * 43758.5453123);
}

``` 

下图时使用python matplotlib库生成的value-nosie的噪声图。  

![](1678971271891.png)  

以下则是使用蒙特卡洛方法计算HBAO的主要代码  

 ```
 for (int i = 0; i < DIRECTION; ++i)
    {
        float angle = delta * (float(i) + rnd);
        float cos, sin;
        sincos(angle, sin, cos);
        float2 dir = float2(cos, sin);
        float rayPixel = 1;
        float top = _AngleBias;
        UNITY_UNROLL
        for(int j = 0; j < STEPS; ++j)
        {
            float2 stepUV = round(rayPixel * dir) * _TexelSize.xy + input.uv;
            float3 stepViewPos = FetchViewPos(stepUV);
            ao += SimpleAO(viewPos, stepViewPos, normal, top);
            rayPixel += stepSize;
        }
    }
    ao /= STEPS * DIRECTION;

``` 

#### 室内  

![](1678971271954.png)  

调整HBAO力度，最小值->最大值：  

![](1678971272002.png)  

#### 室外  

![](1678971272089.png)  

调整HBAO力度，最小值->最大值：  

![](1678971272127.png)  

个人感觉，HBAO在处理倒角部分的阴影的时候效果挺显著的。  

### GTAO篇  

GTAO具体原理还需要多多揣摩，可以挪步观看[这篇文章](https://zhuanlan.zhihu.com/p/145339736)，可以说是SSAO的一种进化版本。  

限于篇幅，原理先不多赘述，以后慢慢填充在这里。我们直接来看效果~  

#### 室外  

SSAO 开启：  

![](1678971272179.png)  

GTAO 开启：  

![](1678971272298.png)  

可以发现GTAO的效果比SSAO要更自然一些，SSAO感觉比较硬。  

看一下动图  

调整SSAO力度，最小值->最大值:  

![](1678971272362.png)  

调整GTAO力度，最小值->最大值:  

![](1678971272414.png)  

个人认为，在比较大的室外场景中，GTAO在处理遮蔽问题上略胜一筹。  

#### 室内  

但是在物件比较多的场景下：  

SSAO 开启：  

![](1678971272573.png)  

我们看远处书架这里，SSAO处理效果看起来会比GTAO更精细一点（实际上可能不是）：  

SSAO的处理可以明显看到有一些阴影分层感  

![](1678971272679.png)  

GTAO 开启：  

![](1678971272764.png)  

远处感觉直接压暗了：  

![](1678971273102.png)  

不知道为什么，GTAO在处理室内的时候力度增加会非常地暗。  

调整了一下亮度，但感觉还是略微有点失真：  

![](1678971273416.png)  

我看了AO图，还算正常（处理细节感觉比SSAO强，感觉特别接近真实场景）：  

![](1678971273466.png)  

RO图  

![](1678971273681.png)  

法线图  

![](1678971273786.png)  

做一个总结吧（借鉴了某个知乎大佬的话）：  

GTAO其实是HBAO的一个变种, 由于SSAO计算时对半径内的采样不看遮挡关系，导致会“过黑”， HBAO则是针对当前采样方向在固定步长下，寻找一个最大的权重影响点。这就使得凹凸的表面上的遮挡不会像SSAO那么“平均”。但是问题在于，比采样点“低”的临近采样点，也会对AO做出贡献。这就需要你的场景是否有这种足够的微观程度，不然用了只是徒增性能开销。  

个人的测试中，HBAO在室内的细节遮蔽方面效果很好，GTAO在室外效果不错，SSAO处于平庸的位置。  

### 参考链接  

HBAO(屏幕空间的环境光遮蔽) [https://zhuanlan.zhihu.com/p/103683536](https://zhuanlan.zhihu.com/p/103683536)  

UE4 Mobile GTAO 实现(HBAO续) [https://zhuanlan.zhihu.com/p/145339736](https://zhuanlan.zhihu.com/p/145339736)  

Ambient Occlusion(AO)使用指南 [https://zhuanlan.zhihu.com/p/150431414](https://zhuanlan.zhihu.com/p/150431414)  

Ground-Truth Ambient Occlusion [https://zhuanlan.zhihu.com/p/150178776](https://zhuanlan.zhihu.com/p/150178776)
# 4.3 实时阴影
需要补一下games202 阴影内容
[实时阴影技术总结 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/45805097)
[实时渲染中的软阴影技术 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/26853641)
## 一、基于图片的实时阴影技术
### 1、平面投影映射
#### ①平面投影阴影
-   平面投影映射并不是一个基于图片的解决方案
-   **内容**：
	-   根据光的方向，把物体的每个顶点投影到平面地面上。
![[Pasted image 20221209130631.png]]
-   **数学原理**：
	-   相似三角形
	-   光Light所在的点L已知，V已知，P很容易就能求得

-   **缺点**：
	-   只能投影到平面（阴影的接收物只能是平面）
	-   投影物体必须在光线和平面之间
#### ②投影阴影

-   **平面投影阴影的阴影接收物只能是平面**，为了**在曲面上得到阴影**，做了以下改进：
-   **投影阴影**：
	-   把光源当做一个相机/投影器
	-   然后将阴影投影渲染到一张纹理
	-   最后**渲染阴影接收者时**，将上一步得到的阴影合并进去
- ![[Pasted image 20221209144815.png]]

**投影阴影在Unity中的实现**
![[Pasted image 20221209144939.png]]
-   第一步，设置Project组件，通过它的参数使用给它的材质生成一个视锥体
-   第二步，使用Render Texture生成一张纹理，将阴影绘制到纹理中
-   第三步，将设置Project组件的物体和阴影纹理进行混合

### 2、阴影映射（ShadowMapping）
#### 理解：阴影

-   **如何确定“是不是阴影”**
	-   我们能看见+光能看见 = 正常渲染
	-   我们能看见 + 光看不见 = 阴影

-   **GAMES101中，讲过的关于阴影的理解：**
	-   是一个经典的双Pass做法
	-   Pass1：从光源看向场景，记录看到点的深度
		-   需要从光源的位置渲染整个场景的深度图（这张图就是**shadowmap**）

	-   Pass2：从相机看向场景
		-   再从相机位置渲染这个场景

	- 在投影回光源所在的图像上，比较在相机位置渲染和光位置的深度，结果可以这样理解：
		-   **深度一致：说明相机和光都能看到**
		-   **深度不一致：我们能看到，但是光看不到 =>阴影**
		![[Pasted image 20221209145349.png]]
#### 阴影映射（Shadow Map）
![[Pasted image 20221209145547.png]]
-   光源视角下（shadowmap）
-   我们从光源看向场景的角度渲出一张深度图，这张深度图：
-   离光源越近就越黑（0）；越远就越白（1）
-   这张图就反映出了场景中物体的远近关系

![[Pasted image 20221209145655.png]]
**阴影映射的流程**
-  1. 从光源的位置生成一张深度图，这一步称之为阴影映射（ShadowMap）。
-  2. 从摄像机的视角，渲染整个渲染整个场景的物体。每次渲染时，需要和shadowmap的深度做比较（深度测试）。
>比如左下角红色点的位置，他的深度是摄像机视角的深度，所以要先转换成阴影映射的坐标系，保证坐标系是一致的。然后比较深度值，如果一个**片元的深度>它在shadowmap中的深度**，那么就认为它在阴影中。

**总结/注意事项：**
-   ShadowMapping本质上是一种图像空间做法，也就是说生成shadow这一步不需要这个场景的几何信息
-   ShadowMapping只能做**硬阴影**

**软硬阴影的区别**：硬阴影没有一个明显的从有阴影到没有的过度/界限（因为绝大多数的生活中的光源是面光源）
![[Pasted image 20221209164343.png]]
### 3、Unity中的屏幕空间阴影映射
#### 步骤
1.  渲染屏幕空间的深度贴图
2.  从光源方向渲染出shadowmap
3.  **在屏幕空间**做一次阴影收集计算（Shadows Collector，将前两步生成的图做深度比较），
		a.  **这次计算会得到一张屏幕空间的shadowmap（Collect Shadows）**
		b.  //实际上就是对前两步的深度图做一个比较，得到一张新的深度图
4.  在绘制物体时，用物体的**屏幕坐标uv**采样第三步中生成的屏幕空间shadowmap

从FrameDebugger查看过程
![[Pasted image 20221209151539.png]]
![[Pasted image 20221209151553.png]]
## 二、阴影映射的问题与优化
### 1、问题1：自阴影/自遮挡
#### 自阴影self-shadowing：

-   ![](https://cdn.nlark.com/yuque/0/2021/png/12962324/1633341927186-40aabad1-e7a8-4c89-88cd-56b34a595ea6.png?x-oss-process=image%2Fresize%2Cw_426%2Climit_0)![](https://cdn.nlark.com/yuque/0/2021/png/12962324/1633445543605-b88095b0-6757-403f-8343-3bc09f0d2905.png)
-   因为shadowmapping的分辨率有限，离散的采样点以及数值上的偏差可能造成不正确的自遮挡效果
-   也被称为阴影粉刺（Surface Acne）**

 **出现自阴影的原因：**
-   如下图所示，在映射shadowmap的过程中，用表面上**点的uv**采样阴影映射的深度值都是一个值（shadowmap的一格代表的是一个纹素，在这一格里面采样阴影映射得到的值都是一样的）
- 例如：红点和蓝点采样后得到的深度值都是一样的，这一段所有点采样得到的深度值都和红线（代表深度值）相等。
![[Pasted image 20221209153218.png]]
当比较深度时，为了避免表面自阴影，需要设置容错阈值：
- 深度偏移（Depth Bias），增加深度偏移会使该像素向光源靠近
- 法线偏移（Normal Bias），沿着表面法线方向向外偏移
- 偏移单位是shadowmap的纹素

**深度偏移会造成的问题（Peter Panning）**
-   当Bias设置过大时，会导致漏光现象，即阴影与投影者之间发生脱节：也叫做**Peter panning问题（学术界叫作detached shadow）**
-   //Peter Panning名称的由来：Peter是西方漫画中的人物，他和影子是可以分开的
![[Pasted image 20221209152205.png]]
#### GAMES101补充理解
![[Pasted image 20221209152350.png]]
-   从light往场景看（左边太阳的线）时，比如沿着某一个像素往过看，看到的位置（红色斜线）就是像素所代表的深度，（假设任何一个像素是一个常值的深度）。也就是说，在shadowmap看来，场景离散成了图中红色斜线形成的结果。
-   第二个pass渲染的时候，从camera出发（右边连着眼睛的蓝色虚线），常规操作：连向light （左边蓝色虚线），这段虚线长度就是它的深度
-   **问题**：在shadowmap记录的对应的深度是橙色的部分（更浅），而实际上从相机看向这个像素，它的深度应该是被橙色部分遮挡住的点（两条蓝色虚线的交点）→这就是自遮挡情况
-   **特殊情况**：

-   光垂直的，从上往下照时候，不存在问题
-   光非常偏的时候，问题最大（例如：塞尔达中夕阳西下时）
-   //不难理解，斜着的时候就完全被挡住了，垂直的时候就不会被遮挡，图已经画的很清楚了

**如何解决这个问题：**
![[Pasted image 20221209153545.png]]
-   加上一个所谓的Bias避免自遮挡（图中橙色那段）
	-   认为shadowmap上的深度，明显比实际的深度小的情况下，橙色的这一段障碍物就不算了。
	-   bias可以不是一个常数，可调整，如果垂直打光，可以非常小，夹角很大的情况，可以更长一点。
#### Unity中实现自阴影的优化
-   思路：
-   在生成shadowmap时去做Bias（偏移）
![[Pasted image 20221209153656.png]]
### 2、问题2：走样
#### ①走样问题
-   走样，最明显的表现就是锯齿，我们可以看下边的例图
![[Pasted image 20221209153803.png]]
#### ②走样问题的具体内容和解决方法
**在什么阶段会走样？**
-   初始采样：渲染shadowmap时
-   重采样：从相机位置对shadowmap进行重采样时

**初始采样阶段**
-   最严重的问题：**透视走样**
-   透视走样是如何形成的？
![[Pasted image 20221209162811.png]]
-   shadowmap在世界空间均匀分配（左图的三段纹素对应的世界空间的一个像素大小是一样的）
-   **经过透视投影后**，根据近大远小的原理，原来大小不一样的近平面和远平面，在屏幕内占的像素便一样了。（右图）
-   这时远平面对应在shadowmap中的纹素就比近平面大了
-   这种离相机近的部分走样的情况一般称为**透视误差**。

-   **如何解决**？
-   思路1：
-   因为我们在使用shadowmap时，相机是经过透视投影的，但生成shadowmap时并没有经过透视投影
-   所以我们在生成shadowmap时就进行一次透视投影

-   思路2：
-   尽量减少近平面和远平面之间的像素差距
-   Unity中的**级联阴影映射**（投影走样最有效的解决方案）就是用的这个 思路
![[Pasted image 20221209163324.png]]

#### 级联阴影映射
(Cascaded Shadow Map)
- 投影走样最有效的解决方案
- 把视锥体分割为多个子视锥体；
- 为每个子视锥体计算独立的相等大小的阴影映射；
-   //就是给shadowmap不同位置不同的分辨率
#### 重采样误差
-   shadowmap可以理解为一张动态生成的纹理
-   重采样误差的解决方法：**滤波（Filter）**
 ![[Pasted image 20221209163546.png]]
-   滤波：在图像处理中，通过滤波强调一些特征或者去除图像中一些不需要的部分(高斯模糊就是一种滤波方式)
-   滤波是一个**领域操作算子**，利用给定像素周围的像素的值决定该像素的最终输出值

-   **补充**：
-   滤波就是抹掉特殊频率的东西
-   高通滤波 = 边界
-   低通滤波 = 模糊
-   //滤波（Filtering）=卷积（Convolution）=平均（Averaging）

-   **阴影滤波**：
-   使用一部分shadowmap采样点来计算某个指定View采样点的最终阴影结果的方法

#### PCF滤波
![[Pasted image 20221209163837.png]]
-   **一句话总结**：PCF中，不是取shadowmap上一个点的深度去比较，而是：取shadowmap上任意一个点的周围一个Filter大小的区域（滤波核），将**这个区域的所有点都去做一次**深度比较（0或1的结果），最后再对这些结果做一个平均（不是非0即1的结果，而是一个权重值/像素的颜色值）
-   **采样数K**（其实就是Filter的大小）
-   可以是规则滤波，3x3或者5x5等
-   也可以采用泊松滤波（Poisson DIsk）的形式来分布一定数量的采样点![[Pasted image 20221209164052.png]]
**Filter的大小重要吗？**
-   如果小（比如1×1 = 没做Filter），结果是锐利（sharpener）对应硬阴影
-   如果大，结果是softer，对应软阴影
-   既然Filter的大小可以决定阴影的软硬，我们不妨这样理解：
-   软阴影 = 硬阴影做一个非常大的Filter
**现实世界的软硬阴影**
![[Pasted image 20221209164914.png]]
-   阴影在笔尖的地方是硬的，远的地方就很虚（软）
-   也就是说：**阴影的软硬和遮挡物的距离有关（注意：不是和光源的距离）**
-   **关键结论**：
-   要想做一个软阴影的效果，应该给硬阴影各个位置不同大小的filter

-   **那么这个不同位置不同的Filter的大小怎么解决呢？**
-   既然阴影的软硬和遮挡物距离有关，那么我们就做一个定义
-   定义一个**blocker distance（遮挡物和阴影接收物的距离**）
-   //更准确的说法：相对的、平均的、投射的遮挡物的深度

-   **到此为止我们做个总结：**
-   我们知道了PCF怎么做
-   要做PCF，我们还需要知道Filter的大小，接下来就是PCSS的舞台了

#### PCSS
-   PCSS是古老的方法，是经过探索之后最终的选择（因为现在的降噪技术ok）
-   **PCSS是产生真正软阴影的方法**

-   值得注意的是，PCF最早研究出来是为了做阴影边缘的反走样的
-   **用PCF做软阴影就是PCSS**

![[Pasted image 20221209165125.png]]
-   上方黄色线段表示Light（光），中间绿色线段表示Blocker（遮挡物），下边蓝色线段就是Receiver（阴影的接收物）
-   将Light 和Blocker连到Receiver上，就可以看到很明显的相似三角形
-   通过相似三角形关系我们就可以得到以下信息：

-   蓝色虚线=dReceiver，绿色虚线 = dBlocker
-   **绿色虚线 /（蓝色虚线 - 绿色虚线） = WLight/ WPenumbra
-   当遮挡物（Blocker）离接收阴影物的距离变小，WPenumbra也会变小（阴影变硬），反之变大（阴影更软）
-   WPenumbra就是filter的范围

-   我们继续分析
-   现在唯一的问题就是：该Filter多大？

-   **这取决于 light的大小，也取决于blocker depth dblocker
-   平常指的blocker depth是说average blocker depth，意思就是：对于一个shading point，要看在一定的范围内，有多少能够挡住它的，在shadowmap上记录的像素，这些像素记录的深度的平均值是什么

-   这样一来，人们就可以把PCF的思想用在PCSS上了

**PCSS的核心思想**
-   为了知道Filter要多大，得知道blocker到shading point的距离是多少

-   **Step1：Blocker search**
-   从shading point连向点光源，找到一个点，去周围的某一个区域，来判断是不是在阴影里，如果在，那个像素就是一个blocker，记下来。所有的都走一遍，记下所有的blocker的深度记下来，然后取一个平均。

-   **Step2：Penumbra estimation**
-   用average blocker 计算 filter size

-   **Step3：做PCF**
-   既然知道了Filter的大小，就可以计算PCF了

-   //第2、 3步和PCF没有区别：第一步之后，知道了blocker的距离，就可以计算filter size，知道filter多大，就和之前PCF做法完全一样。

-   在第一步Blocker search时，在多大的范围内search呢（在多大的范围找blocker）？

-   是一个鸡生蛋蛋生鸡的问题：
-   本来就是为了确定在shadowmap的周围多大的范围做PCF，为了知道这个信息，就要知道average blocker depth信息。要知道average blocker depth的话，也得先取一个区域找average blocker，再把他们平均起来。

-   取多大？
-   ①可以取固定大小的范围
-   ②更好的方法：shading point连向light，覆盖shadowmap的区域（红色区域），我们只在这个区域范围内找blocker
![[Pasted image 20221209165510.png]]
#### VSSM

-   针对解决了PCSS中第一步和第三步慢的问题（甚至不用做任何采样和循环，但是做了大量的近似）
-   对应的是GAMES202-L4部分
-   考虑的篇幅问题，就不再整理了，我直接贴笔记地址，感兴趣的自行扩展
-   [https://www.yuque.com/sugelameiyoudi-jadcc/okgm7e/gcuczo](https://www.yuque.com/sugelameiyoudi-jadcc/okgm7e/gcuczo)

#### Moment Shadowmapping
-   VSSM的发明是为了解决PCSS的问题，其实VSSM自己同样会有问题
-   当分布非常简单的情况下，就会出错（分布描述的不准）
-   Moment ShadowMapping解决了分布描述不准的问题

# 4.4 抗锯齿
前向渲染：SSAA、MSAA、CSAA，RGSS 等
延迟渲染：FXAA、MLAA、SMAA 等
时域上的抗锯齿：temprial anti-alasing，TXAA
基于深度学习：DLAA

## 一、锯齿是怎么产生的
图形渲染中的锯齿是指在渲染结果图像上颜色剧烈变化的区域出现锯齿状一样的斑纹的现象。在图像色彩边缘线上，由于边缘两边的颜色差异很大，这样的现象特别明显。

**锯齿产生的原因**就是因为信号的变化频率高，而相应的采样频率低。就三角形边缘不规则的情况来说，因为三角形的边上是无限多个点，而用有限个方块去逼近无限多个点的三角形的边，所以当然会产生不规则的锯齿，硬件的解决办法一是可以加大屏幕的分辨率，使得像素变小，从而可以得到更多个有限的方块去逼近三角形。而软件的方法就是加入抗锯齿算法。无论是硬件方法还是软件方法，都不能完全解决锯齿问题，只能缓解锯齿问题，直到人眼察觉不出来

**锯齿（Jaggies）现象也被称为走样（Aliasing）**。常见的走样有几何走样，着色走样、动画走样。**消除锯齿现象的技术就是抗锯齿，也被称为反走样（Anti- Aliasing，AA）**。

如下图可以看到，在光栅化阶段进行图元转化时，当三角形覆盖像素中心的时候输出颜色，反之不输出，就会出现下图中右边的锯齿情况。
![[Pasted image 20221209170311.png]]
## 二、常见的抗锯齿

### 1、SSAA超采样抗锯齿

**SSAA(Super Sample Anti-aliasing)**

SSAA本质上是先渲染出一张面具为屏幕分辨率为N倍的图像，然后再通过降采样的方式输出最后的渲染图像，这样输出一个屏幕像素就对应原来N个采样点计算的屏幕大小的图像，这是理论上最完美的抗锯齿。

先渲染一张大分辨率的图像，然后把它缩小，这样它的锯齿效果就减弱了，大致如下图：
![[Pasted image 20221209170357.png]]
上图中最右边有明显的锯齿，但是通过降采样一直输出到左边的话，这种锯齿感从肉眼来看的话就会明显地减弱。

假设最终屏幕输出的分辨率是800x600,4xSSAA就会渲染到一个分辨率1600x1200的Buffer上，然后再把这个放大4倍的Buffer采样输出至800x600。**这种做法在数学上是最完美的抗锯齿。但是缺点也很明显，计算量增大了4倍，render target的大小也涨了4倍。**
### 2、MSAA多重采样抗锯齿

**MSAA(Multi sample Anti-aliasing)**

MSAA是GPU在光栅化阶段对片段进行超采样，单个像素中的所有采样都决定像素中心点的着色结果，最后根据样本的覆盖率来计算像素最终颜色的方法。

它的原理是这样的，在逐像素阶段，如果我们只布置一个采样点，虽然如下图左边三角形虽然占据了像素一部分，但是却没有包含中心采样点，那么不输出颜色，，反之如果我们**布置了多个采样点**的话，那么根据覆盖的样本的百分比来输出最终的颜色，就如下图右边所示。
![[Pasted image 20221209170602.png]]
这种抗锯齿方式，**布置的采样点越多越准确，但是随之计算的性能消耗也会增大。**

下面是MSAA抗锯齿输出的颜色示例：
![[Pasted image 20221209170610.png]]
但是MSAA也有一些缺点：
1、MSAA在与HDR一起使用时可能会产生问题，必须进行专门处理。
2、由于MSAA是在光栅化阶段对图元进行超采样，因此它只能解决几何走样现象(即物体轮廓地锯齿)，不能解决着色走样(高光闪点)问题。
3、延迟渲染无法支持MSAA。

**CSAA覆盖采样抗锯齿(Coverage Sampling Anti-Aliasing)**

在MSAA的基础上，将用来计算Coverage的采样点与用来存储色彩的采样点进行分离，这样可以用较少的显存来存储较多的专门用来计算Coverage的采样点，让覆盖率计算的更加精确，从而提升视觉效果。从结果上看，CSAA在性能上比4x的MSAA稍微低一些，但也可以达到8x或16xMSAA的效果。
![[Pasted image 20221209170912.png]]

**EQAA(Enhanced Quality Anti-Aliasing)**

EQAA（Enhanced Quality Anti-Aliasing），是 AMD 在其 GPU 产品中对 MSAA 的优化实现。在 EQAA 中，也采用了与 CSAA 相同的做法，将计算 Coverage 的采样点和计算 Color 的采样点分开存储，例如，4f8x EQAA 表示有4个颜色采样点并且总共8个采样点。
![[Pasted image 20221209170926.png]]
EQAA与CSAA一样，也采用了Coverage与Color两种类型的采样点。另外EQAA还使用了一个表格来存储采样点的颜色、深度、模板数据，而在采样点上通过索引的方式引用表格中的数据。因为在单个像素中，采样点公用数据的几率非常大，因此这种方法可以节省大量的显存资源。
![[Pasted image 20221209171009.png]]
### 3、TAA时间抗锯齿

**TAA时间抗锯齿(Temporal Anti-Aliasing)**

TAA的核心思想是将点SSAA采样空间不同样本方法改成使用多个历史帧的渲染结果作为样本来达到类似SSAA的效果。**它的实现需要N倍的显存空间，但是不需要额外的着色计算。**

实现思路如下：

1、把多次采样的过程分布到每一帧中去，每一帧都平均前面几帧保存下来的数据。从下图可以看到将4次采样分布在4帧的运算中。

2、每一帧会有一定的偏移，继承了MSAA采样。从下图可以看到每一帧的采样点的位置并不一样。

3、用motion vector保存每帧移动的偏移。
![[Pasted image 20221209171131.png]]下面是抗锯齿的前后对比：
![[Pasted image 20221209171223.png]]
在实际工程中，需要注意TAA带来的一些细节问题；

1、因为灯光改变、遮挡改变、相机或场景运动所带来的历史帧中的样本失效问题。
2、因为样本来自于多个历史帧而产生的画面收敛延迟(残影)的问题等等。
3、HDR协同工作的问题。

**TXAA**

TXAA 是 NVIDIA 在 GTX 600 及以上版本的显卡上基于传统 MSAA（CSAA？） 的基础上，引入了时间过滤器（TAA的思想）的硬件抗锯齿技术。TXAA 旨在减少时间性锯齿 (运动中的蠕动和闪烁)。 该技术集时间性过滤器、硬件抗锯齿以及定制的 CG 电影式抗锯齿解算法于一身。 要过滤屏幕上任意特定的像素，TXAA 需要使用像素内部和外部的采样以及之前帧中的采样，以便提供最高画质的过滤。 TXAA 在标准 2xMSAA 和 4xMSAA 的基础上改进了时间性过滤。 例如，在栅栏或植物上以及在运动画面中，TXAA 已经开始接近、有时甚至超过了其它高端专业抗锯齿算法的画质。TXAA 由于采用更高画质的过滤，因而与传统 MSAA 较低画质的过滤相比，图像更加柔和。

**MFAA多帧采样抗锯齿(Multi-Frame Sampled Anti-Aliasing)**

Maxwell 架构提供了一种名为动态超分辨率（Dynamic Super Resolution，DSR）的技术，能在硬件上实现超采样。另外 Maxwell 将AA的样本数据存放在了 RAM 中（以前的架构都是将 AA 样本数据存储在 ROM中），从而提高了样本存储的灵活性和可编程性。借助这样的新的架构，实现了 MFAA。由于 RAM 中的数据是可以自由读写的，通过在时间和空间上交替 AA 样本的模式，MFAA 的样本在历史每帧（甚至同一帧的不同时刻）都可以不同。

4x MFAA 的画质于 4x MSAA 相当，但只需要 2x MSSAA 相当的性能开销。
![[Pasted image 20221209171352.png]]

### 4、FXAA快速近似抗锯齿
后处理抗锯齿方法，渲染完之后进行后处理

**FXAA(Fast Approximate Anti-Aliasing)**

它的特点如下：
1、利用边缘检测有效的模糊混合。
2、是在后处理完成不依赖硬件支持。

测试场景中对应的后处理图像：
![[Pasted image 20221209171451.png]]
**FXAA本质就是在后处理中对图像进行描边，然后对描边图像进行模糊，之后再与原图像进行混合。**

下图是对应的效果图：
![[Pasted image 20221209171540.png]]
相比于MSAA，FXAA的目标是**速度更快、显存占用更低**，还有着**不会造成镜面模糊和亚像素模糊**(表面渲染不足一个像素时的闪烁现象)的优势，而**代价就是精度和质量上的损失**。

**MLAA形态学抗锯齿(morphological Anti-Aliasing)**

它的基本思路是检测每帧图像的边缘(通常可对亮度、颜色、深度或者法线进行边缘检测)，然后对这些边缘进行模式识别，归类出Z、U、L三种形状，根据形状对边缘进行重新矢量化(re-vectorization)，并对边缘上的像素根据覆盖面积计算混合权重，将其与周围的颜色进行混合，从而达到平滑锯齿的目的。
![[Pasted image 20221209171721.png]]
以下图为例详细说明。用绿色标记的线条为检测到的Z形边缘，从这些边缘我们可以推断出原始边缘的形状，即图中蓝色线条。这个过程叫重新矢量化。此时我们能够得知边缘附近每个像素被蓝色线条截断了百分之多少。根据此信息将当前像素与邻近像素的颜色混合，便能得到平滑的边缘。
![[Pasted image 20221209171749.png]]
像素被蓝线截断面积的百分比（小于50%的部分）叫做权重因子。对于任一个边缘像素，要计算其权重因子，只需要知道d_left、d_right，以及两端交叉边的朝向（即边缘的形状，此处为Z形）即可。其中d_left和d_right为当前像素距离边缘两端的距离。得到权重因子a后，通过如下计算实现边缘平滑：
![[Pasted image 20221209171850.png]]
为了节省计算资源，Jimenez为每种形状模式预计算了一张查找表（称作areaTex）以便快速获取权重因子，如下图所示：
![[Pasted image 20221209171901.png]]
注意贴图中的每个像素保存了两个数值（分别储存在R和G通道里），这两个数值分别代表当前像素及共享同一边缘的邻接像素的权重因子。

以上是仅针对水平方向锯齿的情况，对于垂直方向的锯齿，处理方式类似。

MLAA 天然的能与延迟着色完美配合，但它只能用来消除几何锯齿，不能消除着色锯齿。

**SMAA增强型子像素形态学抗锯齿(Subpixel Monorplogical Anti-Aiasing)**

SMAA是思路是建立在Jimenez版MLAA的基础之上，但是每个步骤都经过了强化或者彻底的更新。

Jimenez版MLAA的处理流程如下图所示：
![[Pasted image 20221209171916.png]]
分为三个步骤：1、边缘检测；2、计算权重因子；3、混合周围像素。

SMAA在（b）、（c）的基础上加入了针对尖锐几何特征的处理，并加入了对角线模式识别；在（d）中加入了对局部对比度的考虑；在（e）改善了距离搜索算法。

详情查看原文：[(7条消息) SMAA算法详解_qezcwx11的博客-CSDN博客](https://blog.csdn.net/qezcwx11/article/details/78426052)

### 5、DLSS深度学习超采样

**DLSS(Deep Learning Super Samping)**

DLSS（Deep Learning Super Sampling），深度学习超采样，是 NVIDIA 于 2018 年在 GeForce 20 系列的显卡上推出的一种基于深度学习算法进行图像缩放的技术。2020 年 NVIDIA 推出了 DLSS 2.0。

这项技术在Turing架构中首次引入，它的工作原理是利用NVIDIA神经图形框架NGX，在超级计算机中以极低的帧率和每像素64个样本对数万张高分辨率的精美图像进行离线渲染，训练出一个深度神经网络。基于无数个小时的训练所获得的数据，网络就可以将分辨率较低的图像作为输入，构建高分辨率的精美图像。

训练好网络后，NGX会通过Game Ready驱动程序和OTA更新将AI模型传递到GeForce RTX PC或笔记本电脑。并借助RTX GPU 中专用于AI计算的Tensor Core，DLSS网络可以与密集的3D游戏同时实时运行。

![[Pasted image 20221209172002.png]]

# 4.5 DOF景深
先理解下概念。。
## 一、什么是景深

维基百科是这样描述的：
景深(Depth of fielf,DOF) 景深是指相机对焦点前后相对清晰的成像范围。在光学中，尤其是录影或是摄影，是一个描述在空间中，可以清楚成像的距离范围。虽然透镜只能够将光聚在某一固定的距离，远离此点则会逐渐模糊，但是在某一段特定的距离内，图像模糊的程度是肉眼无法察觉的，这段距离称之为景深。当焦点设在超焦距处时，景深会从超焦距的一半延伸到无限远，对一个固定的光圈值来说，这是最大的景深。

下图是景深的示例：
![[Pasted image 20221209172919.png]]
可以看到上图中间的蝴蝶部分是清晰的，而其它都是模糊的。

那么为什么画面中特定的部分是清晰的，而另一些则是模糊到几乎无法辨认？

有一个概念叫离散圈，能够帮助我们理解为什么照片的特定部分是清晰的，而其它部分是模糊的，离散圈是指在准确对焦的时候，场景中的一个点会被完美还原出来，就是指下图的合焦状态，而在对焦错误的情况下，这个点会在画面中形成一个错误的圆形。

![[Pasted image 20221209173022.png]]
### 二、景深的作用

景深的作用：

选择性的突出或者强调画面中的一部分，例如某个物体或者某个人物，吸引观察者的注意力到画面中清晰对焦的部分，而忽略其它的模糊部分的细节。

强调所拍摄场景的深度，增加画面的层次立体感。

艺术意境的表达。摄影师可以利用景深效果，营造处虚幻、梦境、或者神奇等意境。

表示主观的视线。在电影学中，通过调节浅景深的镜头，使之对焦在不同的位置上，来表示某个人的主管视线的转移。

交代人物的关系。在电影学中，通过景深聚焦位置的变化，来表达前景和背景人物之间的关系。

下面是景深的视频示例：
![[Pasted image 20221209173338.png]]
### 三、移动端景深效果实现

**景深的效果是在后处理阶段完成的**：
![[Pasted image 20221209173418.png]]
它的制作思路这样的，先制作景深mask，然后对场景进行模糊，之后再和正常场景插值合并。
![[Pasted image 20221209173440.png]]
四、高级景深效果思路拓展

景深渲染中出现的三种情况的示意图以及相对应的滤波核。下面a图中p点在背景区域，b图中p点在前景区域，c图中p点在聚焦区域。
![[Pasted image 20221209173449.png]]
# 4.6 雾效（鸽）
指数高度雾：[【百人计划先行版】大世界雾效实现_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1eq4y177bq/?vd_source=9d1c0e05a6ea12167d6e82752c7bc22a)
大气散射（难）：[【百人计划】图形 4.6 雾之 大气散射_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV11d4y1o7f4/?spm_id_from=333.999.0.0&vd_source=9d1c0e05a6ea12167d6e82752c7bc22a)

[游戏魔法编程：unity实现完整大气散射 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/548799663)

# 4.7 基于屏幕空间的溶解（鸽）
