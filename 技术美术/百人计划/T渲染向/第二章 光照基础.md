# 2.1 颜色空间
[色彩空间表示与转换 ](https://zhuanlan.zhihu.com/p/24281841)
# 2.5 凹凸映射 Bump Map 
[【UnityShader】ParallaxMapping 视差映射（7）](https://zhuanlan.zhihu.com/p/574361162#ref_4)

[[图形学/书籍/《Real-Time Rendering 3rd》/第六章 · 纹理贴图及相关技术/README#8.1 凹凸贴图 Bump Mapping]]
![[Pasted image 20221029194237.png]]
![[Pasted image 20221029194339.png]]
## 物体细节的尺度
在毛星云的这篇文章中，提到了**三个尺度**：
![[技术美术/PBR/PBR白皮书/content/4 法线分布函数/README#一、基于物理的渲染理念：从宏观表现到微观细节]]

**本科中，老师给出了描述一个物体绘制在屏幕上（表达物体的细节）的三个尺度

-   **宏观尺度**
-   特征可能覆盖很多个像素
-   由顶点、三角形、其他几何图元表示
-   例如：角色的四肢、头部

-   **中观尺度**
-   特征可能覆盖几个像素
-   描述了宏观和微观尺度之间的特征
-   包含的细节比较复杂，无法用单个三角形进行渲染，
-   细节相对较大，可以被观察者看到几个像素 以上的变化
-   例如：人脸上的皱纹、肌肉的褶皱、砖头的缝隙

-   **微观尺度**
-   特征可能是一个像素
-   通常在着色模型，写在像素着色器中，并且使用纹理贴图作为参数
-   模拟了物体表面微观几何的相互作用
-   例如：
-   有光泽的物体表面是光滑的、漫反射的物体，在微观下表面是粗糙的
-   角色的皮肤和衣服看起来也是不同的，因为使用了不同的着色模型/不同的参数
## Bump Mapping

模拟**中观尺度**的常用方法之一，可以让观察者感受到比模型尺度更小的细节
法线贴图所带来的凹凸感是怎么来的？是改变了【光照信息】（NdotL）而来。我们视觉上认为，较亮的向光面与较暗的背光面会组成一个“立体”的事物。

**基本思想**：
-   在纹理中将尺度相关的信息编码进去
-   着色过程中，用受到干扰的表面去代替真实的表面
-   这样一来，表面就会有小尺度的细节

**原理**：
-   **对物体表面贴图进行变化然后再进行光照计算的一种技术**
-   主要的原理是通过改变表面光照方程的法线，而不是表面的几何法线，或对每个待渲染的像素在计算照明之前都要加上一个从高度图中找到的扰动，来模拟凹凸不平的视觉特征
-   例如：
-   给法线分量添加噪音（法线映射贴图）
-   在一个保存扰动值的纹理图中进行查找（视差映射、浮雕映射贴图）
-   是一种提升物体真实感的有效方法， 且不用提升额外的几何复杂度（不用改模型）

-   列举一个使用法线贴图的效果
![[Pasted image 20221029193834.png]]
-   可以看到，使用了法线贴图的有了明显的立体感和细节
-   对于中间的高亮部分，左边的都是均匀的，而右边即使是很高亮度的部分也能看到阴影
## Normal Mapping

![[Pasted image 20221029194529.png]]

![[TA101#NormalMap]]
## 视差映射 Parallax Mapping
参考：[【UnityShader】ParallaxMapping 视差映射（7） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/574361162)
### 原理
-   视差贴图Parallax Mapping，又称为 Offset Mapping
-   主要为了赋予模型表面遮挡关系的细节。引入了一张高度图
-   可以和法线贴图一起使用，来产生一些真实的效果
-   高度图一般视为顶点位移来使用，此时需要三角形足够多，模型足够精细，否则看起来会有块状
-   如果在有限的三角形面的情况下，怎么办？这就用到了视差映射技术
-   **视差映射技术**：
 ![[Pasted image 20221030144850.png]]
>左：无视差 右：基础视差
- 核心：改变纹理坐标
-   需要一张存储模型信息的高度图，利用模型表面高度信息来对纹理进行偏移（例如：低位置的信息被高位置的信息遮挡掉了，所以会采样更高的信息）
### 视差映射的实现

-   和法线贴图一样，是欺骗眼睛的做法（只改变纹路，不增加三角形）
-   我们的模型在**切线空间**下，所有的点都位于切线和副切线组成的平面内（图中0.0点），但实际上物体要有更丰富的细节。

-   例如图中的情况
-   如果不使用视差贴图，要计算当前视角下，片元A点（黄色）的信息，就是图中的Ha
-   实际使用视差贴图时，真实的情况应该是视线和A点延长线和物体的交点，也就是B点，相应的就是<font color="#ff0000">Hb</font>
![[Pasted image 20221029211242.png]]
-   **视差映射的具体算法**：如何在知道A的uv值的情况下，算出B的uv值

-   知道AB两者的偏移量即可
-   **偏移量的获得**：用近似的方法去求解

-   首先拿A的高度信息进行采样，得到物体表面距离水平面（0.0）的深度值Ha。
-   用深度值Ha和视线的三角关系算出物体上等比的偏移方向，算出近似的B点（可以看到图中近似点B和实际点B还是有挺大差距的，所以模拟度比较低）
![[Pasted image 20221029211255.png]]
![[Pasted image 20221029211302.png]]
```c
// uv高度图采样
float height = tex2D(_HeightMap,i.uv).r;  
//根据高度图信息计算出uv偏移量
float3 tangent_ViewDir = normalize(mul(TBN, world_ViewDir));  
float2 offsetUV = tangent_ViewDir.xy / tangent_ViewDir.z * height * _HeightScale;  
i.uv += offsetUV;
```
得到偏移之后B点的uv，再去对法线贴图进行采样、计算时，就不会采样A点了，而是B点

其中`viewDir.xy / viewDir.z`为什么能代表uvOffset？见下图：
![[Pasted image 20221029233656.png]]
>不同视线的uv offset应该不一样
![[Pasted image 20221029233719.png]]
>1. 以90°角直视表面时，切线空间中的视图方向等于表面法线（0、0、1），因此不会发生位移。视角越浅，投影越大，位移效果越大。  
>2. xy/z 之所以能得到正确的offset，是由于定义高度差为1，使用“相似三角形”原理而来。  
1 / offset = z / (x,y) => offset = xy/z


- **理解：视差贴图是如何产生遮挡效果的**

-   当视线看到的是A点这样深度吗 比较大的，那么视差贴图计算出的偏移值也是非常大的，这样A点最终被渲染出来的机会就比较小（偏移后就被采样到其他点上了）
-   当视线看到B点这样深度比较小的点，计算出来的偏移就比较下，甚至原来点的附近，所以被采样的机会就比较大
-   深度大的点很容易被深度小的点覆盖掉，这样就会表现出遮挡的效果


## 陡峭视差映射 Steep Parallax Mapping
![[Pasted image 20221029212354.png]]
-   也是近似解，但比视差映射精确

  **基本思想**：
  采用了**光线步进**（RayMarching）的方法。
-   将物体表面分为若干层，从最顶端开始采样，**每次沿着视角方向偏移一定的值**
-   如果当前采样的层数，**大于**实际采样的层数，就停止采样。
-   例如图中D点，采样到0.75层，实际是0.5层，就停止采样，返回偏移坐标

 **陡视差映射的算法**：（计算偏移点的过程）
 定义步长、循环采样、判断高度信息、得到偏移量。
 
-   首先对A点采样，得到深度大约为0.8的位置，而其对应视线深度为0.0，不符合我们的基本思想，继续采样
-   采样B点，深度为1，视线深度为0.25，不符合，继续采样
-   采样C点，深度大约为0.8，视线深度为0.5，不符合，继续采样
-   采样D点，采样深度为0.5，视线深度约为0.75，符合上述的条件，认为是比较合理的一个偏移点，就返回结果（return）。

**陡视差的问题**：
-   在于分层机制，如果
-   分层多，性能开销就会大；
-   分层小，渲染锯齿就比较明显。
-   一种做法：可以根据视角v和法线n的角度限定采样层数
-   **锯齿问题会在浮雕贴图上做改善**

```c
// 陡峭视察映射 光线步进算法  
float2 SteepParallaxMapping(float2 uv, float3 tangent_viewDir)  
{  
    float numLayers = 20;   //最大步进次数, 步进次数越多，效果越好,开销也越大。
    float layerHeight = 1 / numLayers;  //单层步进高度  
    float2 deltaUV = tangent_viewDir.xy / tangent_viewDir.z * _HeightScale / numLayers; //单次uv偏移量  
  
    float currentLayerHeight = 0;   //步进初始高度（视线深度）  
    float2 currentUV = uv;           //当前UV  
    float2 offsetUV = float2(0, 0);  //初始化偏移量  
    float currentHeightMapValue = tex2D(_HeightMap, currentUV).r;  //采样初始高度信息(采样深度)  
  
    for(int i = 0;i<numLayers;i++)  
    {  
        if(currentLayerHeight > currentHeightMapValue)  
        {  
            return offsetUV;  
        }  
        offsetUV += deltaUV;  
        currentHeightMapValue = tex2D(_HeightMap, currentUV + offsetUV).r;  
        currentLayerHeight += layerHeight;  
    }  
    return offsetUV;  
}
```

![[Pasted image 20221030144926.png]]
>numLayers不同时的采样结果（数值递增）

从上述图我们也能看出，采样次数较少的时候，结果有很明显的分层感。这也是RayMarching系特效的通病。

那么其他的RayMarching是怎么解决采样次数过少所导致的问题呢？一个屡试不爽的万精油解法就是——**Jitter（抖动）**。

当我们想使用抖动时，需要确定2个东西：抖动的噪声，抖动噪声确定，抖动最大step次数。
## 浮雕映射 Relief Mapping
![[Pasted image 20221029212425.png]]
![[Pasted image 20221029212436.png]]

-   浮雕映射一般用**光线步进**和**二分查找**来**决定uv偏移量**
-   第一步：光线步进部分，和视差贴图一样
-   后边：二分查找部分：通过光线步进找到合适的步进后，在此步进内使用二分查找来找到精确的偏移值（可能要查找多次，性能可以继续优化，由此提出改进方法POM，见下一节）

-   **为什么不直接使用二分查找？**
-   会产生比较大的误差
-   如果直接使用二分查找，在深度0和1的中间的1点，进一步为2点 -> 3点 ->Q点。但我们要的结果是P点，可以看到结果很明显是错误的
```c
// 浮雕贴图  
float2 ReliefMapping(float2 uv, float3 tangent_viewDir)  
{  
    float numLayers = 30;   //最大步进次数  
    float layerHeight = 1 / numLayers;  //单层步进高度  
    float2 deltaUV = tangent_viewDir.xy / tangent_viewDir.z * _HeightScale / numLayers; //单次uv偏移量  
  
    float currentLayerHeight = 0;   //步进初始高度（视线深度）  
    float2 currentUV = uv;           //当前UV  
    float2 offsetUV = float2(0, 0);  //初始化偏移量  
    float currentHeightMapValue = tex2D(_HeightMap, currentUV).r;  //采样初始高度信息(采样深度)  
  
    // 光线步进  
    for(int i = 0;i<numLayers;i++)  
    {  
         if(currentLayerHeight > currentHeightMapValue)  
        {  
            break;  
        }  
        offsetUV += deltaUV;  
        currentHeightMapValue = tex2D(_HeightMap, currentUV + offsetUV).r;  
        currentLayerHeight += layerHeight;  
    }  
  
    //二分查找(是否可以继续优化？)  
    float2 P2 = uv + offsetUV;  //步进停止点  
    float2 P1 = P2 - deltaUV;   //停止点的上一个点  
  
    for(int j = 0; j < 20; j++)  
    {  
        float2 P = (P1 + P2) / 2;   //中间点  
        float P_LayerHeight =  currentLayerHeight / 2;  //该点视线深度为P2点的二分之一  
        float P_HeightMapValue = tex2D(_HeightMap, P).r; //采样深度  
        if(P_LayerHeight > P_HeightMapValue)  
        {  
            P2 = P;  
        }  
        else  
        {  
            P1 = P;  
        }  
    }  
    return (P1 + P2) / 2 - uv;;  
}
```
## 视差闭塞映射（POM = Parallax Occlusion Mapping）

-   相对于浮雕贴图，不同之处在于最后一步
-   浮雕贴图是在确认最后步进之后进行二分查找（在迭代次数比较多的情况下，还是挺耗的）
-   **视差闭塞贴图是在最后步进的两端uv值进行采样（下图红色箭头），采样之后再对这两个结果进行插值，插值的结果作为P点最终的偏移值**
- ![[Pasted image 20221029213350.png]]
-  优点：

-   相对于浮雕映射，性能更好（最后只做插值，而浮雕要做二分查找）
-   相对于陡视差贴图，精确性更好

-   要求：
-   因为最后要做插值，所以要求表面是相对比较平滑/连续的，如果有莫名的凸起结果可能会出错

## 代码集合
```c
Shader "Unlit/normalmap"  
{  
    Properties  
    {  
        _MainTex ("Texture", 2D) = "white" {}  
        _NormalMap("NormalMap", 2D)= "gray" {}  
        _NormalScale("NormalScale",float) = 1  
        _HeightMap("_HeightMap", 2D) = "blue" {}  
        _HeightScale("HeightScale",float) = 1  
        _SpecularExp("SpecularExp",float) = 1  
        _SpecularScale("SpecularScale",float) = 1  
    }  
    SubShader  
    {  
        Tags { "RenderType"="Opaque" }  
        LOD 100  
  
        Pass  
        {  
            CGPROGRAM  
            #pragma vertex vert  
            #pragma fragment frag  
  
            #include "UnityCG.cginc"  
  
            struct appdata  
            {  
                float4 vertex : POSITION;  
                float4 normal : NORMAL;  
                float4 tangent : TANGENT;  
                float2 uv : TEXCOORD0;  
            };  
  
            struct v2f  
            {  
                float4 pos : SV_POSITION;  
                float2 uv : TEXCOORD0;  
                float3 normal : TEXCOORD1;  
                float3 tangent : TEXCOORD2;  
                float3 bitangent : TEXCOORD3;  
                float4 worldPosion : TEXCOORD4;  
            };  
  
            sampler2D _MainTex;  
            sampler2D _NormalMap;  
            sampler2D _HeightMap;  
              
            float _NormalScale;  
            float _HeightScale;  
            float _SpecularExp;  
            float _SpecularScale;  
  
  
            // 陡峭视察映射 光线步进函数  
            float2 SteepParallaxMapping(float2 uv, float3 tangent_viewDir)  
            {  
                float numLayers = 30;   //最大步进次数  
                float layerHeight = 1 / numLayers;  //单层步进高度  
                float2 deltaUV = tangent_viewDir.xy / tangent_viewDir.z * _HeightScale / numLayers; //单次uv偏移量  
  
                float currentLayerHeight = 0;   //步进初始高度（视线深度）  
                float2 currentUV = uv;           //当前UV  
                float2 offsetUV = float2(0, 0);  //初始化偏移量  
                float currentHeightMapValue = tex2D(_HeightMap, currentUV).r;  //采样初始高度信息(采样深度)  
  
                for(int i = 0;i<numLayers;i++)  
                {  
                    if(currentLayerHeight > currentHeightMapValue)  
                    {  
                        return offsetUV;  
                    }  
                    offsetUV += deltaUV;  
                    currentHeightMapValue = tex2D(_HeightMap, currentUV + offsetUV).r;  
                    currentLayerHeight += layerHeight;  
                }  
                return offsetUV;  
            }  
  
            // 浮雕贴图  
            float2 ReliefMapping(float2 uv, float3 tangent_viewDir)  
            {  
                float numLayers = 30;   //最大步进次数  
                float layerHeight = 1 / numLayers;  //单层步进高度  
                float2 deltaUV = tangent_viewDir.xy / tangent_viewDir.z * _HeightScale / numLayers; //单次uv偏移量  
  
                float currentLayerHeight = 0;   //步进初始高度（视线深度）  
                float2 currentUV = uv;           //当前UV  
                float2 offsetUV = float2(0, 0);  //初始化偏移量  
                float currentHeightMapValue = tex2D(_HeightMap, currentUV).r;  //采样初始高度信息(采样深度)  
  
                // 光线步进  
                for(int i = 0;i<numLayers;i++)  
                {  
                     if(currentLayerHeight > currentHeightMapValue)  
                    {  
                        break;  
                    }  
                    offsetUV += deltaUV;  
                    currentHeightMapValue = tex2D(_HeightMap, currentUV + offsetUV).r;  
                    currentLayerHeight += layerHeight;  
                }  
  
                //二分查找  
                float2 P2 = uv + offsetUV;  //步进停止点  
                float2 P1 = P2 - deltaUV;   //停止点的上一个点  
  
                for(int j = 0; j < 20; j++)  
                {  
                    float2 P = (P1 + P2) / 2;   //中间点  
                    float P_LayerHeight =  currentLayerHeight / 2;  //该点视线深度为P2点的二分之一  
                    float P_HeightMapValue = tex2D(_HeightMap, P).r; //采样深度  
                    if(P_LayerHeight > P_HeightMapValue)  
                    {  
                        P2 = P;  
                    }  
                    else  
                    {  
                        P1 = P;  
                    }  
                }  
                return (P1 + P2) / 2 - uv;;  
            }  
              
            v2f vert (appdata v)  
            {  
                v2f o;  
                o.pos = UnityObjectToClipPos(v.vertex);  
                o.uv = v.uv;  
                o.normal = UnityObjectToWorldNormal(v.normal);  
                o.tangent = UnityObjectToWorldDir(v.tangent);  
                o.bitangent = cross(o.normal, o.tangent) * v.tangent.w;  
                o.worldPosion = mul(unity_ObjectToWorld,v.vertex);   
                return o;  
            }  
  
            fixed4 frag (v2f i) : SV_Target  
            {  
                float3 world_LightDir = normalize(UnityWorldSpaceLightDir(i.worldPosion));  
                float3 world_ViewDir = normalize(UnityWorldSpaceViewDir(i.worldPosion));  
                float3 world_HalfVector = normalize(world_LightDir + world_ViewDir);  
                  
                //TBN  
                float3x3 TBN = float3x3(i.tangent,normalize(i.bitangent),i.normal);  
  
                // NormalMap  
                float3 NormalMap = UnpackNormal((tex2D(_NormalMap, i.uv)));  
                fixed4 col = tex2D(_MainTex, i.uv);  
                NormalMap *= _NormalScale;  
                float3 world_NormalMap = normalize(mul(NormalMap,TBN));  
                  
                // Parallax Mapping  
                /*                
                float height = tex2D(_HeightMap,i.uv).r;                
                float3 tangent_ViewDir = normalize(mul(TBN, world_ViewDir));                
                float2 offsetUV = tangent_ViewDir.xy / tangent_ViewDir.z * height * _HeightScale;                
                i.uv += offsetUV;                
                */                      
                          
                // Steep Parallax Mapping  
                /*                
                float3 tangent_ViewDir = normalize(mul(TBN, world_ViewDir));                
                float2 offsetUV = SteepParallaxMapping(i.uv, tangent_ViewDir);                
                i.uv += offsetUV;               
                 */           
                                      
                 //Relief Mapping  
                float3 tangent_ViewDir = normalize(mul(TBN, world_ViewDir));  
                float2 offsetUV = ReliefMapping(i.uv, tangent_ViewDir);  
                i.uv += offsetUV;  
                  
                  
                float3 Basecolor = tex2D(_MainTex, i.uv);  
                float3 Diffuse = dot(world_NormalMap,world_LightDir) * 0.5 + 0.5;  
                float3 Specular = pow(max(0, dot(world_NormalMap,world_HalfVector)), _SpecularExp) * _SpecularScale;  
                float3 FinalColor = (Diffuse + Specular) * Basecolor;  
                return float4(FinalColor,1);  
            }  
            ENDCG  
        }  
    }  
}
```
# 2.6 伽马矫正
## Gamma校正
### 颜色空间
![[Pasted image 20221030155028.png]]
-   一些颜色空间的举例，（具体参考2.1节内容）
-   图中可以看到，sRGB和Rec.709的色域是差不多的，三原色的位置是相同的，那么它们之间的区别就是：**传递函数**不同
- 
### 传递函数
-   什么是传递函数：

-   知道了颜色的颜色值之后，想要在电子设备上显示，就需要把它转换为视频信号，传递函数就是用来做转换的。

-   一个传递函数包括两部分：

-   **光转电传递函数（OETF）**，把场景线性光转到非线性视频信号值。
-   **电转光传递函数（EOTF）**，把非线性视频信号值转到显示光亮度。
-   **一个简单理解**：拍照时，将照片存储在内存卡中，就是用视频信号存储的，如果要看这个照片，就把视频信号再转换成光信号。
- ![[Pasted image 20221030155110.png]]
-  **传递函数其实就是Gamma校正所使用的函数。**
### 简单定义

![[Pasted image 20221030155454.png]]
-   **Gamma是指对线性三色值和非线性视频信号之间进行编码和解码的操作。**
### 编码和解码的理解

-   拍到的照片，存在电脑里，就是把自然界中的光信号编码为视频信号
-   查看照片时，就要把视频信号还原为线性的光信号，进行解码操作
-   如图：线性空间（相机捕捉到的真实世界光信号） + gamma编码 + 显示器显示 = 结果
-   ![[Pasted image 20221030155355.png]]

**用一张图来举例：**
 ![[Pasted image 20221030155903.png]]
**gamma编码：**
-   左图为存在硬盘中，**将捕获到的物理数据做一次gamma值约为0.45的映射**，这个过程称为gamma编码
![[Pasted image 20221030171028.png]]
-   右图中可以看到，此时图像要比实际物理像素更亮（图不一定是实际的情况，只是亮度提高了的直观表示）

**gamma校正：**
-   中间为显示图像时，需要为每一个像素做一次gamma值约为2.2的校正，来使的最终结果为正确的物理数据。
-   可以看到经过gamma校正好，之前偏亮的图像亮度降低了。

**为什么不用线性的方式存储，而要来回转换呢？**
-   ①和人眼的特性有关
-   人眼对暗部的变化感应更敏感

-   ②非线性转换为了优化存储空间和带宽
-   我们用于显示图像数据都是8bit，**为了充分利用带宽，就需要使用更多位置去存储暗部值**。也就是 暗部使用高精度保存，亮部使用相对较低精度保存。

## 韦伯定律与CRT
### 美术上的均匀和物理上的均匀
![[Pasted image 20221030160526.png]]
美术上的中灰色and物理上的中灰色
**那一个变化更均匀？**
- 正如上边所说，我们人眼对于暗部是更敏感的，视觉上人们会认为上面这条线亮度是均匀增加的，实际在物理上下面这条线亮度均匀增加。

-   上边是视觉上的均匀变化，而下边是物理量上的均匀变化。

-   补充：理论上上边的中灰是物理量上（下边）的21.8%，视觉上认为的美术中灰色，大约是物理中灰色的20%
### Gamma编码曲线
![[Pasted image 20221030160838.png]]
-   **gamma编码的曲线：**
将自然界线性增长的灰阶变化和人心理上感受到的灰阶变化做一个映射，得到Gamma编码的曲线。

-   由图中可以看到
-   自然界中亮度的0.2左右的亮度，对应的就是人眼感受到的中灰色（0.5）
-   将0.2以下看作暗部，0.2以上看作亮部，可以看到暗部的变化率更高，也就是说人眼对暗部的变化感受更敏感。

### 韦伯定律（用gamma校正的一个原因）
![[Pasted image 20221030160904.png]]
- 简单来说就是：
- 当所受刺激越大时，需要增加的刺激也要足够大才会让人感觉到明显的变化，但是只适用于中等强度的刺激。

### CRT非线性响应（用gamma校正的另外一个原因）
![[Pasted image 20221030161656.png]]
#### CRT与转换函数
-   **CRT（阴极射线显像管）**
![[Pasted image 20221030161813.png]]
在物理世界中，如果光的强度增加一倍，那么亮度也会增加一倍，这是线性关系。
而历史上最早的显示器(阴极射线管)显示图像的时候，电压增加一倍，亮度并不跟着增加一倍。即输出亮度和电压并不是成线性关系的，而是呈亮度增加量等于电压增加量的2.2次幂的非线性关系
![[Pasted image 20221030170827.png]]
2.2也叫做该显示器的**Gamma**值，**现代显示器的Gamma值也都大约是2.2**。


- 这种设备的亮度和电压不成线性关系，而是和gamma值约为2.2类似幂律的关系
-   由于CRT的这个物理特性，刚好可以把亮度压暗，也就说，左图变亮的情况下，经过右图显示器的压低亮度校正，结果刚好可以显示正常（有趣的巧合）。

- CRT设备的显示原理：
- 通过一个电子束去攻击屏幕上的凝光质图层，发射电子束的电子枪的电压和屏幕上产生的光强成非线性关系。
![[Pasted image 20221030162052.png]]
>左图（提亮）：gamma编码的曲线：人眼对于物理光强度的一个响应曲线
>右图（压暗）：crt电压与屏幕亮度关系的曲线


-   值得注意的是，上述所说的前提是，在条件相同的情况下（在明暗不同的环境下，看到的结果可能不同），我们取的**中灰值**，也不是指特定的一个值。

#### 中灰值
-   **所谓的中灰值，并非某个固定的具体数值，而是取决于视觉感受**

 一个例子可以证明：
-   对于第一张图，可以很明显看到AB颜色不同
- ![[Pasted image 20221030162541.png]]
- -   对于下面这张图，只是把AB连起来，就可以看到，其实是一种颜色  
    //ps：如果你不信的话，可以用Snipaste取一下图1的A、B两种颜色，你就会发现，真的是一个颜色
    ![[Pasted image 20221030162554.png]]
## 线性工作流
### 线性空间与Gamma空间
线性空间：相机捕捉到的真实世界光信号
Gamma空间：对线性空间颜色值编码，把场景线性光转到非线性视频信号值。
**在着色器中必须使用线性空间，最终显示要使用gamma空间（对线性空间行进依次gamma校正）**

-   对于技术美术来说，知道上边所说的还不够，因为很多时候我们会接触到一些图形效果的制作和修改。
-   这时候就需要一个**正确的工作流程**。
![[Pasted image 20221030162743.png]]
- 所谓 **线性工作流**，就是在各个环节正确的使用gamma编码/解码，来达到最终输出的数据和最初输入的物理数据一致的目的。

-   如果**使用Gamma空间的贴图，在传给着色器之前需要从Gamma空间转到线性空间**。

-   目的是**在着色器中做一些渲染计算时会使用线性空间的颜色值**，这样就不会出现一些显示错误的结果。

-   **如果不在线性空间下进行渲染工作，可能会产生的问题：**

-   **①亮度叠加时**
前文提到，图像存储在硬盘中，**会将捕获到的物理数据做一次gamma值约为0.45的映射**，这个过程称为gamma编码。
线性空间下，原本四个亮度为0.098的值，进行叠加后应该是0.196。结果由于在Gamma空间下，他们都是0.098<sup>0.45</sup>，也就是四个0.35叠加，叠加后为1.4，亮度>1，产生**过曝**现象。 

-   可以看到非线性空间下亮度叠加出现了过曝（亮度>1的）的情况
-   **因为Gamma空间经过gamma编码后的亮度值相对之前会变大**。
![[Pasted image 20221030163030.png]]

- **②颜色混合时**
-   如果在混合前没有非线性的颜色进行转换，就会在纯色的边界出现一些黑边。
![[Pasted image 20221030163044.png]]

  ③**光照计算时**
-    在光照渲染结算时，如果我们把非线性空间下（视觉上的）的中灰色0.5当做实际物理光强为0.5来计算时，就会出现左边这种情况
-   在显示空间下是0.5，但在渲染空间下它的实际物理光强为0.18（如右图）
![[Pasted image 20221030163101.png]]

### 补充
**①渲染时，灯光容易过曝，室内容易偏黑都是因为没有使用线性工作流**

-   原因如下
-   **渲染器是物理渲染器**，但是输入的贴图是美术数据（存储在硬盘中，是gamma空间），所以是偏亮的
-   在渲染时，加入的灯光是正常的，贴图是偏亮的
-   之后经过屏幕的sRGB标准下压曲线校正后，偏亮的贴图正常了，而本来正常的灯光就偏暗了。（灯光容易过曝）
-   VRay中有一个按钮可以抵消显示器压暗的效果，但是这样一来，灯光正常，贴图又过亮了。
-   这就是为什么不用线性工作流时，室内昏暗、灯光容易过曝
-   具体过程图下图
![[Pasted image 20221030164036.png]]

 ![[Pasted image 20221030164051.png]]
**②正确的线性工作流内容**

-   在贴图输入给渲染器时，用下压的曲线还原回物理数据（De-Gamma）
-   这样输入渲染器中的贴图和光照，就都是正常的了
-   线性的输入经过线性的运算后，结果也是线性的。
-   线性的数据会经过屏幕校正压暗，此时点击VRay的抵消显示器压暗的按钮，就可以在屏幕上看到正常的结果了
 ![[Pasted image 20221030164133.png]]
![[Pasted image 20221030164147.png]]
### Unity中的颜色空间
#### 在Unity中选择颜色空间
-   点击菜单 -> Project Settings -> Player页签 -> Other Settings 下的Rendering部分，通过修改Color Space可以来选择Gamma/Linear（线性）
-![[Pasted image 20221030164250.png]]
-   当选择Gamma Space时
-   Unity不会做任何操作（默认Gamma）

-   **当选择Linear Space时
-   **引擎的渲染流程在线性空间计算**，理想情况下项目使用线性空间的贴图颜色，不需要勾选sRGB；
-   如果勾选了sRGB的贴图，Unity会通过硬件特性采样时进行线性转换。
#### 硬件支持
-   目前以下平台可以进行线性空间下的硬件支持
-   Windows，Mac OSx ，Linux（Standalone）
-   Xbox One
-   PS4
-   Android（OpenGL ES3.0）
-   IOS（Metal）
-   WebGL

 **Unity主要通过以下两个硬件特性来支持**
-   **sRGB Frame Buffer**
	-   将Shader的计算结果输出到显示器前做Gamma校正
	-   作为纹理被读取时会自动把存储的颜色从sRBG空间转换到线性空间
	-   调用ReadPixels（）、ReadBackImage（）时，会直接返回sRGB空间下的颜色
	-   sRBG Frame Buffer 只支持每通道为8bit的格式，不支持float浮点格式
	-   HDR开启后会先把渲染结果会知道浮点格式的FB中，最后绘制到sRGB FB上输出。

-   **sRGB Sampler**
	-   将sRBG的贴图进行线性采样的转换
	-   使用硬件特性完成sRGB贴图的线性采样和shader计算结果的gamma校正，比在shader里对贴图采样和计算结果的校正要快。
### 资源导出问题/注意事项
#### SubstancePainter
![[Pasted image 20221030164600.png]]
-   SubstancePainter的贴图导出时，其线性的颜色值经过了gamma编码，所以颜色被提亮了。
-   此时这个贴图需要在Unity中勾选sRBG选项（让unity知道这个贴图在gammar空间），来让它被采样时能还原回线性值。
#### PS
![[Pasted image 20221030164711.png]]
-   如果使用线性空间，一般来说PS可以什么都不改，导出的帖图只要勾上sRGB就可以了。
- 如果调整PS的伽马值为1，导出的贴图在Unity中也不需要勾选sRGB了。
![[Pasted image 20221030164917.png]]
#### 半透明效果
-   **Unity中**：
-   Unity进行半透明混合时，会先将它们转换到一个线性空间下然后再混合

-   **PS中**：
-   PS的图层和图层之间做混合时，每个上层的图层都会读取他们的Color Profile（gamma值），然后经过一个gamma变换再做混合，这样做得结果就会偏暗一些。
-   （可以在它的工作空间 的设置中进行更改，选择用灰度系数混合RGB颜色，参数设置为一，这样图层才是一个最终直接混合的结果）
![[Pasted image 20221030165103.png]]

## 知乎总结
[Gamma、Linear、sRGB 和Unity Color Space，你真懂了吗？](https://zhuanlan.zhihu.com/p/66558476)
本文将会简单介绍Gamma、Linear、sRGB和伽马校正的概念。接着通过实例解析统一到线性空间的步骤，最后介绍如何在Unity中实施相应的工作流。
### 什么是Linear、Gamma、sRGB和伽马校正？

在物理世界中，如果光的强度增加一倍，那么亮度也会增加一倍，这是**线性关系**。

而历史上最早的显示器(阴极射线管)显示图像的时候，电压增加一倍，亮度并不跟着增加一倍。即输出亮度和电压并不是成线性关系的，而是呈亮度增加量等于电压增加量的2.2次幂的非线性关系：

![[Pasted image 20221030172034.png]]

2.2也叫做该显示器的**Gamma**值，**现代显示器的Gamma值也都大约是2.2**。

> [!NOTE] 
>现在的液晶显示器依然保留了2.2方的解码gamma校正。但这并不是什么历史遗留问题，也不是因为CRT的物理特性，而是现代数据编码上实实在在的需求——对物理线性的颜色编码做0.45次方的gamma校正，目的是为了让颜色编码的亮度分级与人眼主观亮度感受线性对应。这样，在相同的数据位数下，图像数据可以保留更多人眼敏感的信息。以8位色为例：由于人眼对暗色调更加敏感，那就对物理线性的颜色做0.45次方的处理，也就是编码gamma。校正完成后，相当于使用了0~128的范围来表达原来与物理强度保持线性时0~55的亮度变化。因此，显示器做解码gamma的目的是为了让便于保存和传输的颜色编码变回物理线性的形式，以便人眼观察显示器时能得到与观察现实世界时相近的感受。

这种关系意味着当电压线性变化时，相对于真实世界来说，亮度的变化在暗处变换较慢，暗占据的数据范围更广，颜色整体会偏暗。

如图，直线代表物理世界的**线性空间（Linear Space）**，下曲线是显示器输出的**Gamma2.2空间（Gamma Space）**。
![[Pasted image 20221030173023.png]]

好了，正常情况下，人眼看物理世界感知到了正常的亮度。而如果显示器输出一个颜色后再被你看到，即相当于走了一次Gamma2.2曲线的调整，这下子颜色就变暗了。如果**我们在显示器输出之前，做一个操作把显示器的Gamma2.2影响平衡掉**，那就和人眼直接观察物理世界一样了！这个平衡的操作就叫做**伽马校正。**

在数学上，伽马校正是一个约0.45的幂运算（和上面的2.2次幂互为逆运算）：
![[Pasted image 20221030173109.png]]
![[Pasted image 20221030173124.png]]
>左(Gamma0.45) 中(Gamma2.2) 右(线性物理空间)

经过0.45幂运算，再由显示器经过2.2次幂输出，最后的颜色就和实际物理空间的一致了。

**最后，什么是sRGB呢？** 1996年，微软和惠普一起开发了一种标准**sRGB**色彩空间。这种标准得到许多业界厂商的支持。**sRGB对应的是Gamma0.45所在的空间**。

**为什么sRGB在Gamma0.45空间？**

假设你用数码相机拍一张照片，你看了看照相机屏幕上显示的结果和物理世界是一样的。可是照相机要怎么保存这张图片，使得它在所有显示器上都一样呢？ 可别忘了所有显示器都带Gamma2.2。反推一下，那照片只能保存在Gamma0.45空间，经过显示器的Gamma2.2调整后，才和你现在看到的一样。换句话说，**sRGB格式相当于对物理空间的颜色做了一次伽马校正**。

还有另外一种解释，和人眼对暗的感知更加敏感的事实有关。
![[Pasted image 20221030173401.png]]
如图，在真实世界中（下方），如果光的强度从0.0逐步增加到1.0，那么亮度应该是线性增加的。但是对于人眼来说（上方），感知到的亮度变化却不是线性的，而是在暗的地方有更多的细节。换句话说，**我们应该用更大的数据范围来存暗色，用较小的数据范围来存亮色。** 这就是sRGB格式做的，定义在Gamma0.45空间。而且还有一个好处就是，**由于显示器自带Gamma2.2，所以我们不需要额外操作显示器就能显示回正确的颜色**。

以上内容，看完后还是不懂也没关系，在继续之前你可以先死记住以下几个知识点：

-   **显示器的输出在Gamma2.2空间。**
-   **伽马校正会将颜色转换到Gamma0.45空间。**
-   **伽马校正和显示器输出平衡之后，结果就是Gamma1.0的线性空间。**
-   **sRGB对应Gamma0.45空间。**

### 统一到线性空间

现在假设你对上文的概念有一定认识了，我们来讲重点吧。

在Gamma 或 Linear空间的渲染结果是不同的，从表现上说，在Gamma Space中渲染会偏暗，在Linear Space中渲染会更接近物理世界，更真实：

![[Pasted image 20221030173512.png]]
>左（Gamma Space），右（Linear Space）

**为什么Linear Space更真实？**

你可以这么想，物理世界中的颜色和光照规律都是在线性空间描述的对吧？（光强度增加了一倍，亮度也增加一倍）。 而计算机图形学是物理世界视觉的数学模型，Shader中颜色插值、光照的计算自然也是在线性空间描述的。如果你用一个非线性空间的输入，又在线性空间中计算，那结果就会有一点“不自然”。

换句话说，**如果所有的输入，计算，输出，都能统一在线性空间中，那么结果是最真实的**，玩家会说这个游戏画质很强很真实。事实上因为计算这一步已经是在线性空间描述的了，所以只要保证输入输出是在线性空间就行了。

所以为什么你的游戏画面不真实呢？因为你可能对此混乱了，你的输入或输出在Gamma Space，又没搞清楚每个纹理应该在什么Space，甚至也不知道有没用伽马校正，渲染结果怎么会真实呢？

**现在假设我们的目标是获得最真实的渲染，因此需要统一渲染过程在线性空间，怎么做呢？**

**注**：统一在Linear空间是最真实的，但不代表不统一就是错的。一般来说，如果是画质要求高的作品（如3A）等，那么都是统一的。没这方面要求的则未必是统一的，还有一些项目追求非真实的渲染，它们也未必需要统一。

统一到线性空间的过程是看起来是这样的，用图中橙色的框表示（现在看不懂图没关系，跟着后面的步骤来一步步看）：
![[Pasted image 20221030173707.png]]
我们从橙色框的左上角出发。

第一步，输入的纹理如果是sRGB（Gamma0.45），那我们要进行一个操作转换到线性空间。这个操作叫做**Remove Gamma Correction**，在数学上是一个2.2的幂运算 c→c2.2 。如果输入不是sRGB，而是已经在线性空间的纹理了呢？那就可以跳过Remove Gamma Correction了。

**注**：美术输出资源时都是在sRGB空间的，但Normal Map等其他电脑计算出来的纹理则一般在线性空间，即Linear Texture。详见后文！

第二步，现在输入已经在线性空间了，那么进行Shader中光照、插值等计算后就是比较真实的结果了（上文解释了哦~），如果不对sRGB进行Remove Gamma Correction直接就进入Shader计算，那算出来的就会不自然，就像前面那两张球的光照结果一样。

第三步，Shader计算完成后，需要进行**Gamma Correction**，从线性空间变换到Gamma0.45空间，在数学上是一个约为0.45的幂运算 c→c12.2 。如果不进行Gamma Correction输出会怎么样？那显示器就会将颜色从线性空间转换到Gamma2.2空间，接着再被你看到，结果会更暗。

第四步，经过了前面的Gamma Correction，显示器输出在了线性空间，这就和人眼看物理世界的过程是一样的了！

  

我们再举个例子，我们取sRGB纹理里面的一个像素，假设其值为0.73。那么在统一线性空间的过程中，它的值是怎么变化的？
![[Pasted image 20221030184116.png|300]]
第一步，0.73(上曲线) * [Remove Gamma Correction] = 0.5(直线)。（ 0.73<sup>2.2</sup>=0.5 ）

第二步，0.5(直线) * [Shader] = 0.5(直线)（假设我们的Shader啥也不干保持颜色不变）

第三步，0.5(直线) * [Gamma Correction] = 0.73(上曲线)。（ 0.5<sup>1/2.2</sup>=0.73 ）

第四步，0.73(上曲线) * [显示器] = 0.5(直线)。（ 0.73<sup>2.2</sup>=0.5 ）

如果不进行Gamma Correction，就会变暗，因为第三步不存在了，第四步就会变成：

0.5(直线) * [显示器] = 0.218(下曲线)。（ 0.5<sup>2.2</sup>=0.218 ）

再对照上面的图琢磨琢磨？

### Unity中的Color Space

我们回到Unity，Editor>Project Setting>Player中的“Color Space”属性可以选择Gamma 或 Linear作为Color Space：
![[Pasted image 20221030184401.png]]
**这两者有什么区别呢？**

**如果选择了Gamma，那Unity不会对输入和输出做任何处理**，换句话说，Remove Gamma Correction 、Gamma Correction都不会发生，除非你自己手动实现。

**如果选了Linear，那么就是上文提到的统一线性空间的流程了**。对于sRGB纹理，Unity在进行纹理采样之前会自动进行Remove Gamma Correction，对于Linear纹理则没有这一步。而在输出前，Unity会自动进行Gamma Correction再让显示器输出。

怎么告诉Unity纹理是sRGB还是Linear呢？**对于特定用途的纹理，你可以直接设置他们所属的类型：如Normal Map、Light Map等都是Linear，设置好类型Unity自己会处理他们。**
![[Pasted image 20221030184638.png]]
**还有一些纹理不是上面的任何类型，但又已经在线性空间了（比如说Mask纹理、噪声图），那你需要取消sRGB这个选项让它跳过Remove Gamma Correction过程**：
### UE中的Color Space
[Unreal中关于颜色的技术分享_百科TA说 (baidu.com)](https://baike.baidu.com/tashuo/browse/content?id=9167c87c2cd4f1c2f4d1c173&lemmaId=2147136&fromLemmaModule=pcRight)
用过unreal的小伙伴应该都会注意到，我们在unreal里面进行贴图设置的时候，对于basecolor都需要勾选上sRGB。为什么需要勾选？每张贴图都需要勾选么？如果不做勾选会怎么样？这就需要用我们的gamma校正和线性空间来破案了。接下来就一起来看这篇Unreal中关于颜色的技术分享。
![[Pasted image 20221030185833.png]]

Gamma校正

首先什么是gamma校正。官方解释，RGB值与功率并非简单的线性关系，而是幂函数关系，这个函数的指数称作Gamma值，一般为2.2（power2.2），而这个换算过程，称为Gamma校正。官方来源，开发gamma编码是用来抵消阴极射线管（CRT）显示器的输入和输出特性，电子枪的电流，也是光的亮度，与输入的正极电压的变化是非线性的。通过gamma压缩来改变输入信号抵消了这个非线性，因此输出图像就能有预期的亮度。

画图来理解就是如下，

如果我们有一张线性的照片，如果我们显示器也是线性的，那经过显示器输出的图像就应该和真实的图像是一样的；
![[Pasted image 20221030185921.png]]
但是实际上我们的显示器根本不按套路来，它的gamma值是2.2，所以如果我们的图片是线性的，那么从gamma为2.2的显示器中输出出来就是下面这个样子
![[Pasted image 20221030185928.png]]
可以看到结果有明显的色彩失真，所以如果我们把照片的gamma值设置成1/2.2的话，经过两次调整，结果就是正确的啦

![[Pasted image 20221030185956.png]]
在进行gamma校正的方式就是采样进行输入的时候，Gamma=1/2.2，调亮Gamma；

在显示输出的时候Gamma=2.2，调暗Gamma。
![[Pasted image 20221030190026.png]]
**线性空间**

一般在图片的渲染中存在两个颜色空间，第一个是Gamma（非线性）的颜色空间；然后是Linear（线性）颜色空间。Gamma使用的是进行了校正的颜色表；而linear使用的是一个线性的完整颜色表，而且渲染中用到的光线也是线性空间的，所以我们在进行计算的时候要在线性空间中进行，输入和输出需要进行gamma校正。

最好的办法就是在图片输入的时候采用sRGB格式，目的是为了告诉linear color space，需要对输入的颜色进行power2.2校正切换到线性空间，然后再进行shader计算，计算完毕以后再通过power1/2.2切换回gamma空间。所以解决了我们刚开始提的在unreal中的**basecolor需要勾选sRGB选项**。而**非sRGB纹理则会直接在shader中进行计算，比如normal和mask。**

所以以上就解释了我们在导入贴图的时候需要注意到的问题，只有勾选了引擎才会进行正确的像素计算；一般basecolor才需要勾选；对于basecolor来说，不勾选GPU就不会进行gamma校正，而直接使用存储的值进行渲染，但同时也不会得到真实的效果。

### 到底什么纹理应该是sRGB，什么是Linear？

关于这一点，我个人有一个理解：**所有需要人眼参与被创作出来的纹理，都应是sRGB（如美术画出来的图）。所有通过计算机计算出来的纹理（如噪声，Mask，LightMap）都应是Linear**。

这很好解释，人眼看东西才需要考虑显示特性和校正的问题。而对计算机来说不需要，在计算机看来只是普通数据，自然直接选择Linear是最好的。

**除了纹理外，在Linear Space下，Shaderlab中的颜色输入也会被认为是sRGB颜色，会自动进行Gamma Correction Removed。**

有时候你可能需要想让一个Float变量也进行Gamma Correction Removed，那么就需要在ShaderLab中使用[Gamma]前缀：

```c
[Gamma]_Metallic("Metallic",Range(0,1))=0
```

如上面的代码，来自官方的Standard Shader源代码，其中的_Metallic这一项就带了[Gamma]前缀，表示在Lienar Space下Unity要将其认为在sRGB空间，进行Gamma Correction Removed。

**扩展：为什么官方源代码中_Metallic项需要加[Gamma]？** 
这和底层的光照计算中考虑能量守恒的部分有关，Metallic代表了物体的“金属度”，如果值越大则反射(高光)越强，漫反射会越弱。在实际的计算中，这个强弱的计算和Color Space有关，所以需要加上[Gamma]项。

虽然Linear是最真实的，但是Gamma毕竟少了中间处理，渲染开销会更低，效率会更高。上文也说过不真实不代表是错的，**毕竟图形学第一定律：如果它看上去是对的，那么它就是对的**。

**注**：在Android上，Linear只在OpenGL ES 3.0和Android 4.3以上支持，iOS则只有Metal才支持。

在早期移动端上不支持Linear Space流程，所以需要考虑更多。不过随着现在手机游戏的发展，越来越多追求真实的项目出现，很多项目都选择直接在Linear Space下工作。

一旦确定好Color Space，那么就需要渲染工程师、技术美术和美术商量和统一好工作流了。在中小团队或项目中，这些概念很容易被忽略，导致工作流混乱，渲染效果不尽人意。现在你懂了吗？

# 2.7 LDR和HDR
## 基本概念
-   **Dynamic Range**（动态范围）=最高亮度/最低亮度
-   **HDR**= High Dynamic Range
-   **LDR** = Low Dynamic Range（我们日常看到的）
-   **ToneMapping**：将超高的动态范围（HDR）转换到我们日常显示的屏幕上的低动态范围（LDR）的过程
![[Pasted image 20221030192152.png]]
-   一些小芝士：
-       因为不同的厂家生产的屏幕亮度（物理）实际上是不统一的，那么我们在说**LDR时，它是一个0到1范围的值**，对应到不同的屏幕上就是匹配当前屏幕的最低亮度（0）和最高亮度（1）
-       自然界中的亮度差异是非常大的。例如，蜡烛的光强度大约为15，而太阳光的强度大约为10w。这中间的差异是非常大的，有着超级高的动态范围。将太阳光显示在屏幕上就是ToneMapping。
-       我们日常使用的屏幕，其最高亮度是经过一系列经验积累的，所以使用、用起来不会对眼睛有伤害；但自然界中的，比如我们直视太阳时，实际上是会对眼睛产生伤害的。
- 
- **相机是如何将HDR映射到LDR的**
-   首先将曝光值进行计算，映射到相机可以感应的范围
-   受光圈、快门、传感器的灵敏度等影响
-   然后把这个值输入为线性的值，存储到图片中（一般为raw格式）
-   之后会经过一个变化（LUT），通过白平衡、色彩校正、色调映射、伽马校正这个过程，最后的结果烘焙成LUT（pbr中LUT的图，就是这个过程的结果）
-   每个相机厂商的LUT格式不太一样。
### 为什么需要HDR
![[Pasted image 20221030192615.png]]
-   LDR只能将现实中的颜色压缩再呈现出来
-   HDR可以由更好的色彩，更高的动态范围和更丰富的细节。
-   可以有效防止画面过曝，超过1的亮度值的色彩也能很好地表现，像素光亮度变得很正常，视觉传达更真实
-   HDR才有超过1的数值，才会有光晕（bloom）效果，高质量的bloom效果能体现出画面的渲染品质
![[Pasted image 20221030192649.png]]
### 一些可以直接下载HDR图的网址
-   [http://www.hdrlabs.com/sibl/archive.html](http://www.hdrlabs.com/sibl/archive.html)
-   [https://www.openfootage.net/hdri-panorama/](https://www.openfootage.net/hdri-panorama/)
## Unity中的HDR
### 1. Camera中的HDR设置
![[Pasted image 20221030192735.png]]
-   开启的话，会将场景渲染为HDR图像缓冲区
-   下一步进行屏幕后处理：Bloom和ToneMapping
-   在ToneMapping过程中，会把HDR转换为LDR
-   LDR的图像会发送给显示器
###  2. Lightmap的HDR设置
-   选择High Quality将启用HDR光照贴图的支持，选择Normal Quality将切换为使用RGBM编码
-   RGBM编码：将颜色存储在RGB通道中，将乘数（M）存储在Alpha通道中
![[Pasted image 20221030192844.png]]
### 3. 拾色器的HDR设置
![[Pasted image 20221030192922.png]]
![[Pasted image 20221030192919.png]]
-   如果将Property的颜色参数的前边加上[HDR]就会将其标识为HDR
-   颜色设置为HDR，那么拾色器中就会出现一个Intensity的滑条用来调整强度
-   **滑条每增加1，提供的光强度增加一倍。**
### 4. HDR的优点、缺点

-   优点

-   画面中亮度超过1的部分不会被截掉，增加了亮部的细节，减少了曝光
-   减少画面暗部的色阶感
-   更好的支持bloom效果

-   缺点

-   渲染速度慢，需要更多显存
-   不支持硬件抗锯齿
-   部分低端手机不支持

## HDR与Bloom
[[第四章 高级扩展#①Bloom实现原理]]
![[Pasted image 20221030193025.png]]
-   Bloom用来表现光晕的效果

-   **Bloom实现过程**
-   渲染出原图
-   计算超过某个阈值的高光像素
-   对高光的像素进行高斯模糊
-   然后叠加光晕、成图
- 
-   **简述Unity中的Bloom过程**
- ![[Pasted image 20221030193120.png]]
-   **后边的课程会详细介绍**
-   在第一步down sample处计算高光的像素，然后不停的做down sample并存在rt里，到达一定次数后（由参数控制），再一步步up scale回去，在这个过程中会将之前的rt加入，一步步up sample回到原来。
-   //这里说了降采样和上采样等概念，后续bloom课程中有涉及，这里直接贴点参考链接：
- -   [https://catlikecoding.com/unity/tutorials/advanced-rendering/bloom/](https://catlikecoding.com/unity/tutorials/advanced-rendering/bloom/)
-   [https://zhuanlan.zhihu.com/p/339443207](https://zhuanlan.zhihu.com/p/339443207)
## HDR与ToneMapping
### 1. ToneMapping概念

-   前边的回顾：LDR范围为0到1，HDR可以超过1,；
-   **ToneMapping的概念：**

-   **想要在显示器上表现更高动态范围的颜色，就要把HDR压缩为LDR（这个过程就是ToneMapping），这种映射关系就是色调映射。**

-   下边例子是一个<font color="#ff0000">线性的亮度映射</font>，但这并不符合我们对真实世界的理解，因此，基本上所有的映射最后都是通过一个**s曲线**来实现。
![[Pasted image 20221030193256.png]]
### 2. ACES曲线
![[Pasted image 20221102204612.png]]
-   Academy Color Encording System学院颜色编码系统
-   是最流行、最被广泛使用的ToneMapping映射曲线
-   效果：对比度提高，压暗暗部使暗部变化更不明显。能很好的保留暗部和亮部的细节。之后在这个基础上再进行调色
- ![[Pasted image 20221030193438.png]]
### 3.其他类型的ToneMapping算法
[Tone mapping进化论 ](https://zhuanlan.zhihu.com/p/21983679)

![[Pasted image 20221030193542.png]]

### 4.LUT（Lookup Table）
![[Pasted image 20221030193623.png]]
-   **简单的理解**：就是滤镜，通过LUT，你可以将一组RGB值输出为另一组RGB值，从而改变画面的曝光与色彩
-   和ToneMapping不同，**LUT是在LDR之间做变化**。 而ToneMapping是对HDR做变换的。
-   调整RGB三通道的LUT被称为3D LUT
	-   格式有如下几种
	- ![[Pasted image 20221030193651.png]]
 -   小trick：可以在PS中调整LUT，导出的LUT作为滤镜调整画面
 ![[Pasted image 20221030193716.png]]
 UE4的后处理滤镜部分中也有LUT相应的位置
 ![[Pasted image 20221030193748.png]]
# 2.8 FlowMap
[# 【技术美术百人计划】图形 2.8 flowmap的实现——流动效果实现](https://www.bilibili.com/video/BV1Zq4y157c9?p=2&vd_source=9d1c0e05a6ea12167d6e82752c7bc22a)
[[TA101#FlowMap]]
## 什么是FlowMap
### 1. FlowMap
![[Pasted image 20221030195248.png]]
-   是Valve在2010年GDC中介绍的一种在求生之路2中用来实现水面流动效果的技术
-   容易实现，且运算量较小（所以到现在还在被使用）
-   使用了一张被称为flowmap的贴图（右图），来达到控制场景中水面流向的效果
### 2. FlowMap的实质
![[Pasted image 20221030195419.png]]
>发现左图和中图颜色不对应？下一小节会讲原因
- 是一张记录了2D向量信息的纹理
-   **理解**：
-   假设有一个2D平面，平面上每个点都对应一个向量，这个向量指向这个点接下来要运动的方向。
-   我们通过颜色的色值（**RG两个通道**）来记录这些向量的信息，就得到了一张flowmap
-   在shader中干扰uv（偏移uv），对纹理进行采样，这样就得到了一个模拟流动的效果

### 3. 回顾：UV映射（纹理映射）

-   对一个贴图进行纹理查找，就要用到uv坐标
-   **理解**：
-   如图为Unity中的uv坐标，类似于xy轴，用此uv坐标查找右边贴图的颜色值，采样会得到和贴图一模一样的结果
![[Pasted image 20221030195659.png]]
-   如图，如果改变查找时的uv坐标，让每一列都有相同的uv值，那么采样结果就是右图的条纹状的结果
![[Pasted image 20221030195810.png]]
如图，如果用同一个uv值采样的话，结果就会是同一的颜色, 下图采样（0，0）即左下角颜色
![[Pasted image 20221030195841.png]]
-   **也就是说，uv贴图上颜色相同的地方：意味着采样纹理时使用了同一位置**
### 4. Flowmap的原理

-   通过所带有的向量场信息对uv进行了一个偏移，来干扰我们采样时候的这个过程。

-   如图可以看到，经过flowmap发生偏移后，让原本正常的采样变成了一个扭曲的效果
-   **注意**：
-   **不同软件的不同uv坐标：UE4与Unity相比，反转了绿通道**，所以使用的flowmap也会发生变化。（根据不同的引擎需求进行调整）
![[Pasted image 20221030195932.png]]
### 5. 为什么要使用flowmap
![[Pasted image 20221030200104.png]]
## FlowMap shader
### 1. 基本流程
-   1.采样flowmap获取向量场信息
-   2.用向量场信息，使采样贴图时的uv随时间变化
-   3.对同一贴图以半个周期的相位差采集两次，并线性插值，使贴图**流动连续**

### 2. 实现思路

**目标：根据flowmap上的值，使纹理随时间偏移**

-   **操作1**：最简单的uv随时间偏移的方法： **uv - time**
- 
-   **关于为什么是相减**：
-   先理解一下相加的情况：模型上的某个点（u，v）+（time，0）
-   可以理解为随着time增加，采样到的像素越远
-   这个效果在视觉上可以形容为：更远距离的像素偏移向这个点，也就是说和我们直观认识到的运算法则是**相反**的。
-   用uv值作为向量时，是遵守运算法则的
-   uv偏移并没有改变顶点位置，只是采样到了更远的像素

-   **操作2**：从flowmap获取流动方向
- 
-   从flowmap获取流动的方向，再乘time，就可以达到让某个点根据flowmap流动的目的
-   问题：flowmap不能直接使用
-   解决：将flowmap上的色值从[0，1]映射到方向向量的[-1，1]

-   也就是一个乘2减1的操作，之前推导mvp矩阵时也用过这个方法

-   **操作3**：流动无缝循环，把偏移控制在一定的范围内（随着时间进行，变形太过夸张）
![[Pasted image 20221030201326.png]]
-   构造两个相位相差半个周期的波形函数
![[Pasted image 20221030201416.png]]
-   用相位差半个周期的两层采样进行加权混合，用纹理流动另一层采样来覆盖一个周期重新开始时的不自然情况

**代码**
![[Pasted image 20221030202010.png]]
![[Pasted image 20221030202017.png]]
![[Pasted image 20221030202025.png]]

### 3. 用flowmap修改法线贴图
![[Pasted image 20221030201822.png]]
## Flowmap的制作

### 1.  Flowmap Painter

-   **Flowmap Painter**

-   下载地址：[http://teckartist.com/?page_id=107](http://teckartist.com/?page_id=107)
-   直接上手绘制即可，很简单
-   可以使用反转UV通道选项，以便不同符合不同的引擎需求
-   **注意**：使用Flowmap Painter绘制得到的贴图为**线性空间下的颜色**，不需要伽马校正。（Unity里不用勾选sRGB）
UE引擎烘焙时要勾选File Red选项。

### 2. Houdini（待学习）

待学习，课程介绍了详细流程！

### 3. 注意事项

-   flowmap贴图的设置：
-   要使用无压缩或高质量
-   确认色彩空间

# 2.9 GPU硬件架构概述（未学习）
[深入GPU硬件架构及运行机制 - 0向往0 - 博客园 (cnblogs.com)](https://www.cnblogs.com/timlly/p/11471507.html#32-gpu%E5%BE%AE%E8%A7%82%E7%89%A9%E7%90%86%E7%BB%93%E6%9E%84)
