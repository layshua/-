
---
title: 03 深度缓冲
aliases: []
tags: []
create_time: 2023-07-08 14:23
uid: 202307081423
banner: "![[Pasted image 20230708142421.png]]"
---


> [!note]  约定
> 本文采用 OpenGL 标准进行推导
> 1. 列向量
> 2. 模型空间、世界空间、观察空间是右手坐标系，而裁剪空间与 NDC 是左手坐标系
> 3. Camera 的 LookAt （Forward）方向为 $- z$ 轴方向
> 4. **NDC 空间范围** $[-1,1]^3$ 
> 5.  OpenGL 使用**距离值**表示 $n、f$。$n$ 被映射到 $-1$，$f$ 被映射到 $1$


> [!note] 深度纹理
> Depth Texture = 深度纹理 = 深度图
> 上面保存了深度缓冲区的值，是非线性深度，使用时要先转换成线性深度

深度是指像素到摄像机的距离，观察空间的深度为**线性深度**，NDC 空间的深度为**非线性深度**。
引擎通常帮我们收集 NDC 空间空间的深度信息保存在一张 Depth Texture 中，其中寸的也是**非线性深度**。
当我们想要精确表达物体的深度差异或者重建像素世界坐标位置，就需要使用将非线性深度转化为线性深度。
# 理论与推导
## 非线性深度

假如在 MV 变换后，**观察空间（View Space）** 下的某个点对应的齐次坐标为 $(x,y,z,1)$，那么经过透视投影变换和 GPU 裁剪后转换到**齐次裁剪空间（Clip Space）**，变换过程如下：（该变换同样适用于 Unity，Unity 与 OpenGL 投影矩阵相同）

![[02 视图变换#^bgahra]]

我们只关注深度，即 $\displaystyle z'=-(\frac{f+n}{f-n})z-\frac{2fn}{f-n}$

然后进行**齐次除法**转换到**NDC 空间**
$$ z''=\frac{z'}{w'}=(\frac{f+n}{f-n})+\frac{2fn}{(f-n)z}\tag{1}$$
$z''$ 值范围为 $[-1,1]$ ，而 ZBuffer 中存储的值应该为 $[0,1]$，所以我们将 NDC 空间的 $Z$ 值范围转换到 $[0,1]$：（$NonLinearDepth$ 与 $\displaystyle \frac{1}{z}$ 相关，是非线性的，即**非线性深度**）
$$NonLinearDepth = z''\times0.5+0.5=\frac f{f-n}+\frac{2fn}{(f-n)\color\red{z}}$$
带入 $z=n, z=f$ 可得近平面 $NonLinearDepth$ 为 $1$，远平面 $NonLinearDepth$ 为 $0$

![[Pasted image 20230708123806.png]]
>范围转换前后对比，横轴为 z 值。可以看出靠近摄像机的十个单位占了 90%的深度缓冲区精度，故离摄像机越远的值精度越低

## 线性深度
![[Pasted image 20230708110316.png]]
>观察空间的深度为**线性深度**


![[Pasted image 20230708152129.png]]
>线性深度受 far 的影响

**线性深度分为两种：**
1. $LinearEyeDepth$：观察空间下的线性深度值，取值范围$[n, f]$
2. $Linear01Depth$：把线性深度归一化到$[0,1]$，我们通常会使用这个线性深度
$$
\begin{aligned}&LinearEyeDepth=-Pview.z\\\\&Linear01Depth=\frac{-Pview.z-n}{f-n}or\frac{-Pview.z}{f}\end{aligned}
$$

由方程（1）（2）可得：
$$ \begin{cases}z''=(\frac{f+n}{f-n})+\frac{2fn}{(f-n)z}  \\
z'' = NonLinearDepth\times2-1\end{cases}$$
联立可以求出 
$$z=\frac1{(\frac{f-n}{fn}*NonlinearDepth-\frac1n)}$$
由于世界空间以 $-Z$ 为正反向，所以求深度需要取反得到正数：
$$LinearEyeDepth=\frac{1}{(\frac{n-f}{fn}*NonlinearDepth+\frac1n)}$$

然后将 $LinearEyeDepth$ 除以 $f$ 即可得到归一化的线性深度 $Linear01Depth$
$$
\begin{aligned}Linear01Depth&=(\frac{1}{(\frac{n-f}{fn}*NonlinearDepth+\frac{1}{n})}\text{-n})/(f\text{-n})\\\\or&=\frac{1}{(\frac{n-f}{n}*NonlinearDepth+\frac{f}{n})}\end{aligned}
$$ 

![[Pasted image 20230708152153.png]]
>曲线对比图

## 深度纹理重建像素的世界空间坐标
### 使用 VP 逆矩阵重建
设 NDC 空间上的点 $P_{ndc}$ 映射到屏幕空间上为点 $P(x, y)$
设 ${P.x=u}*{Width}, {P.y=v}*{Height}$

首先将屏幕空间坐标转换到 NDC 空间，从 NDC 空间到屏幕空间，点 P 相对于左下角坐标的比例是不变的，可以列出等式：

![[Pasted image 20230708152930.png]]

由前文可知,  $P_{ndc}.z=2*NonlinearDepth-1$
则：
$$
\begin{array}{lcr}P_{ndc}.x=2*u-1\\ P_{ndc}.y=2*v-1\\ P_{ndc}.z=2*NonlinearDepth-1\\ P_{ndc}.w=1.0\end{array}
$$ 
由齐次除法可知 $\displaystyle \frac{Pclip}{Pclip.w}=Pndc$，则 
$$\displaystyle P_{clip}=P_{ndc}*P_{clip}.w \tag{1}$$

因为 $P_{clip}$ 是由 $P_{world}$ 经过 $VP$ 矩阵变换的来，我们将 $VP$ 矩阵写作 $M$ ，则 $MP_{world} = P_{clip}$，带入（1）
$$P_{world}=M^{-1}P_{clip}=M^{-1}P_{ndc}*P_{clip}.w\tag{2}$$

因为 $P_{world}=(x,y,z,1)$ ，我们将其 $w$ 分量分量带入（2）
$$
P_{world}.w=({M^{-1}}P_{ndc}).w*P_{clip}.w=1
$$
$$
P_{clip}.w={\frac1{(M^{-1}P_{ndc}).w}}\tag{3}
$$
将（3）带入（2）即可得出世界空间坐标：
 $$P_{world}=\frac{M^{-1}P_{ndc}\tag{2}}{{(M^{-1}P_{ndc}).w}}$$
### 使用摄像机射线构建
使用 VP 逆矩阵的方法需要在片元着色器中进行矩阵乘法，通常会影响性能。本节介绍的方法性能更好。
[ Unity3D Shader系列之深度纹理重建世界坐标_textrue3d unity 切片重建](https://blog.csdn.net/sinat_25415095/article/details/124764443)
在某些情况下，我们需要**屏幕后处理阶段**得到像素点对应的世界坐标。如下图，我们在屏幕后处理阶段，想要知道屏幕空间中 $A1$ 点对应的世界坐标 $A$ 点。那么 $A$ 点该怎么求呢？
![[Pasted image 20230709122334.png]]

- @ **已知条件：**
    1.  $O$ 点的世界坐标（即相机的世界坐标）：`_WorldSpaceCameraPos` 
    2. $OD$ 的长度（观察空间的线性深度值 `linearEyeDepth`） ：可以采样深度纹理得到
    3. 透视相机的各项参数：
        - 近、远裁剪平面的值
        - 视口角度 $FOV$
        - 横纵比 $Aspect$

- ! $A$ 点的世界坐标 = $O$ 点的世界坐标 + $\overrightarrow{OA}$，**我们要做的就是求** $\overrightarrow{OA}$ 

- % 主要步骤如下：
1. 求 $\mathrm{\overrightarrow{OLT},\overrightarrow{OLB},\overrightarrow{ORT},\overrightarrow{ORB}}$（ $\overrightarrow {OLT}$ 即从相机指向 $LT$ 的向量）
    - 当顶点为 $LT$ 点（即屏幕左上角）时将 $\overrightarrow {OLT}$ 向量的值放置在顶点着色器输出结构体中
    - 当顶点为 $LB$ 点（即在屏幕左下角）时将  $\overrightarrow {OLB}$ 放置在顶点着色器输出结构体中
    - 当顶点为 $RT$ 点（即在屏幕右上角）时将  $\overrightarrow {ORT}$ 放置在顶点着色器输出结构体中
    - 当顶点为 $RB$ 点（即在屏幕右下角）时将  $\overrightarrow {ORB}$  放置在顶点着色器输出结构体中
![[Pasted image 20230709123140.png|500]]
2. 利用 GPU 硬件的插值（顶点着色器的输出结构体会在三角形遍历阶段进行重心坐标插值，然后将插值后的值传递给片元着色器使用），得到 $\overrightarrow {OA1}$ 
3. 利用三角形的相似关系，可以得到 $\overrightarrow {OA}$ 
 
#### 步骤 1 
- @ 求 $\mathrm{\overrightarrow{OLT},\overrightarrow{OLB},\overrightarrow{ORT},\overrightarrow{ORB}}$

![[Pasted image 20230709121439.png]]

为了方便计算，我们可以先计算两个向量——$toTop$ 和 $toRight$, 它们是**起点位于近裁剪平面中心、分别指向摄像机正上方和正右方的向量**。它们的计算公式如下:
$$
halfHeight=Near\times\tan\biggl(\frac{FOV}2\biggr)
$$
$$
to Top = camera.up \times halfHeight
$$
$$
toRight =camera.right \times halfHeight \cdot aspect
$$
> $camera.up$ 是单位向量，指向摄像机正上方，只是用来确定向量方向

得到这两个辅助向量后，就可以计算 4 个角（图中的 TL、TR、BL、BR）相对于摄像机的方向了，只需要简单的向量运算：
$$
\begin{gathered}
\overrightarrow{OLT}=camera.forward \cdot Near+to Top-to Right\\
\overrightarrow{OLB}=camera.forward·Near-toTop-toRight \\
\overrightarrow{ORT}=camera.forward·Near+toTop+toRight \\
\overrightarrow{ORB}=camera,forward\cdot Near-toTop+toRight 
\end{gathered}
$$
注意这四个向量不仅包含了方向信息，它们的模对应了四个点到摄像机的距离。
#### 步骤 2
- @ 利用 GPU 硬件的插值，得到 $\overrightarrow {OA1}$ 

这一步是这种方法的核心。只有真正理解了这一步，才可以说是真正理解了这种方法。
我们先要明白什么是屏幕后处理。相机渲染完场景中所有物体后会得到一张渲染纹理，但是我们不直接把这张渲染纹理显示在屏幕上，而是额外对这张渲染纹理的每一个像素点进行处理一遍（这个过程就叫做屏幕后处理），然后将屏幕后处理的结果递到屏幕上。
屏幕后处理一般是通过额外渲染一个与屏幕大小相同的矩形网格来实现的。该网格只有 2 个三角面，共 4 个顶点，如下图。对每个像素的额外处理则会放到片元着色器中，具体处理的是哪一个像素用 uv 坐标来得到。
![[Pasted image 20230709124035.png]]

我们知道，在渲染流水线中，GPU 会在三角形设置阶段对顶点着色器输出结构体中的值进行重心坐标插值，然后再传递给片元着色器，就像下图这样。
![[Pasted image 20230709124046.png]]
也就是说，我们在步骤 1 中传递的 $\mathrm{\overrightarrow{OLT},\overrightarrow{OLB},\overrightarrow{ORT},\overrightarrow{ORB}}$ 经过 GPU 硬件的插值后，在片元着色器中将会得到（方向和长度通过重心坐标插值都能得到）。
这一步根本不用写代码，GPU 硬件已经实现了。

#### 步骤 3
- @ 利用三角形的相似关系，可以得到 $\overrightarrow {OA}$ 

我们得到的线性深度值 `linearEyeDepth` 并非是摄像机的欧氏距离，而是在 $z$ 方向的距离。
![[Pasted image 20230709124513.png]]

如图，以世界空间中的的 $A$ 点为例
$|OA|$ 是 A 点到摄像机的距离
$|OD|$ 为 A 点在观察空间的线性深度
$|OD1|$ 是相机的近裁剪平面距离

$\overrightarrow {OA1}$ 在第二步插值得到 ，用图中公式即可求得 $\overrightarrow {OA}$ 

# Unity 深度法线纹理
## 获取深度纹理
Unity 深度纹理深度纹理存储了高精度的深度值，范围是 $[0,1]$，通常是非线性分布。

- @ 1 首先要开启 URP Asset ->Depth Texture 并设置 Depth Texture Mode 为 Force Prepass 或 Depth Priming Mode 设置为 Auto 或Force
![[Pasted image 20230707140918.png|450]]
![[Pasted image 20230717212059.png|500]]

- @ 2 采样深度纹理，计算线性深度
`_CameraDepthTexture`：深度纹理
`_ZBufferParams`：用于线性化 Z 缓冲区值。`x` 是 (1-near/far)，`y` 是 (far/near)，`z` 是 (x/far)，`w` 是 (y/far)。
`LinearViewDepth`: 把深度纹理的采样结果转换成观察空间下的深度值，返回范围在 $[near, far]$ 的线性深度值
`Linear01Depth`：返回范围在 $[0,1]$ 的线性深度值

```cs fold file:手写
//声明深度纹理
TEXTURE2D(_CameraDepthTexture);  
SAMPLER(sampler_CameraDepthTexture);

//获取屏幕空间UV
float2 ScreenUV = i.positionCS.xy / _ScreenParams.xy;

//用屏幕采样屏幕深度纹理得到非线性深度，转换成0-1线性深度图
float depthColor = SAMPLE_TEXTURE2D(_CameraDepthTexture, sampler_CameraDepthTexture, ScreenUV).r;
float linearDepthColor = Linear01Depth(depthColor,_ZBufferParams);

//计算模型深度，转换成线性深度图
float depth = i.positionCS.z;
float linearDepth = Linear01Depth(depth,_ZBufferParams);
```

```cs file:⭐直接调用api
//内部声明了深度纹理_CameraDepthTexture
#include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/DeclareDepthTexture.hlsl"

//获取屏幕UV
float2 ScreenUV = GetNormalizedScreenSpaceUV(i.positionCS);

//从深度纹理中采样深度
#if UNITY_REVERSED_Z
    // 具有 REVERSED_Z 的平台（如 D3D）的情况。
    float depth = SampleSceneDepth(ScreenUV); //根据实际情况除以缩放因子
#else
    // 没有 REVERSED_Z 的平台（如 OpenGL）的情况。
    // 调整 Z 以匹配 OpenGL 的 NDC ([-1, 1])
    float depth = lerp(UNITY_NEAR_CLIP_VALUE, 1, SampleSceneDepth(uvSS));
#endif

//用屏幕采样屏幕深度纹理得到非线性深度，转换成0-1线性深度图
float linearDepthColor = Linear01Depth(depth,_ZBufferParams);
```

- @ 3 深度图内对象添加 pass
**要想对象在深度纹理中显示，对象的 shader 需要加一个 pass：**
可以直接添加 Lit 内置的 pass，这种方法会打破 SRPBatcher 的，可以自己写一个来匹配 SRPBatcher 条件
```cs fold file:Lit-DepthOnly
        Pass
        {
            Name "DepthOnly"
            Tags
            {
                "LightMode" = "DepthOnly"
            }

            // -------------------------------------
            // Render State Commands
            ZWrite On
            ColorMask R
            Cull[_Cull]

            HLSLPROGRAM
            #pragma target 2.0

            // -------------------------------------
            // Shader Stages
            #pragma vertex DepthOnlyVertex
            #pragma fragment DepthOnlyFragment

            // -------------------------------------
            // Material Keywords
            #pragma shader_feature_local_fragment _ALPHATEST_ON
            #pragma shader_feature_local_fragment _SMOOTHNESS_TEXTURE_ALBEDO_CHANNEL_A

            // -------------------------------------
            // Unity defined keywords
            #pragma multi_compile_fragment _ LOD_FADE_CROSSFADE

            //--------------------------------------
            // GPU Instancing
            #pragma multi_compile_instancing
            #include_with_pragmas "Packages/com.unity.render-pipelines.universal/ShaderLibrary/DOTS.hlsl"

            // -------------------------------------
            // Includes
            #include "Packages/com.unity.render-pipelines.universal/Shaders/LitInput.hlsl"
            #include "Packages/com.unity.render-pipelines.universal/Shaders/DepthOnlyPass.hlsl"
            ENDHLSL
        }
```

```cs fold file:手写-DepthOnly
//景深Pass
    Pass
    {
        //Pass名称
        Name "DepthOnly"

        //渲染目标
        Tags{"LightMode" = "DepthOnly"}

        //开启深度写入
        ZWrite On

        //深度测试 正向前后排序
        ZTest LEqual      
        //Greater > , GEqual >= , Less < , LEqual <= , Equal == , NotEqual != ,Always(永远渲染), Never(从不渲染);

        //只输出深度数据，以节省带宽
        ColorMask 0


        HLSLPROGRAM  //URP 程序块开始

        //指向顶点函数
        #pragma vertex vert

        //指向面渲染函数
        #pragma fragment frag 

        //URP函数命令库
        #include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl"

        //源数据
        struct VertexInput
        {
            //原始物体空间顶点数据
            float4 positionOS : POSITION;//数据来源于（寄存器POSITION）
        };

        //顶点输出
        struct VertexOutput
        {
            //物体裁切空间坐标
            float4 positionCS : SV_POSITION;//数据目标（寄存器SV_POSITION）

            //世界空间顶点
            float3 positionWS :  TEXCOORD0;//数据目标（寄存器TEXCOORD0~7）
        };

        //顶点函数
        VertexOutput vert(VertexInput v)
        {
            //声明输出变量o
            VertexOutput o;

            //输入物体空间顶点数据
            VertexPositionInputs positionInputs = GetVertexPositionInputs(v.positionOS.xyz);

            //获取裁切空间顶点
            o.positionCS = positionInputs.positionCS;

            //获取世界空间顶点
            o.positionWS = positionInputs.positionWS;

            //输出
            return o;
        }

        //面渲染函数
        float4 frag(VertexOutput i) : SV_Target
        {
            //输出物体的世界空间顶点到摄像机距离
            return distance(i.positionWS.xyz , _WorldSpaceCameraPos.xyz);
        }

        ENDHLSL  
    }
```

## 获取深度+法线纹理
- @ 1 首先添加 SSAO RenderFeature，Source->Depth Normals
![[Pasted image 20230717212229.png|500]]
>这里也可单选 depth，来实现获取深度纹理

- @ 2 采样深度+法线纹理
```cs
//在shader脚本中定义
TEXTURE2D(_CameraDepthTexture);       SAMPLER(sampler_CameraDepthTexture);
TEXTURE2D(_CameraNormalsTexture);       SAMPLER(sampler_CameraNormalsTexture);

//或者直接引用，内部声明了纹理和采样器
#include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/DeclareDepthTexture.hlsl"
#include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/DeclareNormalsTexture.hlsl"

//深度使用方法和上节一样

//法线使用方法：
//屏幕空间uv  
float2 ScreenUV = GetNormalizedScreenSpaceUV(i.positionCS);
//采样即可
float3 normal = SampleSceneNormals(ScreenUV);
```

- @ 3 法线图内对象添加 pass
**要想对象在法线纹理中显示，对象的 shader 需要加一个 pass：**
可以直接添加 Lit 内置的 pass，这种方法会打破 SRPBatcher 的，可以自己写一个来匹配 SRPBatcher 条件
```cs fold file:DepthNormals
// This pass is used when drawing to a _CameraNormalsTexture texture
        Pass
        {
            Name "DepthNormals"
            Tags
            {
                "LightMode" = "DepthNormals"
            }

            // -------------------------------------
            // Render State Commands
            ZWrite On
            Cull[_Cull]

            HLSLPROGRAM
            #pragma target 2.0

            // -------------------------------------
            // Shader Stages
            #pragma vertex DepthNormalsVertex
            #pragma fragment DepthNormalsFragment

            // -------------------------------------
            // Material Keywords
            #pragma shader_feature_local _NORMALMAP
            #pragma shader_feature_local _PARALLAXMAP
            #pragma shader_feature_local _ _DETAIL_MULX2 _DETAIL_SCALED
            #pragma shader_feature_local_fragment _ALPHATEST_ON
            #pragma shader_feature_local_fragment _SMOOTHNESS_TEXTURE_ALBEDO_CHANNEL_A

            // -------------------------------------
            // Unity defined keywords
            #pragma multi_compile_fragment _ LOD_FADE_CROSSFADE

            // -------------------------------------
            // Universal Pipeline keywords
            #include_with_pragmas "Packages/com.unity.render-pipelines.universal/ShaderLibrary/RenderingLayers.hlsl"

            //--------------------------------------
            // GPU Instancing
            #pragma multi_compile_instancing
            #include_with_pragmas "Packages/com.unity.render-pipelines.universal/ShaderLibrary/DOTS.hlsl"

            // -------------------------------------
            // Includes
            #include "Packages/com.unity.render-pipelines.universal/Shaders/LitInput.hlsl"
            #include "Packages/com.unity.render-pipelines.universal/Shaders/LitDepthNormalsPass.hlsl"
            ENDHLSL
        }
```

```cs fold file:手写-DepthNormals
  //深度法线Pass
    Pass
    {
        //Pass名字
        Name "DepthNormals"

        //渲染目标深度法线
        Tags{"LightMode" = "DepthNormals"}

        //开启深度写入，这样就可以在模型渲染种获得正确层Mask
        ZWrite On

        //深度测试 正向前后排序
        ZTest LEqual

        HLSLPROGRAM  //URP 程序块开始

        //指向顶点函数
        #pragma vertex vert

        //指向面渲染函数
        #pragma fragment frag 

        //URP函数命令库
        #include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl"

        //源数据
        struct VertexInput
        {
            //原始物体空间顶点数据
            float4 positionOS : POSITION;//数据来源于（寄存器POSITION）

            //顶点中的物体空间法线数据
            float4 normalOS  : NORMAL;//数据来源于（寄存器NORMAL）

            //顶点中的物体空间切线数据
            float4 tangentOS  : TANGENT;//数据来源于（寄存器TANGENT）
        };

        //顶点输出
        struct VertexOutput
        {
            //物体裁切空间坐标
            float4 positionCS : SV_POSITION;//数据目标（寄存器SV_POSITION）

            //世界空间顶点
            float3 normalWS :  TEXCOORD0;//数据目标（寄存器TEXCOORD0~7）
        };

        //顶点函数
        VertexOutput vert(VertexInput v)
        {
            //声明输出变量o
            VertexOutput o;

            //输入物体空间顶点数据
            VertexPositionInputs positionInputs = GetVertexPositionInputs(v.positionOS.xyz);

            //获取裁切空间顶点
            o.positionCS = positionInputs.positionCS;


            //输入物体空间法线数据
            VertexNormalInputs normalInputs = GetVertexNormalInputs(v.normalOS.xyz, v.tangentOS);

            //获取世界空间法线
            o.normalWS = normalInputs.normalWS;

            return o;
        }


        //面渲染函数
        float4 frag(VertexOutput i) : SV_Target
        {
            //输出世界法线
            return  float4 (i.normalWS,1);
        }

        ENDHLSL  //URP 程序块结束
    }
```

## 脚本获取（未试验）
在 C# 端利用 **Shader. GetGlobalTexture** 来获取深度图，然后用 **Blit** 方法将其复制到 RenderTexture 上（这也是做 Hiz 剔除时重要的一环）。

这里需要注意的是，在 Unity 的生命周期中有很多的事件函数，例如 Start，Update，OnRenderObject 等等，那么我们应该在哪个事件中获取_CameraDepthTexture 才能保证是当前帧的深度图呢？
经过测试，**建议在 OnPostRender 中获得到当前帧的深度图**，如下：

```
void OnPostRender() {
    Graphics.Blit(Shader.GetGlobalTexture("_CameraDepthTexture"), renderTexture);
}
```

![[f0a1eb5076c260137185e5717bb453f5_MD5.jpg]]

注：有时在 OnPreRender 或者 Update 函数中获取_CameraDepthTexture，那么得到的深度图将会是 Scene 窗口下的深度图，具体原因暂时不明~

## 使用 RF 获取深度法线纹理（未试验）
描边需要深度 + 法线[纹理](https://so.csdn.net/so/search?q=%E7%BA%B9%E7%90%86&spm=1001.2101.3001.7020)的加持，效果才能达到最好，但 URP 下很多版本不支持直接获取_CameraNormalsTexture，而我本人也尝试了一下在 12.1.7 下偷懒直接拿 SSAO 里的 Depth Normal 图，

虽然也能实现吧，但是需要打开 SSAO 的同时，再在 [shader](https://so.csdn.net/so/search?q=shader&spm=1001.2101.3001.7020) 中加入指定的 Tag 为 "DepthNormals" 的 Pass 才能实现：

![](https://img-blog.csdnimg.cn/c3ee0729f62a4b50bec24bf4a593ab5e.png)

稍微有点麻烦，而且总有种用别人东西的感觉。

那就尝试一下自己动手吧！动手造一个获取深度法线纹理的轮子！

贴一下项目环境：

URP12.1.7

Unity2021.3.8f1

浅看两篇手动获取深度法线纹理的文章：[URP 深度法线纹理 - 简书 (jianshu. com)]( https://www.jianshu.com/p/6c75bb64e8a0 "URP 深度法线纹理 - 简书 (jianshu. com)") 和雪风大佬的 [urp 管线的自学 hlsl 之路第二十四篇科幻扫描效果后篇 - 哔哩哔哩 (bilibili. com)]( https://www.bilibili.com/read/cv6672641 "urp 管线的自学 hlsl 之路第二十四篇科幻扫描效果后篇 - 哔哩哔哩 (bilibili. com)")，实现都是依靠 build-in 底下的 shader，然后将绘制出来的纹理传递给 [URP](https://so.csdn.net/so/search?q=URP&spm=1001.2101.3001.7020) 下自己项目定义的 shader 使用。

### 1 定义 RenderFeature 获取法线深度图

这个是参考了上述的过程，说实话，内容太过复杂。只有不断多学习，多做，每次都好好做备注，总有一天会完全理解的：

```
using Unity.VisualScripting;
using UnityEngine;
using UnityEngine.Rendering;
using UnityEngine.Rendering.Universal;
 
 
public class DepthNormalsFeature : ScriptableRendererFeature
{
 
 
    // 定义3个共有变量
    public class Settings
    {
        //public Shader shader; // 设置后处理shader
        public Material material; //后处理Material
        public RenderPassEvent renderPassEvent = RenderPassEvent.BeforeRenderingPostProcessing; // 定义事件位置，放在了官方的后处理之前
    }
 
    // 初始化一个刚刚定义的Settings类
    public Settings settings = new Settings();
    // 初始化Pass
    DepthNormalsPass depthNormalsPass;
    // 初始化纹理
    RenderTargetHandle depthNormalsTexture;
    // 材质
    Material depthNormalsMaterial;
 
    // 给pass传递变量，并加入渲染管线中
    public override void Create()
    {
        // 通过Built-it管线中的Shader创建材质，最重要的一步！
        depthNormalsMaterial = CoreUtils.CreateEngineMaterial("Hidden/Internal-DepthNormalsTexture");
        // 获取Pass（渲染队列，渲染对象，材质）
        depthNormalsPass = new DepthNormalsPass(RenderQueueRange.opaque, -1, depthNormalsMaterial);
        // 设置渲染时机 = 预渲染通道后
        depthNormalsPass.renderPassEvent = RenderPassEvent.AfterRenderingPrePasses;
        // 设置纹理名
        depthNormalsTexture.Init("_CameraDepthNormalsTexture");
    }
 
    //这里你可以在渲染器中注入一个或多个渲染通道。
    //这个方法在设置渲染器时被调用。
    public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData)
    {
        // 对Pass进行参数设置（当前渲染相机信息，深度法线纹理）
        depthNormalsPass.Setup(renderingData.cameraData.cameraTargetDescriptor, depthNormalsTexture);
        // 写入渲染管线队列
        renderer.EnqueuePass(depthNormalsPass);
    }
    
}
 
public class DepthNormalsPass : ScriptableRenderPass
{
    int kDepthBufferBits = 32;                                   // 缓冲区大小
    private RenderTargetHandle Destination { get; set; }         // 深度法线纹理
 
    private Material DepthNormalsMaterial = null;                // 材质
 
    private FilteringSettings m_FilteringSettings;               // 筛选设置
 
    static readonly string m_ProfilerTag = "Depth Normals Pre Pass"; // 定义渲染Tag
 
    ShaderTagId m_ShaderTagId = new ShaderTagId("MyDepthOnly");    // 绘制标签，Shader需要声明这个标签的tag
 
    /// <summary>
    /// 构造函数Pass
    /// </summary>
    /// <param name="renderQueueRange"></param>
    /// <param name="layerMask"></param>
    /// <param name="material"></param>
    public DepthNormalsPass(RenderQueueRange renderQueueRange, LayerMask layerMask, Material material)
    {
        m_FilteringSettings = new FilteringSettings(renderQueueRange, layerMask);
        DepthNormalsMaterial = material;
    }
 
    /// <summary>
    /// 参数设置
    /// </summary>
    /// <param name="baseDescriptor"></param>
    /// <param name="Destination"></param>
    public void Setup(RenderTextureDescriptor baseDescriptor, RenderTargetHandle Destination)
    {
        // 设置纹理
        this.Destination = Destination;
    }
 
    /// <summary>
    /// 配置渲染目标，可创建临时纹理
    /// </summary>
    /// <param name="cmd"></param>
    /// <param name="cameraTextureDescriptor"></param>
    public override void Configure(CommandBuffer cmd, RenderTextureDescriptor cameraTextureDescriptor)
    {
        // 设置渲染目标信息
        RenderTextureDescriptor descriptor = cameraTextureDescriptor;
        descriptor.depthBufferBits = kDepthBufferBits;
        descriptor.colorFormat = RenderTextureFormat.ARGB32;
 
        // 创建一个临时的RT（储存深度法线纹理、目标信息和滤波模式）
        cmd.GetTemporaryRT(Destination.id, descriptor, FilterMode.Point);
        // 配置
        ConfigureTarget(Destination.Identifier());
        // 清楚，未渲染时配置为黑色
        ConfigureClear(ClearFlag.All, Color.black);
    }
 
    // 
    /// <summary>
    /// 后处理逻辑和渲染核心函数，相当于build-in 的OnRenderImage()
    /// 实现渲染逻辑
    /// </summary>
    /// <param name="context"></param>
    /// <param name="renderingData"></param>
    public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData)
    {
        var cmd = CommandBufferPool.Get(m_ProfilerTag);     // 设置渲染标签
 
        using (new ProfilingSample(cmd, m_ProfilerTag))
        {
            // 执行命令缓存
            context.ExecuteCommandBuffer(cmd);
            // 清楚数据缓存
            cmd.Clear();
 
            // 相机的排序标志
            var sortFlags = renderingData.cameraData.defaultOpaqueSortFlags;
            // 创建绘制设置
            var drawSettings = CreateDrawingSettings(m_ShaderTagId, ref renderingData, sortFlags);
            // 设置对象数据
            drawSettings.perObjectData = PerObjectData.None;
            // 设置覆盖材质
            drawSettings.overrideMaterial = DepthNormalsMaterial;
 
            // 绘制渲染器
            context.DrawRenderers(renderingData.cullResults, ref drawSettings, ref m_FilteringSettings);
 
            // 设置全局纹理
            cmd.SetGlobalTexture("_CameraDepthNormalsTexture", Destination.id);
        }
        // 执行命令缓冲区
        context.ExecuteCommandBuffer(cmd);
        CommandBufferPool.Release(cmd);
    }
 
        // 清除此呈现传递执行期间创建的任何已分配资源。
        public override void FrameCleanup(CommandBuffer cmd)
        {
            if (Destination != RenderTargetHandle.CameraTarget)
            {
                cmd.ReleaseTemporaryRT(Destination.id);
                Destination = RenderTargetHandle.CameraTarget;
            }
        }
}
```

### 2 在 Shader 中使用

上述 RenderFeature 我们获得了一个全局的**_CameraDepthNormalsTexture** 变量，我们就可以像 Build-in 下一样访问啦！

但是，一些之前固定管线下的一些采样、解码 Texture 函数在 URP 下不能直接用，要自己定义，主要需要一个解码函数。固定管线下函数：

![](https://img-blog.csdnimg.cn/58a2232f05b045868e3592b7e9f39ada.png)

其中：

![](https://img-blog.csdnimg.cn/be0e3ed0bb254e2180b66622fb53e910.png)

![](https://img-blog.csdnimg.cn/ce95e3b76dc54a3ca25897861f52359b.png)

直接搬运！完全没问题~

我给他合起来了，合成了一个函数，返回的时候用就行：

![](https://img-blog.csdnimg.cn/73d81dfa92bf4e3fb03ca1acc9e788fe.png)

还要注意，采样要是屏幕空间的 UV，不然乱七八糟。

然后 shader 后面必须也要加上一个自定义的 LightTag：

![](https://img-blog.csdnimg.cn/964e38b0b2b44633bc49a9be7e30caf7.png)

突然发现这个复杂程度跟 SSAO 那个差不多。。。

看看效果，我们单独输出深度和法线：

![](https://img-blog.csdnimg.cn/f6fca431af934970842f4c52d09efbc2.png)

![](https://img-blog.csdnimg.cn/ed6a5004be664762a5cf7ca4dbbfb64c.png)

一切正常！终于可以进行下一步了。

### 参考

[URP 深度法线纹理 - 简书 (jianshu. com)]( https://www.jianshu.com/p/6c75bb64e8a0 "URP 深度法线纹理 - 简书 (jianshu. com)")
# 深度纹理重建像素的世界空间位置
![[202377141633.gif]]
### 步骤
1. 包含文件：DeclareDepthTexture. hlsl 文件包含用于对摄像机深度纹理进行采样的实用程序： `SampleSceneDepth` 返回 `[0, 1]` 范围内的 $Z$ 值。
```cs file:包含文件
#include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/DeclareDepthTexture.hlsl"

//包含如下：已经声明了相机深度纹理，我们只需要传入屏幕空间uv调用采样函数
TEXTURE2D_X_FLOAT(_CameraDepthTexture);  
SAMPLER(sampler_CameraDepthTexture);

float SampleSceneDepth(float2 uv)  
{  
    return SAMPLE_TEXTURE2D_X(_CameraDepthTexture, sampler_CameraDepthTexture, UnityStereoTransformScreenSpaceTex(uv)).r;  
}  
  
float LoadSceneDepth(uint2 uv)  
{  
    return LOAD_TEXTURE2D_X(_CameraDepthTexture, uv).r;  
}
```

2. 在片元着色器中计算用于采样深度纹理的屏幕空间 UV 坐标，像素位置除以渲染目标分辨率 `_ScaledScreenParams`。`_ScaledScreenParams.xy` 属性会考虑渲染目标的任何缩放，例如动态分辨率。
```c file:用深度纹理和屏幕空间uv重建像素的世界空间位置  
//屏幕空间uv  
float2 ScreenUV = GetNormalizedScreenSpaceUV(i.positionCS);
//float2 ScreenUV = i.positionCS.xy / _ScaledScreenParams.xy; 等价
```

3. 在片元着色器中，使用 `SampleSceneDepth` 函数对深度缓冲区进行采样。
```c file:从深度纹理中采样深度
#if UNITY_REVERSED_Z
    // 具有 REVERSED_Z 的平台（如 D3D）的情况。
    //返回[1,0]的深度值
    real depth = SampleSceneDepth(ScreenUV);
#else
    // 没有 REVERSED_Z 的平台（如 OpenGL）的情况。
    // 调整 Z 以匹配 OpenGL 的 NDC
    real depth = lerp(UNITY_NEAR_CLIP_VALUE, 1, SampleSceneDepth(ScreenUV));
#endif
```

4. 用像素的 UV 和 Z 坐标重建世界空间位置。
```c file:重建世界空间位置
float3 rebuildPosWS = ComputeWorldSpacePosition(ScreenUV, depth, UNITY_MATRIX_I_VP);
```
`ComputeWorldSpacePosition` ：根据屏幕空间 UV 和深度 ($Z$) 值计算世界空间位置
`UNITY_MATRIX_I_VP` 是一个逆视图投影矩阵，可将点从裁剪空间变换为世界空间。

5. 对于未渲染几何图形的区域，深度缓冲区可能没有任何有效值。以下代码会在这些区域绘制黑色。
```c
//在远裁剪面附近将颜色设置为黑色。
#if UNITY_REVERSED_Z
    if(depth < 0.0001)
        return half4(0,0,0,1);
#else
    if(depth > 0.9999)
        return half4(0,0,0,1);
#endif
```
不同的平台对远裁剪面使用不同的 Z 值（0 == far，或 1 == far）。`UNITY_REVERSED_Z` 常量让代码可以正确处理所有平台。

6. **对象要在重建的世界坐标中显示，需要添加深度法线纹理pass**
### 代码
```c fold file:使用深度纹理和屏幕空间UV坐标来重建像素的世界空间位置
Shader "Custom/RebuildPixelWorldPos from DepthTexture"
{
    Properties
    {
        _MainTex ("MainTex", 2D) = "white" {}
        _BaseColor("BaseColor", Color) = (1,1,1,1)
        [Normal] _NormalMap("NormalMap", 2D) = "bump" {}
        _NormalScale("NormalScale", Range(0, 10)) = 1

        [Header(Specular)]
        _SpecularExp("SpecularExp", Range(1, 100)) = 32
        _SpecularStrength("SpecularStrength", Range(0, 10)) = 1
        _SpecularColor("SpecularColor", Color) = (1,1,1,1)
    }
    
    HLSLINCLUDE

    #include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl"
    #include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/lighting.hlsl"
    #include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/DeclareDepthTexture.hlsl"
    
    CBUFFER_START (UnityPerMaterial)
    float4 _MainTex_ST;
    float4 _BaseColor;
    float _NormalScale;
    float _SpecularExp;
    float _SpecularStrength;
    float4 _SpecularColor;
    CBUFFER_END

    TEXTURE2D(_MainTex);
    SAMPLER(sampler_MainTex);
    TEXTURE2D(_NormalMap);
    SAMPLER(sampler_NormalMap);
    struct Attributes
    {
        float4 positionOS : POSITION;
        float4 color : COLOR;
        float3 normalOS : NORMAL;
        float4 tangentOS : TANGENT;
        float2 uv : TEXCOORD0;
    };

    struct Varyings
    {
        float4 positionCS : SV_POSITION;
        float4 color : COLOR0;
        float2 uv : TEXCOORD0;
        float3 positionWS: TEXCOORD1;
        float3 normalWS : TEXCOORD2;
        float4 tangentWS : TEXCOORD3;
        float3 bitangentWS : TEXCOORD4;
        float3 viewDirWS : TEXCOORD5;
        float3 lightDirWS : TEXCOORD6;
    };
    ENDHLSL
    
    SubShader
    {
        Tags
        {
            "RenderPipeline" = "UniversalPipeline"
            "RenderType"="Opaque" 
        }

        Pass
        {
            Tags
            {
                "LightMode"="UniversalForward"
            }
            
            HLSLPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            Varyings vert (Attributes i)
            {
                Varyings o = (Varyings)0;

                o.positionCS = TransformObjectToHClip(i.positionOS.xyz);
                o.uv = i.uv.xy * _MainTex_ST.xy + _MainTex_ST.zw;
                // output.uv = TRANSFORM_TEX(input.uv, _MainTex);
                o.positionWS = TransformObjectToWorld(i.positionOS.xyz);
                o.normalWS = TransformObjectToWorldNormal(i.normalOS);
                o.tangentWS.xyz = TransformObjectToWorldDir(i.tangentOS.xyz);
                //o.bitangentWS = cross(o.normalWS, o.tangentWS.xyz) * i.tangentOS.w * GetOddNegativeScale();
                o.viewDirWS = normalize(_WorldSpaceCameraPos.xyz - o.positionWS);

                return o;
            }

            float4 frag(Varyings i) : SV_Target
            {
                //主光源
                Light mainLight = GetMainLight();

                //纹理采样
                float4 MainTex = SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv);
                float3 normalMap = UnpackNormalScale(
                    SAMPLE_TEXTURE2D(_NormalMap, sampler_NormalMap, i.uv), _NormalScale);

                //用深度纹理和屏幕空间uv重建像素的世界空间位置
                //屏幕空间uv
                float2 ScreenUV = i.positionCS.xy / _ScaledScreenParams.xy;
                //从深度纹理中采样深度
                #if UNITY_REVERSED_Z
                    // 具有 REVERSED_Z 的平台（如 D3D）的情况。
                    float depth = SampleSceneDepth (ScreenUV);
                #else
                    // 没有 REVERSED_Z 的平台（如 OpenGL）的情况。
                    // 调整 Z 以匹配 OpenGL 的 NDC ([-1, 1])
                    float depth = lerp(UNITY_NEAR_CLIP_VALUE, 1, SampleSceneDepth(ScreenUV));
                #endif
                // 重建世界空间位置
               float3 rebuildPosWS = ComputeWorldSpacePosition(ScreenUV, depth, UNITY_MATRIX_I_VP);
                
                //在远裁剪面附近将颜色设置为黑色。
                // #if UNITY_REVERSED_Z
                //     if(depth < 0.0001)
                //         return half4(0,0,0,1);
                // #else
                //     if(depth > 0.9999)
                //         return half4(0,0,0,1);
                // #endif
                return float4(normalize(rebuildPosWS), 1);
                
                //向量计算
                float3x3 TBN = CreateTangentToWorld(i.normalWS, i.tangentWS.xyz, i.tangentWS.w);
                float3 N = TransformTangentToWorld(normalMap, TBN, true);
                float3 L = normalize(mainLight.direction);
                float3 V = normalize(i.viewDirWS);
                float3 H = normalize(L + V);
                float NdotL = dot(N, L);
                float NdotH = dot(N, H);
                
                //颜色计算
                float3 diffuse = (0.5 * NdotL + 0.5) * _BaseColor.rgb * mainLight.color;
                float3 specular = pow(max(0, NdotH), _SpecularExp) * _SpecularStrength * _SpecularColor.rgb * mainLight.color;

                float4 finalColor = MainTex * float4((diffuse + _GlossyEnvironmentColor.rgb) + specular, 1);
                
                return finalColor;
            }
            
            ENDHLSL
        }
    }
    FallBack "Packages/com.unity.render-pipelines.universal/FallbackError"
}
```


# 深度图应用
## 相交高亮

思路是判断当前物体的深度值与深度图中对应的深度值是否在一定范围内，如果是则判定为相交。  
首先访问当前物体的深度值：

```c
//vertex
COMPUTE_EYEDEPTH(o.eyeZ);
```

然后访问深度图。由于此时不是 Post Process，因此需要利用投影纹理采样来访问深度图：

```c
//vertex
o.screenPos = ComputeScreenPos(o.vertex);
//fragment
float screenZ = LinearEyeDepth(SAMPLE_DEPTH_TEXTURE_PROJ(_CameraDepthTexture, UNITY_PROJ_COORD(i.screenPos)));
```

最后就是进行相交判断：

```c
float halfWidth = _IntersectionWidth / 2;
float diff = saturate(abs(i.eyeZ - screenZ) / halfWidth); //除以halfWidth来控制相交宽度为_IntersectionWidth

fixed4 finalColor = lerp(_IntersectionColor, col, diff);
return finalColor;
```

[完整代码点这里](https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2FKaimaChen%2FUnity-Shader-Demo%2Fblob%2Fmaster%2FUnityShaderProject%2FAssets%2FDepth%2FShaders%2FIntersectionHighlight.shader)

![[345bdadf2be3da62d8a6d331495c14c0_MD5.webp]]

IntersectionHighlight 场景

## 能量场

在相交高亮效果的基础上，加上**半透明**和**边缘高亮**，就能制造出一个简单的能量场效果：

```
float3 worldNormal = normalize(i.worldNormal);
float3 worldViewDir = normalize(i.worldViewDir);
float rim = 1 - saturate(dot(worldNormal, worldViewDir)) * _RimPower;

float screenZ = LinearEyeDepth(SAMPLE_DEPTH_TEXTURE_PROJ(_CameraDepthTexture, UNITY_PROJ_COORD(i.screenPos)));  
float intersect = (1 - (screenZ - i.eyeZ)) * _IntersectionPower;
float v = max (rim, intersect);

return _MainColor * v;
```

[完整代码点这里](https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2FKaimaChen%2FUnity-Shader-Demo%2Fblob%2Fmaster%2FUnityShaderProject%2FAssets%2FDepth%2FShaders%2FForceField.shader)

![[c9caaa1b8ea54e3b1463c0bfe39b2df8_MD5.webp]]

ForceField 场景

## 全局雾效

思路是让雾的浓度随着深度值的增大而增大，然后进行的原图颜色和雾颜色的插值：

```
fixed4 frag (v2f i) : SV_Target
{
    fixed4 col = tex2D(_MainTex, i.uv.xy);
    float depth = UNITY_SAMPLE_DEPTH(tex2D(_CameraDepthTexture, i.uv.zw));
    float linearDepth = Linear01Depth(depth);
    float fogDensity = saturate(linearDepth * _FogDensity);
    fixed4 finalColor = lerp(col, _FogColor, fogDensity);
    return finalColor;
}
```

[完整代码点这里](https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2FKaimaChen%2FUnity-Shader-Demo%2Fblob%2Fmaster%2FUnityShaderProject%2FAssets%2FDepth%2FShaders%2FFog.shader)

![[aa8f778ba81f5d48c2e06800891e7134_MD5.webp]]

Fog 场景

## 扫描线

思路与相交高亮效果类似，只是这里是 Post Process。自定义一个 [0,1] 变化的值_CurValue，根据_CurValue 与深度值的差进行颜色的插值：

```
fixed4 frag (v2f i) : SV_Target
{
    fixed4 originColor = tex2D(_MainTex, i.uv.xy);
    float depth = UNITY_SAMPLE_DEPTH(tex2D(_CameraDepthTexture, i.uv.zw));
    float linear01Depth = Linear01Depth(depth);
    float halfWidth = _LineWidth / 2;
    float v = saturate(abs(_CurValue - linear01Depth) / halfWidth); //线内返回(0, 1)，线外返回1
    return lerp(_LineColor, originColor, v);
}
```

[完整代码点这里](https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2FKaimaChen%2FUnity-Shader-Demo%2Fblob%2Fmaster%2FUnityShaderProject%2FAssets%2FDepth%2FShaders%2FScanLine.shader)

![[8a5f6b329c528a4a855e12d23f41a880_MD5.webp]]

ScanLine 场景

## 水淹

利用上面提到的第二种重建世界空间坐标的方法得到世界空间坐标，判断该坐标的 Y 值是否在给定阈值下，如果是则混合原图颜色和水的颜色：

```
fixed4 frag (v2f i) : SV_Target
{
    fixed4 col = tex2D(_MainTex, i.uv.xy);
    float depth = UNITY_SAMPLE_DEPTH(tex2D(_CameraDepthTexture, i.uv.zw));
    float linearEyeDepth = LinearEyeDepth(depth);
    float3 worldPos = _WorldSpaceCameraPos.xyz + i.frustumDir * linearEyeDepth;
                
    if(worldPos.y < _WaterHeight)
        return lerp(col, _WaterColor, _WaterColor.a); //半透明

    return col;
}
```

[完整代码点这里](https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2FKaimaChen%2FUnity-Shader-Demo%2Fblob%2Fmaster%2FUnityShaderProject%2FAssets%2FDepth%2FShaders%2FWaterFlooded.shader)

![[f72d713427cce38f7905b4f065e792dc_MD5.webp]]

WaterFlooded 场景

## 垂直雾效

利用上面提到的第二种重建世界空间坐标的方法得到世界空间坐标，让雾的浓度随着 Y 值变化：

```
fixed4 frag (v2f i) : SV_Target
{
    fixed4 col = tex2D(_MainTex, i.uv.xy);
    float depth = UNITY_SAMPLE_DEPTH(tex2D(_CameraDepthTexture, i.uv.zw));
    float linearEyeDepth = LinearEyeDepth(depth);
    float3 worldPos = _WorldSpaceCameraPos + linearEyeDepth * i.frustumDir.xyz;

    float fogDensity = (worldPos.y - _StartY) / (_EndY - _StartY);
    fogDensity = saturate(fogDensity * _FogDensity);
                
    fixed3 finalColor = lerp(_FogColor, col, fogDensity).xyz;
    return fixed4(finalColor, 1.0);
}
```

[完整代码点这里](https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2FKaimaChen%2FUnity-Shader-Demo%2Fblob%2Fmaster%2FUnityShaderProject%2FAssets%2FDepth%2FShaders%2FVerticalFog.shader)

![[9f21be08c43cbbcda925da3f7b4df461_MD5.webp]]

VerticalFog 场景

## 边缘检测

思路是取当前像素的附近 4 个角，分别计算出两个对角的深度值差异，将这两个差异值相乘就得到我们判断边缘的值。  
首先是得到 4 个角：

```
//vertex
//Robers算子
o.uv[1] = uv + _MainTex_TexelSize.xy * float2(-1, -1);
o.uv[2] = uv + _MainTex_TexelSize.xy * float2(-1, 1);
o.uv[3] = uv + _MainTex_TexelSize.xy * float2(1, -1);
o.uv[4] = uv + _MainTex_TexelSize.xy * float2(1, 1);
```

然后是得到这 4 个角的深度值：

```
float sample1 = Linear01Depth(UNITY_SAMPLE_DEPTH(tex2D(_CameraDepthTexture, i.uv[1])));
float sample2 = Linear01Depth(UNITY_SAMPLE_DEPTH(tex2D(_CameraDepthTexture, i.uv[2])));
float sample3 = Linear01Depth(UNITY_SAMPLE_DEPTH(tex2D(_CameraDepthTexture, i.uv[3])));
float sample4 = Linear01Depth(UNITY_SAMPLE_DEPTH(tex2D(_CameraDepthTexture, i.uv[4])));
```

最后就是根据对角差异来得到判断边缘的值：

```
float edge = 1.0;
//对角线的差异相乘
edge *= abs(sample1 - sample4) < _EdgeThreshold ? 1.0 : 0.0;
edge *= abs(sample2 - sample3) < _EdgeThreshold ? 1.0 : 0.0;

return edge;
// return lerp(0, col, edge); //描边
```

[完整代码点这里](https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2FKaimaChen%2FUnity-Shader-Demo%2Fblob%2Fmaster%2FUnityShaderProject%2FAssets%2FDepth%2FShaders%2FEdgeDetection.shader)

![[53076353ed3980d076d80e1a29017c3c_MD5.webp]]

EdgeDetection 场景

PS：上面这种只用深度值来检测边缘的效果并不太好，最好结合法线图来判断，原理都是一样的。

## 运动模糊 (Motion Blur)

![[7aadb194b02b3f359dca989ea1950907_MD5.webp]]

MotionBlur 场景

运动模糊主要用在竞速类游戏中用来体现出速度感。这里介绍的运动模糊只能用于**周围物体不动，摄像机动**的情景。  
思路是利用上面提到的重建世界坐标方法得到世界坐标，由于该世界坐标在摄像机运动过程中都是不动的，因此可以将该世界空间坐标分别转到摄像机运动前和运动后的坐标系中，从而得到两个 NDC 坐标，利用这两个 NDC 坐标就能得到该像素运动的轨迹，在该轨迹上多次取样进行模糊即可。  
首先是得到世界坐标（这里使用提到的第一种重建方法）：

```
float depth = UNITY_SAMPLE_DEPTH(tex2D(_CameraDepthTexture, i.uv.zw));
float4 H = float4(i.uv.x * 2 - 1, i.uv.y * 2 - 1, depth * 2 - 1, 1); //NDC坐标
float4 D = mul(_CurrentInverseVP, H);
float4 W = D / D.w; //将齐次坐标w分量变1得到世界坐标
```

然后是计算出运算前后的 NDC 坐标：

```
float4 currentPos = H;
float4 lastPos = mul(_LastVP, W);
lastPos /= lastPos.w;
```

最后就是在轨迹上多次取样进行模糊：

```
//采样两点所在直线上的点，进行模糊
fixed4 col = tex2D(_MainTex, i.uv.xy);
float2 velocity = (currentPos - lastPos) / 2.0;
float2 uv = i.uv;
uv += velocity;
int numSamples = 3;
for(int index = 1; index < numSamples; index++, uv += velocity)
{
    col += tex2D(_MainTex, uv);
}
col /= numSamples;
```

[完整代码点这里](https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2FKaimaChen%2FUnity-Shader-Demo%2Fblob%2Fmaster%2FUnityShaderProject%2FAssets%2FDepth%2FShaders%2FMotionBlur.shader)

## 景深 (Depth Of Field)

![[2dc7222d3d4f3469ba8fb883fb63086d_MD5.webp]]

DepthOfField 场景

景深是一种聚焦处清晰，其他地方模糊的效果，在摄影中很常见。  
思路是首先渲染一张模糊的图，然后在深度图中找到聚焦点对应的深度，该深度附近用原图，其他地方渐变至模糊图。  
第一步是使用 SimpleBlur Shader 渲染模糊的图，这里我只是简单地采样当前像素附近的 9 个点然后平均，你可以选择更好的模糊方式：

```
v2f vert (appdata v)
{
    v2f o;
    o.vertex = UnityObjectToClipPos(v.vertex);

    o.uv[0] = v.uv + _MainTex_TexelSize.xy * float2(-1, -1) * _BlurLevel;
    o.uv[1] = v.uv + _MainTex_TexelSize.xy * float2(-1, 0) * _BlurLevel;
    o.uv[2] = v.uv + _MainTex_TexelSize.xy * float2(-1, 1) * _BlurLevel;
    o.uv[3] = v.uv + _MainTex_TexelSize.xy * float2(0, -1) * _BlurLevel;
    o.uv[4] = v.uv + _MainTex_TexelSize.xy * float2(0, 0) * _BlurLevel;
    o.uv[5] = v.uv + _MainTex_TexelSize.xy * float2(0, 1) * _BlurLevel;
    o.uv[6] = v.uv + _MainTex_TexelSize.xy * float2(1, -1) * _BlurLevel;
    o.uv[7] = v.uv + _MainTex_TexelSize.xy * float2(1, 0) * _BlurLevel;
    o.uv[8] = v.uv + _MainTex_TexelSize.xy * float2(1, 1) * _BlurLevel;

    return o;
}
            
fixed4 frag (v2f i) : SV_Target
{
    fixed4 col = tex2D(_MainTex, i.uv[0]);
    col += tex2D(_MainTex, i.uv[1]);
    col += tex2D(_MainTex, i.uv[2]);
    col += tex2D(_MainTex, i.uv[3]);
    col += tex2D(_MainTex, i.uv[4]);
    col += tex2D(_MainTex, i.uv[5]);
    col += tex2D(_MainTex, i.uv[6]);
    col += tex2D(_MainTex, i.uv[7]);
    col += tex2D(_MainTex, i.uv[8]);
    col /= 9;
    return col;
}
```

第二步就是传递该模糊的图给 DepthOfField Shader：

```
RenderTexture blurTex = RenderTexture.GetTemporary(source.width, source.height, 16);
Graphics.Blit(source, blurTex, blurMat);
dofMat.SetTexture("_BlurTex", blurTex);
Graphics.Blit(source, destination, dofMat);
```

第三步就是在 DepthOfField Shader 中根据焦点来混合原图颜色和模糊图颜色：

```
fixed4 col = tex2D(_MainTex, i.uv.xy);
fixed4 blurCol = tex2D(_BlurTex, i.uv.zw);
float depth = UNITY_SAMPLE_DEPTH(tex2D(_CameraDepthTexture, i.uv.zw));
float linearDepth = Linear01Depth(depth);
float v = saturate(abs(linearDepth - _FocusDistance) * _FocusLevel);
return lerp(col, blurCol, v);
```

[完整代码点这里](https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2FKaimaChen%2FUnity-Shader-Demo%2Fblob%2Fmaster%2FUnityShaderProject%2FAssets%2FDepth%2FShaders%2FDepthOfField.shader)

## 完整项目地址

[https://github.com/KaimaChen/Unity-Shader-Demo/tree/master/UnityShaderProject](https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2FKaimaChen%2FUnity-Shader-Demo%2Ftree%2Fmaster%2FUnityShaderProject)

## 参考

[Unity Docs - Camera’s Depth Texture](https://link.jianshu.com?t=https%3A%2F%2Fdocs.unity3d.com%2FManual%2FSL-CameraDepthTexture.html)  
[Unity Docs - Platform-specific rendering differences](https://link.jianshu.com?t=https%3A%2F%2Fdocs.unity3d.com%2FManual%2FSL-PlatformDifferences.html)  
[神奇的深度图：复杂的效果，不复杂的原理](https://link.jianshu.com?t=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F27547127%3Frefer%3Dchenjiadong)  
[SPECIAL EFFECTS WITH DEPTH](https://link.jianshu.com?t=https%3A%2F%2Fwww.google.com%2Furl%3Fsa%3Dt%26rct%3Dj%26q%3D%26esrc%3Ds%26source%3Dweb%26cd%3D2%26ved%3D0ahUKEwiM1Zic6efWAhXLrVQKHXLyCywQFggsMAE%26url%3Dhttps%253A%252F%252Fhalckemy.s3.amazonaws.com%252Fuploads%252Fpdf_file%252Ffile%252F91281%252FSiggraph2011_SpecialEffectsWithDepth_WithNotes.pdf%26usg%3DAOvVaw2AoGZitmnmb76btIOG0YWB)  
[GPU Gems - Chapter 27. Motion Blur as a Post-Processing Effect](https://link.jianshu.com?t=https%3A%2F%2Fdeveloper.nvidia.com%2Fgpugems%2FGPUGems3%2Fgpugems3_ch27.html)  
《Unity Shader 入门精要》  
《Unity 3D ShaderLab 开发实战详解》